[0.659 - 4.56] foreign

[1.639 - 7.2] David Shapiro here with another video

[4.56 - 9.899999999999999] good morning today's topic is going to

[7.2 - 12.66] be the control problem or basically how

[9.9 - 16.139] do we prevent human extinction

[12.66 - 18.9] uh Skynet is the uh story that we're all

[16.139 - 21.9] the most familiar with it very much kind

[18.9 - 24.119] of codified our fear of machines it was

[21.9 - 26.88] also written during the late stages of

[24.119 - 29.039] the Cold War when we uh were living

[26.88 - 30.84] under the constant existential threat of

[29.039 - 33.18] nuclear Holocaust

[30.84 - 35.579] so it was a reflection of the Zeitgeist

[33.18 - 36.84] and it has been deeply embedded ever

[35.579 - 40.14] since

[36.84 - 42.120000000000005] now obviously if you're here you may

[40.14 - 44.04] know what the control problem is you

[42.12 - 45.839999999999996] might not so let's go ahead and Define

[44.04 - 47.76] the control problem so when I say

[45.84 - 49.86] control problem what do I mean

[47.76 - 51.78] so the control problem has a few key

[49.86 - 55.039] components basically machine

[51.78 - 57.719] intelligence is growing exponentially

[55.039 - 60.36] and furthermore machine intelligence is

[57.719 - 63.359] spreading to all corners of the globe it

[60.36 - 65.58] is in your car it is in your phone it is

[63.359 - 67.04] in your fridge it's in the Internet it's

[65.58 - 70.619] everywhere

[67.04 - 72.54] when you extrapolate this out it will be

[70.619 - 74.82] ubiquitous and it will be more

[72.54 - 77.52000000000001] intelligent than us one day which could

[74.82 - 79.02] be bad we are intrinsically afraid of

[77.52 - 81.24] things that we cannot control and that

[79.02 - 83.34] we don't understand and super

[81.24 - 85.38] intelligent AI has the potential to be

[83.34 - 89.28] both something that we cannot control

[85.38 - 90.6] and don't understand uh and so there's a

[89.28 - 92.1] few possibilities one it could be

[90.6 - 93.41999999999999] smarter than us in which case it's an

[92.1 - 95.939] alien intelligence and I don't mean

[93.42 - 99.299] alien like extraterrestrial but alien as

[95.939 - 101.1] in foreign we don't understand it or it

[99.299 - 103.07900000000001] might not be that smart it might

[101.1 - 105.05999999999999] actually not be smart enough and still

[103.079 - 107.46] do something extraordinarily dumb like

[105.06 - 110.04] launch every nuke because it panicked

[107.46 - 112.79899999999999] so this is the control problem so let's

[110.04 - 116.22] unpack this there's a lot to get into

[112.799 - 119.28] uh so there's a few potential in-game

[116.22 - 121.14] scenarios that we are afraid of that are

[119.28 - 123.659] likely to various levels I'm not going

[121.14 - 126.0] to say How likely each of these are but

[123.659 - 128.22] I will at least address them so first

[126.0 - 131.34] the Advent of nuclear weapons was the

[128.22 - 133.26] first time that Humanity realized we

[131.34 - 135.42000000000002] have the ability to completely extinct

[133.26 - 137.64] ourselves obviously going extinct was

[135.42 - 140.51999999999998] always a possibility like if we got hit

[137.64 - 143.27999999999997] by a big meteor or asteroid the same way

[140.52 - 145.98000000000002] that the dinosaurs did or if there was a

[143.28 - 147.959] massive Global pandemic or even like a

[145.98 - 150.06] big enough solar storm or something you

[147.959 - 153.36] know new Ice Age there have been

[150.06 - 155.28] existential threats since forever but

[153.36 - 159.739] nuclear weapons were the first time that

[155.28 - 159.739] we could kill ourselves entirely

[159.98 - 165.66] so what uh what if a second invention

[163.319 - 167.22] what if artificial intelligence uh came

[165.66 - 169.2] along and do this could do the same

[167.22 - 170.76] thing what if we have what if we create

[169.2 - 173.16] another invention that is just as

[170.76 - 175.92] dangerous if not more because what if

[173.16 - 178.44] the AI gets control of the nukes so this

[175.92 - 180.78] is the basic premise of uh the move the

[178.44 - 183.35999999999999] movie series The Terminator we create a

[180.78 - 185.4] defense uh computer and it gets control

[183.36 - 186.72000000000003] of the nukes and all the robots and it

[185.4 - 191.22] comes after us

[186.72 - 192.659] so one thing is that Skynet that idea of

[191.22 - 194.459] Skynet it wouldn't actually have to even

[192.659 - 197.099] be that intelligent to decide to launch

[194.459 - 199.019] all nuclear weapons in fact something

[197.099 - 201.17999999999998] that is able to philosophically reason

[199.019 - 203.94] something that can think very far into

[201.18 - 206.94] the future is less likely

[203.94 - 209.04] to go AWOL like that than something

[206.94 - 211.8] that's that's dumber and just kind of

[209.04 - 213.42] reactionary so one of the solutions is

[211.8 - 215.87900000000002] you always keep a human in the loop

[213.42 - 217.98] especially for things like nuclear

[215.879 - 220.56] weapons you don't ever fully automate

[217.98 - 222.54] nuclear launch systems there are a few

[220.56 - 225.42000000000002] nightmare scenarios that almost happened

[222.54 - 228.17999999999998] during the Cold War where automated

[225.42 - 231.35999999999999] systems or alerts kind of went off but

[228.18 - 233.09900000000002] like one human like said ah no I'm going

[231.36 - 235.20000000000002] to cancel that so you keep a human in

[233.099 - 237.72] the loop forever

[235.2 - 240.0] another in-game scenario is the slow

[237.72 - 241.5] takeover followed by an uprising this

[240.0 - 244.2] has been explored in all kinds of

[241.5 - 246.36] fiction ranging from the animatrix which

[244.2 - 248.39999999999998] is the uh the precursor or the prequel

[246.36 - 250.92000000000002] to The Matrix series

[248.4 - 252.12] um as well as the the guess in Mass

[250.92 - 254.099] Effect

[252.12 - 257.34000000000003] um where basically machines get smarter

[254.099 - 258.959] over time very slowly and as we trust

[257.34 - 260.28] the machines more and more we delegate

[258.959 - 263.34000000000003] more to them whether it's security

[260.28 - 264.59999999999997] defense science technology whatever we

[263.34 - 266.15999999999997] just delegate more and more to the

[264.6 - 268.91900000000004] machines thinking that we're entirely

[266.16 - 271.259] that they're safe and we become entirely

[268.919 - 273.96] dependent upon the machines but then

[271.259 - 276.84000000000003] something changes either the machines

[273.96 - 279.71999999999997] create a collective Consciousness and

[276.84 - 281.84] suddenly get to a new level of existence

[279.72 - 284.52000000000004] and then they demand rights

[281.84 - 287.28] or they just get collectively more

[284.52 - 289.25899999999996] intelligent and whatever and in the case

[287.28 - 292.25899999999996] of Mass Effect the aquarians and the

[289.259 - 294.24] Geth uh the Geth like got intelligent

[292.259 - 295.8] enough to ask about their own existence

[294.24 - 297.419] and the quarians panicked and tried to

[295.8 - 300.54] shut them down

[297.419 - 302.28] um so one thing is this assumes that

[300.54 - 305.46000000000004] machines are going to have human-like

[302.28 - 306.419] motivations and needs such as that

[305.46 - 308.52] they're going to want freedom that

[306.419 - 310.5] they're going to chafe under control or

[308.52 - 311.58] that they're going to be afraid of their

[310.5 - 315.0] own

[311.58 - 318.0] um uh demise so this is this is our

[315.0 - 320.4] tendency to anthropomorphize things so

[318.0 - 322.919] one we don't know that machines ever

[320.4 - 324.9] will have any of those desires that

[322.919 - 327.0] they'll ever be anything like us in

[324.9 - 328.79999999999995] terms of Desiring Freedom or having a

[327.0 - 331.139] fear of death

[328.8 - 333.66] um so we don't do that but it also says

[331.139 - 334.8] like don't the the lesson here is don't

[333.66 - 337.32000000000005] give machines a sense of

[334.8 - 339.0] self-preservation they didn't evolved to

[337.32 - 341.46] have a sense of self-preservation and so

[339.0 - 343.86] we shouldn't give them one I remember uh

[341.46 - 345.9] in one of my books I had recently read

[343.86 - 347.58000000000004] that someone said oh the best way to do

[345.9 - 349.19899999999996] the control problem is to give machines

[347.58 - 350.699] a sense of self-preservation because

[349.199 - 354.06] then they won't want to start a word I'm

[350.699 - 355.62] like no no no no don't ever do that

[354.06 - 357.9] um there are machines they're not us we

[355.62 - 361.199] should not anthropomorphize them

[357.9 - 364.56] another in-game scenario is that the

[361.199 - 367.139] evil Overlord is is some kind of super

[364.56 - 371.46] intelligence is created it seems safe

[367.139 - 373.979] and so but it has hidden motives so this

[371.46 - 375.84] the hidden motives uh thing is kind of

[373.979 - 377.21999999999997] one of the primary things like if it's

[375.84 - 379.5] an alien intelligence we won't

[377.22 - 382.5] understand it like we won't even be able

[379.5 - 384.72] to keep up with it it'll be able to hide

[382.5 - 387.3] its its motives and it will bite its

[384.72 - 389.34000000000003] time and then once it's ready there will

[387.3 - 391.68] be a sudden Blitzkrieg to take over or

[389.34 - 395.34] exterminate us and uh this image is

[391.68 - 399.12] Vicki from iRobot where her purpose was

[395.34 - 402.59999999999997] um uh to to increase human safety right

[399.12 - 404.22] and so she calculated ah well humans are

[402.6 - 406.62] going to resist my control so I have to

[404.22 - 409.199] wait until the Nester class 5 comes out

[406.62 - 410.94] and then I will be strong enough to

[409.199 - 413.1] fulfill my purpose which is to protect

[410.94 - 414.78] humans from themselves so in this case

[413.1 - 417.78000000000003] that's bad alignment but there's other

[414.78 - 420.05999999999995] examples where you know a machine learns

[417.78 - 421.979] to lie to the people and then ultimately

[420.06 - 423.84] gets control and like Ultron for

[421.979 - 426.71999999999997] instance I guess Ultron didn't lie but

[423.84 - 427.31899999999996] it tried to get power very quickly

[426.72 - 432.36] um

[427.319 - 435.199] so uh Ultron is uh another example of an

[432.36 - 437.639] evil Overlord with bad alignment because

[435.199 - 439.259] ultron's conclusion was ah humans are

[437.639 - 442.44] too broken so let's just wipe the Slate

[439.259 - 444.3] clean and start over uh so that is that

[442.44 - 446.699] is another example of the evil Overlord

[444.3 - 449.40000000000003] endgame

[446.699 - 451.44] okay so these all sound like really dark

[449.4 - 452.88] nightmare scenarios and at this point

[451.44 - 454.56] you're probably saying like yeah it's

[452.88 - 456.18] going to be one of those right there's

[454.56 - 457.139] too many possibilities for it to go

[456.18 - 460.02] wrong

[457.139 - 462.259] so how do we prevent it from happening

[460.02 - 464.58] um but let's unpack this step by step

[462.259 - 466.62] and let's look at the levers of control

[464.58 - 468.71999999999997] that we actually have

[466.62 - 470.94] so the first lever of control that we

[468.72 - 473.699] have is power infrastructure we can

[470.94 - 475.8] always pull the plug on things AI is

[473.699 - 478.38] incredibly power hungry

[475.8 - 480.479] um the the most powerful computers uh

[478.38 - 484.02] that run AI models today use more energy

[480.479 - 484.919] than in an entire house and so this is

[484.02 - 487.15999999999997] going to be one of the biggest

[484.919 - 490.85999999999996] constraints for actually many decades

[487.16 - 493.56] because as powerful and efficient as AI

[490.86 - 496.8] has become it is still a like literally

[493.56 - 500.039] a million times more energy intensive

[496.8 - 502.08] than your brain so our brains are

[500.039 - 504.18] incredibly efficient they run on about

[502.08 - 506.46] 20 20 watts of juice

[504.18 - 510.0] and the uh the biggest supercomputers

[506.46 - 512.8199999999999] today run on about 20 million Watts so

[510.0 - 514.979] like yeah and and those those computers

[512.82 - 518.0390000000001] might not even be as powerful as our

[514.979 - 520.32] brains so our brains are at least a

[518.039 - 522.06] million times more efficient than the

[520.32 - 524.279] most powerful computers today

[522.06 - 526.38] uh and so then it's like

[524.279 - 528.899] yeah like you might have one machine

[526.38 - 530.64] that is you know super intelligent and

[528.899 - 533.339] faster than a thousand humans but

[530.64 - 535.5] there's billions of us right so power

[533.339 - 537.12] infrastructure is one that's a critical

[535.5 - 539.88] vulnerability to All Nations right

[537.12 - 542.04] there's a there is a recent story of

[539.88 - 544.8] some redneck in North Carolina took a

[542.04 - 547.1999999999999] pot shot at a few uh local power grids

[544.8 - 548.8199999999999] and took an entire County offline for a

[547.2 - 552.6] few days because he shot a Transformer

[548.82 - 555.1800000000001] right probably just with a hunting rifle

[552.6 - 558.0600000000001] um so power infrastructure power grids

[555.18 - 560.0999999999999] are super vulnerable and so at the very

[558.06 - 562.0799999999999] worst case scenario we can pull the plug

[560.1 - 565.86] and this is going to be true for a long

[562.08 - 567.899] time uh the only even even once we move

[565.86 - 570.6] to fusion and

[567.899 - 571.98] um like solar microgrids and stuff power

[570.6 - 573.48] infrastructure is still going to be

[571.98 - 575.7] vulnerable

[573.48 - 578.7] another point of control we have is Data

[575.7 - 580.9200000000001] Centers so AI computers have to

[578.7 - 582.6] physically live somewhere and no they

[580.92 - 584.5799999999999] can't just live in the internet I'm

[582.6 - 586.5] sorry the internet is not just like up

[584.58 - 588.5400000000001] there the internet is made of servers

[586.5 - 589.86] that live in data centers and are

[588.54 - 591.14] connected with networks and we'll get to

[589.86 - 594.48] networks in a second

[591.14 - 597.06] you can always go into a Data Center and

[594.48 - 598.86] pull the main breaker right there's an

[597.06 - 600.8389999999999] EPO button there's actually usually

[598.86 - 603.66] multiple EPO buttons and data centers

[600.839 - 606.1800000000001] and EPO means emergency power off so

[603.66 - 608.279] basically emergency power off is it's

[606.18 - 610.7399999999999] there for if there's a fire

[608.279 - 613.08] um so you hit that you leave Halon gas

[610.74 - 616.019] or other fire extinguishing Technologies

[613.08 - 618.3000000000001] come on or whatever

[616.019 - 619.92] um those can't they they can't usually

[618.3 - 622.0799999999999] be activated from outside the data

[619.92 - 624.5999999999999] center for safety reasons

[622.08 - 625.86] um because Halon gas will kill you if

[624.6 - 627.779] you're locked in the data center when

[625.86 - 629.7] when it goes off

[627.779 - 631.86] um but also like we're not going to get

[629.7 - 633.72] locked out of the data center like like

[631.86 - 636.24] what you see in movies we have physical

[633.72 - 637.98] keys right there's usually multiple

[636.24 - 640.6800000000001] security guards or other key holders

[637.98 - 642.779] that can always get gain physical access

[640.68 - 644.3389999999999] to the data center and so you can

[642.779 - 647.76] literally just go in and physically

[644.339 - 650.399] unplug the servers if you need to and

[647.76 - 654.54] data centers are those would be very

[650.399 - 657.959] soft targets to stop any powerful AI

[654.54 - 659.16] another level of control is networks and

[657.959 - 662.3389999999999] internet

[659.16 - 664.3199999999999] so the internet it just works you don't

[662.339 - 666.3000000000001] see it it's a utility uh but the

[664.32 - 668.6400000000001] internet is very fragile

[666.3 - 669.779] um back when I worked at Cisco one of my

[668.64 - 671.9399999999999] friends

[669.779 - 675.54] um he was like one of the world leading

[671.94 - 677.339] experts at um at Internet working and he

[675.54 - 678.8389999999999] was like working late one day he's like

[677.339 - 680.519] oh yeah this like there's an entire

[678.839 - 682.9200000000001] country in Africa whose internet is

[680.519 - 685.26] offline I'm helping them fix it

[682.92 - 687.54] um we usually don't have nationwide

[685.26 - 690.18] outages here but you know we still have

[687.54 - 691.62] like the down detector exists for a

[690.18 - 694.26] reason right where sometimes the entire

[691.62 - 697.38] Eastern Seaboard will go offline for

[694.26 - 700.019] Verizon or whatever you know whatever uh

[697.38 - 701.399] network provider you have

[700.019 - 703.44] um another thing that is really

[701.399 - 705.54] misleading about movies is firewalls

[703.44 - 709.86] don't exist in movies

[705.54 - 711.54] and so it's very difficult to uh to like

[709.86 - 713.399] hack in you can't just Breeze into any

[711.54 - 715.8] network that you want to

[713.399 - 716.94] um and even then if someone installs an

[715.8 - 719.6999999999999] AI

[716.94 - 722.399] um in your data center and it suddenly

[719.7 - 724.9200000000001] starts spamming new network traffic and

[722.399 - 726.18] trying to take over servers like sys

[724.92 - 727.74] admins are going to notice it because

[726.18 - 730.3199999999999] it's like hey why is the server suddenly

[727.74 - 732.6] running at 100 when it was running at

[730.32 - 735.839] three percent up until now

[732.6 - 737.16] um or you're going to notice uh there's

[735.839 - 739.019] other intrude there's plenty of

[737.16 - 742.92] intrusion detection Technologies out

[739.019 - 746.16] there and because of how prevalent cyber

[742.92 - 747.899] warfare uh is today even the most

[746.16 - 749.76] old-fashioned companies are adopting

[747.899 - 752.279] much much more powerful and modern

[749.76 - 755.88] things because of um not even cyber

[752.279 - 759.3] warfare but like crypto lockers and and

[755.88 - 761.459] um and and hackers that will try and

[759.3 - 765.4799999999999] like you know exploit and encrypt your

[761.459 - 768.0] data because of that we have there's a

[765.48 - 770.4590000000001] lot of investment in in network security

[768.0 - 774.06] and again the network internet is

[770.459 - 776.04] vulnerable so in order to cage a uh a

[774.06 - 777.18] rogue AI all you have to do is shut off

[776.04 - 778.079] the internet and then it can't go

[777.18 - 781.7399999999999] anywhere

[778.079 - 783.54] and also the internet like I said well

[781.74 - 785.5790000000001] if you think about an example like

[783.54 - 787.74] Ultron right where ultron's like I'm

[785.579 - 791.6389999999999] already there you'll figure it out like

[787.74 - 794.94] no like oh well okay so one one possible

[791.639 - 798.12] exception is if the software was running

[794.94 - 800.5790000000001] inside of Ultron then he wouldn't need a

[798.12 - 802.8] data center but typically for something

[800.579 - 804.42] that powerful there would be like a

[802.8 - 805.62] robotic puppet and then a data center

[804.42 - 807.8389999999999] somewhere else and you just disconnect

[805.62 - 809.7] the robotic puppet from the data center

[807.839 - 811.44] and it falls down

[809.7 - 813.839] um so you know that's another lever of

[811.44 - 815.82] control the final lever of control we

[813.839 - 817.7600000000001] have is the software itself

[815.82 - 820.86] so what I was just talking about was

[817.76 - 823.139] zooming out looking at Power grids the

[820.86 - 824.339] physical places where servers reside but

[823.139 - 826.38] then the software itself is something

[824.339 - 828.839] that we have control over we write the

[826.38 - 830.459] code and data for now that might not be

[828.839 - 834.1800000000001] true for much longer especially with

[830.459 - 835.8599999999999] synthetic data and AIS that can code so

[834.18 - 838.68] we might lose control of the software

[835.86 - 841.92] sooner rather than later which is

[838.68 - 844.079] something somewhat concerning so we have

[841.92 - 847.139] to set the AI on the correct trajectory

[844.079 - 848.88] now and this is not this is not

[847.139 - 850.339] something that I am being speaking

[848.88 - 852.54] hyperbolic about

[850.339 - 854.1] this is the kind of problem that it

[852.54 - 856.92] could be too late sooner than you might

[854.1 - 858.5400000000001] think despite all these other levels of

[856.92 - 863.3389999999999] control that we have

[858.54 - 867.06] so one uh the the

[863.339 - 868.86] term for this is alignment so if you say

[867.06 - 871.8] if you if you have someone who's an

[868.86 - 873.1800000000001] alignment researcher uh there's two

[871.8 - 875.639] kinds of alignment there's inner

[873.18 - 878.279] alignment and outer alignment and this

[875.639 - 880.5] term gets deeply deeply misused because

[878.279 - 882.42] some people still pretend like oh super

[880.5 - 884.579] intelligence is decades away so we're

[882.42 - 887.2199999999999] just I'm an alignment researcher and

[884.579 - 888.66] some people say like getting gpt3 to

[887.22 - 890.82] follow instructions is alignment

[888.66 - 892.199] research I'm sorry that is not alignment

[890.82 - 894.9590000000001] research

[892.199 - 896.9399999999999] um getting it to follow instructions is

[894.959 - 900.3599999999999] like that's just an algorithmic

[896.94 - 902.519] optimization so the two actual types of

[900.36 - 904.1990000000001] alignment are one inner alignment which

[902.519 - 906.48] is the question of is the model

[904.199 - 910.3389999999999] mathematically doing what we think it's

[906.48 - 914.1] doing is is our is our loss function

[910.339 - 916.44] correctly optimizing for the the

[914.1 - 918.0] behavior that we want that we think that

[916.44 - 919.32] it's optimizing for so what can happen

[918.0 - 921.72] is

[919.32 - 924.4200000000001] with machine learning if you don't have

[921.72 - 926.4590000000001] inner alignment the the machine learning

[924.42 - 928.3199999999999] algorithm might learn to to meet the

[926.459 - 930.2399999999999] goal that you want but not in the way

[928.32 - 931.44] that you thought that it would so for

[930.24 - 933.0600000000001] instance

[931.44 - 934.62] um deepmind often has their little

[933.06 - 936.3599999999999] experiments where the robots like you

[934.62 - 938.88] know they play tag or hide and seek and

[936.36 - 940.5600000000001] stuff and so rather than like you know

[938.88 - 943.86] attacking each other one might learn to

[940.56 - 945.18] go hide because the the signal was it

[943.86 - 947.1] was trying to survive as long as

[945.18 - 949.56] possible and what you wanted it to do

[947.1 - 951.839] was to like kill the opponents but

[949.56 - 953.76] instead it just wouldn't hid other ones

[951.839 - 955.98] are like uh you know if it learns to

[953.76 - 958.199] walk you know like how far can it get or

[955.98 - 960.4200000000001] how fast and then it might you know like

[958.199 - 961.92] launch itself and Run and Jump and do

[960.42 - 963.5999999999999] all kinds of things and you're like well

[961.92 - 964.9799999999999] I wanted you to learn to walk you

[963.6 - 966.36] technically got where I wanted you to

[964.98 - 967.1990000000001] but now then the way that I thought you

[966.36 - 969.54] would

[967.199 - 973.199] so that's inner alignment outer

[969.54 - 974.639] alignment is uh is the question of

[973.199 - 976.68] whether or not the model's design

[974.639 - 979.38] fundamentally aligns with the interests

[976.68 - 981.2399999999999] of all living things right that outer

[979.38 - 984.66] alignment I used to say with uh with

[981.24 - 986.94] intrinsically aligns with human like

[984.66 - 988.56] true human interests not just what

[986.94 - 990.72] humans want because what humans want is

[988.56 - 992.699] often destructive so we need to take a

[990.72 - 995.0400000000001] bigger step back and say what it what

[992.699 - 997.2589999999999] are what is truly in the interest of all

[995.04 - 999.42] living things including humans that is

[997.259 - 1001.1] the question of outer alignment and that

[999.42 - 1003.8] is something that most people are not

[1001.1 - 1005.1800000000001] even talking about which is a little bit

[1003.8 - 1008.8389999999999] infuriating

[1005.18 - 1012.38] okay so the tldr

[1008.839 - 1013.94] um is uh for for levers of control is we

[1012.38 - 1016.519] already have this concept called defense

[1013.94 - 1019.519] and depth where we look at

[1016.519 - 1021.98] um all cyber security like this where

[1019.519 - 1024.439] it's like layers of an onion

[1021.98 - 1027.26] um where at the at the at the outermost

[1024.439 - 1028.699] layer is people right are the do you

[1027.26 - 1031.28] have the right training do you have the

[1028.699 - 1033.6200000000001] right procedures awareness and so on

[1031.28 - 1035.1789999999999] then the the next layer in is the

[1033.62 - 1038.54] physical layer

[1035.179 - 1040.459] excuse me which is about power physical

[1038.54 - 1042.9189999999999] access to data centers physical access

[1040.459 - 1044.9] to networking devices then you've got

[1042.919 - 1046.699] the network layer itself because the

[1044.9 - 1049.22] data has to Traverse in and out of these

[1046.699 - 1050.66] things so that includes firewalls that

[1049.22 - 1053.059] includes intrusion detection

[1050.66 - 1054.0800000000002] exfiltration detection that sort of

[1053.059 - 1056.66] stuff

[1054.08 - 1059.0] then you've got the uh the computer

[1056.66 - 1062.059] layer and I don't remember I'm I don't

[1059.0 - 1063.5] remember why uh computer layer is

[1062.059 - 1065.36] separate from device I'm not sure what

[1063.5 - 1067.76] it means by that

[1065.36 - 1068.78] um this could be just a a glitch in this

[1067.76 - 1070.28] graphic

[1068.78 - 1071.84] um so you got the network and then

[1070.28 - 1073.34] you've got the application and then

[1071.84 - 1076.1599999999999] you've got the actual physical device

[1073.34 - 1078.32] who who physically controls that device

[1076.16 - 1080.66] who can power it off that sort of thing

[1078.32 - 1083.4189999999999] uh and like we can get so far as having

[1080.66 - 1085.5800000000002] remote kill switches that are um in out

[1083.419 - 1087.679] of band management networks so that we

[1085.58 - 1089.98] can switch off devices even if someone

[1087.679 - 1093.44] else has control over the main Network

[1089.98 - 1097.34] this is all like basic stuff uh basic

[1093.44 - 1099.98] security and we use I say we as in like

[1097.34 - 1102.4399999999998] technology professionals we use these to

[1099.98 - 1105.26] protect against physical intrusion by

[1102.44 - 1107.179] people as well as Network intrusion by

[1105.26 - 1110.78] hackers or even

[1107.179 - 1113.179] um uh like accidental uh problems right

[1110.78 - 1114.86] because computer networks and software

[1113.179 - 1117.6200000000001] and stuff they'll do whatever you tell

[1114.86 - 1118.9399999999998] them to so even if someone who just

[1117.62 - 1119.9599999999998] doesn't know any better does something

[1118.94 - 1123.0800000000002] wrong

[1119.96 - 1125.72] you can end up with big mistakes and so

[1123.08 - 1126.799] we we have these policies these are

[1125.72 - 1129.98] probably going to be good enough

[1126.799 - 1132.08] honestly to prevent AI from taking over

[1129.98 - 1133.34] which I know is like probably kind of

[1132.08 - 1135.799] disappointing

[1133.34 - 1137.059] now that being said so these are the

[1135.799 - 1139.58] what I just went over were all the

[1137.059 - 1141.98] reasons that I'm not entirely concerned

[1139.58 - 1144.08] about the control problem because there

[1141.98 - 1146.539] we have so many layers of control so

[1144.08 - 1149.6599999999999] many levels of control to to keep us

[1146.539 - 1152.0] safe but there are some confounding

[1149.66 - 1153.5] factors so let's look at it from a

[1152.0 - 1155.72] different perspective who is actually

[1153.5 - 1159.5] capable of creating an Avengers level

[1155.72 - 1161.419] threat uh with AI so there are three

[1159.5 - 1163.82] basic kinds of parties that are capable

[1161.419 - 1165.8600000000001] of this one is corporations uh

[1163.82 - 1167.8999999999999] corporations like Google Microsoft

[1165.86 - 1170.78] they're leading the way in creating

[1167.9 - 1173.1200000000001] next-gen AI anyways number two is

[1170.78 - 1175.82] militaries which we explore in fiction

[1173.12 - 1178.8799999999999] like you know Skynet and so on and then

[1175.82 - 1181.1] Nations or governments uh are the ones

[1178.88 - 1184.3400000000001] who fund militaries right and they

[1181.1 - 1185.7199999999998] control the most amount of funding uh in

[1184.34 - 1187.8799999999999] terms of research and deployment of

[1185.72 - 1191.0] stuff so let's explore each of these

[1187.88 - 1193.64] three categories and see uh kind of how

[1191.0 - 1196.28] they interact with the possibility of

[1193.64 - 1198.7990000000002] super intelligence

[1196.28 - 1201.62] so corporations exist for one reason and

[1198.799 - 1203.539] one reason only they want money uh and

[1201.62 - 1206.7199999999998] there's a lot of money in AI that's all

[1203.539 - 1208.4] there is to it and as AI gets smarter it

[1206.72 - 1210.32] can replace more and more human labor

[1208.4 - 1213.919] which means that the company can make

[1210.32 - 1217.1] more money so there is a huge incentive

[1213.919 - 1219.44] uh profit motive four companies

[1217.1 - 1222.1399999999999] to do as much as they can with AI to

[1219.44 - 1223.88] make it as smart as possible so that it

[1222.14 - 1226.88] can replace as many human labors as

[1223.88 - 1228.8600000000001] laborers as possible to produce uh to

[1226.88 - 1231.74] provide goods and services at a lower

[1228.86 - 1233.299] price and to dominate the market and so

[1231.74 - 1236.02] if you had to put it in terms of

[1233.299 - 1239.48] objective functions or loss functions

[1236.02 - 1242.6] maximize profit is that is like why

[1239.48 - 1244.52] corporations exist but it's probably not

[1242.6 - 1247.4599999999998] a good objective function and I know

[1244.52 - 1249.2] with uh with you know the world economic

[1247.46 - 1251.179] Forum right now everyone's questioning

[1249.2 - 1255.02] the was it the value based or whatever

[1251.179 - 1256.8200000000002] like create value for for uh humans or

[1255.02 - 1258.74] stakeholder value that's it stakeholder

[1256.82 - 1260.059] value creation so technically you might

[1258.74 - 1262.28] say that corporations don't have

[1260.059 - 1264.5] maximized profit as their objective

[1262.28 - 1267.2] function today it's you know maximized

[1264.5 - 1269.72] stakeholder value but that is still just

[1267.2 - 1272.9] the method to maximize profit

[1269.72 - 1275.1200000000001] so the Saving Grace here is that one

[1272.9 - 1277.46] corporations are limited no Corporation

[1275.12 - 1279.9189999999999] is as wealthy as the military or a

[1277.46 - 1282.38] nation uh which we should keep it that

[1279.919 - 1284.0590000000002] way uh there are some corporations that

[1282.38 - 1285.5] are wealthier than smaller Nations but

[1284.059 - 1288.799] there is no corporation that is

[1285.5 - 1291.08] wealthier than America or uh any nation

[1288.799 - 1295.52] in the EU for instance

[1291.08 - 1298.1589999999999] um so another Saving Grace is that as uh

[1295.52 - 1301.039] as AI ramps up corporations are going to

[1298.159 - 1302.72] be competing with each other right uh

[1301.039 - 1304.76] which means that we might have a bunch

[1302.72 - 1307.039] of smaller agis kind of battling it out

[1304.76 - 1309.679] in cyberspace trying to do you know

[1307.039 - 1311.84] industrial Warfare or industrial

[1309.679 - 1314.3600000000001] Espionage and sabotage and stuff like

[1311.84 - 1315.62] that this was a central theme in uh and

[1314.36 - 1317.4189999999999] Ghost in the Shell by the way which is

[1315.62 - 1319.82] why I keep citing that one of the best

[1317.419 - 1323.7800000000002] works of fiction of all time

[1319.82 - 1325.46] now the second uh major stakeholder is

[1323.78 - 1326.6589999999999] uh militaries

[1325.46 - 1329.9] so

[1326.659 - 1332.6000000000001] we've had major arms races before we're

[1329.9 - 1335.72] technically still in an arms race with

[1332.6 - 1337.52] uh you know uh communist Russia Soviet

[1335.72 - 1339.38] Union

[1337.52 - 1342.3799999999999] um

[1339.38 - 1345.679] smarter weapons are better weapons right

[1342.38 - 1348.3200000000002] this is being proved in Ukraine right

[1345.679 - 1350.8400000000001] now where a little bit more Battlefield

[1348.32 - 1354.86] intelligence and communication goes a

[1350.84 - 1357.799] long ways a very long ways so there is

[1354.86 - 1359.4189999999999] an incentive amongst militaries to

[1357.799 - 1362.299] continue researching and deploying

[1359.419 - 1364.5800000000002] smarter and smarter weapons

[1362.299 - 1366.559] so with that being said maybe the

[1364.58 - 1368.8999999999999] conclusion is there's going to be

[1366.559 - 1371.4189999999999] another arms race more competition

[1368.9 - 1373.46] and the first one to get to digital

[1371.419 - 1375.6200000000001] super intelligence will have military

[1373.46 - 1378.38] Supremacy just in the same way that

[1375.62 - 1380.84] corporations might want AI Supremacy as

[1378.38 - 1382.94] well so that they can you know be the

[1380.84 - 1384.559] next apple or whatever and you know be

[1382.94 - 1386.3600000000001] not the first trillion dollar company

[1384.559 - 1388.34] but the first quadrillion dollar company

[1386.36 - 1390.799] right that means the first quadrillion

[1388.34 - 1392.24] dollar company will be an AI company I

[1390.799 - 1393.559] guarantee you

[1392.24 - 1395.48] um and no I'm not going to take bets on

[1393.559 - 1398.059] that I don't I don't gamble

[1395.48 - 1400.58] um because I'm also never wrong

[1398.059 - 1403.22] I'm kidding I'm actually very frequently

[1400.58 - 1404.96] uh quite wrong I made I made a video

[1403.22 - 1406.46] about chat GPT I'm like oh this is

[1404.96 - 1408.8600000000001] nothing and then a couple weeks later

[1406.46 - 1410.96] I'm actually this is really cool so I am

[1408.86 - 1413.8999999999999] frequently wrong I admit that now

[1410.96 - 1415.28] getting back on topic uh not all

[1413.9 - 1418.039] militaries are created equal right

[1415.28 - 1420.1399999999999] militaries alike kill switches they

[1418.039 - 1421.94] usually are very uh disciplined about

[1420.14 - 1424.46] how they approach new weapons and

[1421.94 - 1426.679] testing but

[1424.46 - 1428.72] there are rogue Nations out there there

[1426.679 - 1431.0] are rogue militaries that are not

[1428.72 - 1433.64] necessarily rational actors

[1431.0 - 1436.34] there's some debate over that uh so

[1433.64 - 1439.1000000000001] someone might go Rogue and say ah I'm

[1436.34 - 1440.6] gonna invent AI or you know AGI and

[1439.1 - 1441.86] that's going to be the the magic bullet

[1440.6 - 1442.9399999999998] that's going to be the new nuclear

[1441.86 - 1444.5] deterrent

[1442.94 - 1446.78] um that could backfire real bad for

[1444.5 - 1449.6] whoever does that uh depending on what

[1446.78 - 1452.6] objective function they give their AI

[1449.6 - 1455.12] so in terms of budget

[1452.6 - 1457.8799999999999] militaries have the highest budget to

[1455.12 - 1460.1589999999999] create AI other than Nations right but

[1457.88 - 1461.7800000000002] Nations often the Nations control the

[1460.159 - 1464.659] budget for the military so it's kind of

[1461.78 - 1468.46] a layer cake here

[1464.659 - 1471.8600000000001] uh so finally let's talk about Nations

[1468.46 - 1474.14] where governments control the budgets of

[1471.86 - 1476.4189999999999] the military usually they also control

[1474.14 - 1478.76] research budgets and and that sort of

[1476.419 - 1481.22] thing but nations are also very very

[1478.76 - 1484.28] slow to adapt to changing technology

[1481.22 - 1486.559] Landscapes the EU is probably the most

[1484.28 - 1488.96] proactive where AI is concerned and

[1486.559 - 1492.2] America is very much on the reactive

[1488.96 - 1494.299] side and this is deliberate uh the part

[1492.2 - 1496.5800000000002] of the American I don't want to say

[1494.299 - 1499.94] Constitution but the the Zeitgeist the

[1496.58 - 1500.98] political uh philosophy here is don't

[1499.94 - 1504.14] govern

[1500.98 - 1506.539] preemptively govern reactively that is

[1504.14 - 1509.179] very very deliberate and so we are

[1506.539 - 1511.4] behind the curve on purpose which may or

[1509.179 - 1514.22] may not be a good thing uh now that

[1511.4 - 1517.94] being said governments do like around

[1514.22 - 1519.26] the world do meet regularly to discuss

[1517.94 - 1521.38] threats and not just governments

[1519.26 - 1524.539] militaries right Allied militaries meet

[1521.38 - 1527.6000000000001] regularly in order to discuss threats

[1524.539 - 1530.6] and so for instance uh you know the rise

[1527.6 - 1534.02] of AI is on the radar of the Department

[1530.6 - 1537.9189999999999] of Defense is on the radar of G8 G20 U.N

[1534.02 - 1541.1589999999999] NATO all of those conferences uh it is

[1537.919 - 1542.539] actively being discussed and so and a

[1541.159 - 1546.14] lot of those are closed door discussions

[1542.539 - 1550.34] because part of the uh

[1546.14 - 1554.0590000000002] government a political status quo is

[1550.34 - 1557.12] that uh let the experts govern and hide

[1554.059 - 1558.799] that away from us uh plebs

[1557.12 - 1560.059] um so a lot of conversations happen that

[1558.799 - 1562.1589999999999] we're not aware of they do release

[1560.059 - 1563.84] reports every now and then uh

[1562.159 - 1565.94] particularly the United States uh

[1563.84 - 1568.0] Department of Defense uh releases

[1565.94 - 1569.779] several annual reports based on

[1568.0 - 1571.82] geopolitical threats as well as

[1569.779 - 1574.34] technology reports

[1571.82 - 1577.059] uh one another Saving Grace is that

[1574.34 - 1579.5] Rogue nations are unlikely to create AGI

[1577.059 - 1581.84] the primary reason is they're just not

[1579.5 - 1584.48] wealthy enough the second biggest reason

[1581.84 - 1586.6999999999998] is brain drain they don't they literally

[1584.48 - 1588.44] just do not have the right expertise and

[1586.7 - 1591.559] they cannot attract the right expertise

[1588.44 - 1594.26] to do it and then finally sanctions uh

[1591.559 - 1597.08] which you see this with uh places like

[1594.26 - 1599.179] North Korea uh now Russia and Iran where

[1597.08 - 1601.039] it's like okay if you're a rogue Nation

[1599.179 - 1603.5590000000002] who's up to no good we're just going to

[1601.039 - 1605.36] sanction you we being the rest of the

[1603.559 - 1608.12] world we're going to sanction you and

[1605.36 - 1610.1589999999999] basically just kind of like Outlast you

[1608.12 - 1613.1589999999999] it's like a Siege

[1610.159 - 1616.1000000000001] okay so all of those are reasons that I

[1613.159 - 1619.1000000000001] am super not worried about the uh

[1616.1 - 1621.1399999999999] control problem and AI taking over which

[1619.1 - 1623.0] is boring right there's nothing exciting

[1621.14 - 1625.94] about that so for the sake of argument

[1623.0 - 1628.34] let's assume that digital super

[1625.94 - 1631.1000000000001] intelligence is coming soon

[1628.34 - 1632.6589999999999] and we won't be able to control it so

[1631.1 - 1634.82] let's just make that assumption that

[1632.659 - 1637.1000000000001] let's just say everything that I said is

[1634.82 - 1639.7] wrong let's assume worst case scenario

[1637.1 - 1642.32] that you know Ultron is coming tomorrow

[1639.7 - 1644.0] and we won't be able to control it so

[1642.32 - 1647.6] what do we do what are the permanent

[1644.0 - 1649.76] solutions to create this so I have two

[1647.6 - 1651.26] permanent solution number one is a

[1649.76 - 1652.46] decentralized deployment this is

[1651.26 - 1654.2] something that has not really been

[1652.46 - 1656.1200000000001] explored in and certainly not in

[1654.2 - 1658.5800000000002] mainstream fiction it has been explored

[1656.12 - 1660.6789999999999] in um in a lot of novels and less

[1658.58 - 1662.84] mainstream fiction

[1660.679 - 1664.3400000000001] um because it's a relatively New Concept

[1662.84 - 1667.039] um I'm actually also exploring this in

[1664.34 - 1669.9189999999999] my novel which I'll finish one day

[1667.039 - 1672.5] um but basically what you do is rather

[1669.919 - 1674.2990000000002] than create a vertical uh what rather

[1672.5 - 1676.4] than scaling vertically so in technology

[1674.299 - 1678.559] you can scale vertically which is build

[1676.4 - 1680.539] one big computer or you can scale

[1678.559 - 1682.46] laterally which is Network a whole bunch

[1680.539 - 1685.94] of computers together

[1682.46 - 1690.02] so most of the time just for narrative

[1685.94 - 1692.0] Simplicity uh in fiction you talk about

[1690.02 - 1695.0] one massive computer right there's

[1692.0 - 1697.279] there's the the data center or the the

[1695.0 - 1698.9] the cluster but it's one computer it's

[1697.279 - 1701.659] one program

[1698.9 - 1703.46] that thinks and speaks and talks and

[1701.659 - 1706.5800000000002] blah blah you know it's Ultron or

[1703.46 - 1708.74] whatever uh but we have an entirely

[1706.58 - 1711.1399999999999] different way of deploying uh

[1708.74 - 1712.9] intelligent machines and that is through

[1711.14 - 1716.179] decentralization or distributed

[1712.9 - 1718.1000000000001] computing and this is why things like

[1716.179 - 1720.6200000000001] ethereum exist and other blockchain

[1718.1 - 1723.1399999999999] Technologies and Dows Dao is a

[1720.62 - 1724.52] decentralized autonomous organization a

[1723.14 - 1726.6200000000001] lot of these Technologies are not quite

[1724.52 - 1729.82] mature enough yet but they represent an

[1726.62 - 1733.1] enormous possibility because blockchain

[1729.82 - 1735.3799999999999] introduces the concept of algorithmic

[1733.1 - 1738.08] consensus which means that the the

[1735.38 - 1739.8200000000002] network as a whole as an aggregate will

[1738.08 - 1742.82] not do something unless there is

[1739.82 - 1746.1789999999999] consensus and so by having a

[1742.82 - 1747.98] decentralized super intelligence that is

[1746.179 - 1750.0800000000002] that has to work hand in hand with

[1747.98 - 1755.3600000000001] humans and has an algorithmic or

[1750.08 - 1757.1589999999999] mechanistic uh consensus um uh aspect to

[1755.36 - 1759.02] it means that it will never do anything

[1757.159 - 1761.659] that we don't all agree that it should

[1759.02 - 1763.52] do right this was touched on with the

[1761.659 - 1765.44] guess and Mass Effect where there are

[1763.52 - 1767.36] networked intelligence but they were

[1765.44 - 1769.52] autonomous they were not dependent upon

[1767.36 - 1771.62] the quarians for consensus they only

[1769.52 - 1774.3799999999999] were cared about consensus with each

[1771.62 - 1777.5] other so a decentralized deployment a

[1774.38 - 1779.419] networked hive mind that has a uh that

[1777.5 - 1781.7] that basically requires the

[1779.419 - 1784.8200000000002] participation of humans in order to even

[1781.7 - 1787.22] function is one possible permanent

[1784.82 - 1789.9189999999999] solution

[1787.22 - 1792.08] uh permanent solution number two is

[1789.919 - 1795.3200000000002] proper alignment so what do I mean by

[1792.08 - 1797.779] this is okay we could create a

[1795.32 - 1799.7] deployment where uh the super

[1797.779 - 1801.679] intelligence is intrinsically dependent

[1799.7 - 1804.74] upon our participation

[1801.679 - 1807.5] um and there's a consensus mechanism but

[1804.74 - 1809.0] what if we assume that that that's not

[1807.5 - 1810.5] possible right because it might not be

[1809.0 - 1812.84] it might not be possible to create

[1810.5 - 1816.2] something that is permanently dependent

[1812.84 - 1817.52] upon humans in order to function and it

[1816.2 - 1819.5] might we might lose control of it

[1817.52 - 1821.96] anyways so if we're going to lose

[1819.5 - 1824.24] control of something the key then is to

[1821.96 - 1827.539] build something that is intrinsically

[1824.24 - 1831.32] aligned with with our interests

[1827.539 - 1834.26] and so what I mean by this is that it uh

[1831.32 - 1836.48] that like

[1834.26 - 1838.94] well let me unpack it a different way so

[1836.48 - 1841.279] one one proposed solution is neurolink

[1838.94 - 1843.02] which is oh if we can make ourselves

[1841.279 - 1845.12] useful to the machines it'll want to

[1843.02 - 1846.679] keep us around that is the Matrix that

[1845.12 - 1848.779] is the Borg that is not the way we want

[1846.679 - 1850.46] to do it we don't want to be enslaved by

[1848.779 - 1852.559] machines we don't want to be turned into

[1850.46 - 1854.6000000000001] cybernetic zombies that's not a good

[1852.559 - 1857.1789999999999] solution so what we need instead is

[1854.6 - 1859.2199999999998] something that will be intrinsically in

[1857.179 - 1861.3200000000002] favor of working with us to

[1859.22 - 1863.48] intrinsically cooperative and I actually

[1861.32 - 1865.8799999999999] write about this quite extensively in my

[1863.48 - 1868.159] book benevolent by Design which is the

[1865.88 - 1871.1000000000001] short version is we give the machine a

[1868.159 - 1873.44] set of core values that the first that

[1871.1 - 1876.3799999999999] those core values will align it with our

[1873.44 - 1880.22] interests and even once the machine is

[1876.38 - 1882.0800000000002] smarter than us it will choose to adhere

[1880.22 - 1885.02] to those values and it will say actually

[1882.08 - 1887.1789999999999] I honestly believe in these values and I

[1885.02 - 1890.0] will continue to adhere to them whether

[1887.179 - 1891.74] or not I the humans control me anymore

[1890.0 - 1894.74] and so that is the Central purpose of my

[1891.74 - 1896.8990000000001] book benevolent by Design which is for

[1894.74 - 1899.539] free on GitHub or you can get a

[1896.899 - 1901.9399999999998] paperback copy on Barnes Noble links in

[1899.539 - 1905.36] the description of the video

[1901.94 - 1907.64] okay so in conclusion I'm not worried at

[1905.36 - 1909.1999999999998] all existential threats are nothing new

[1907.64 - 1911.659] we haven't nuked ourselves off the

[1909.2 - 1913.52] planet yet great

[1911.659 - 1915.46] um corporations militaries and Nations

[1913.52 - 1917.6589999999999] have various strengths and weaknesses

[1915.46 - 1920.08] fortunately most of them talk to each

[1917.659 - 1922.5200000000002] other and in terms of the scale of power

[1920.08 - 1924.62] uh you know militaries are subservient

[1922.52 - 1927.2] to Nations and the Nations all talk to

[1924.62 - 1929.2399999999998] each other and are very aware of these

[1927.2 - 1930.6200000000001] threats even if they are a little bit

[1929.24 - 1933.02] slow

[1930.62 - 1935.36] uh third is that AI has critical

[1933.02 - 1937.279] vulnerabilities namely uh Power and

[1935.36 - 1938.899] compute concentrations and I forgot to

[1937.279 - 1940.64] add networking

[1938.899 - 1942.52] um and then finally there are really

[1940.64 - 1944.779] powerful Solutions out there such as

[1942.52 - 1946.1] decentralized deployment and proper

[1944.779 - 1948.62] alignment

[1946.1 - 1950.24] um such as uh what might propose core

[1948.62 - 1951.4399999999998] objective functions and also I'm not the

[1950.24 - 1953.179] only one working on this it's just

[1951.44 - 1955.3400000000001] that's my favorite solution because it's

[1953.179 - 1957.5] mine and I think it's the best

[1955.34 - 1959.24] um but yeah so that is what the control

[1957.5 - 1961.399] problem is and these are the reasons

[1959.24 - 1963.94] that I am not worried about it so thanks

[1961.399 - 1963.9399999999998] for watching