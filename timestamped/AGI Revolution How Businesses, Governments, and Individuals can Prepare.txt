[0.56 - 4.4399999999999995] as it seems like it's a foregone

[2.0 - 6.759] conclusion that we will have AGI within

[4.44 - 9.719000000000001] the next year or at least really

[6.759 - 12.4] Advanced AI plus robotics whether or not

[9.719 - 14.959999999999999] you define it as AGI it is time to start

[12.4 - 17.64] preparing and adapting so today we will

[14.96 - 20.8] go over how businesses governments and

[17.64 - 23.199] individuals can prepare for the coming

[20.8 - 25.16] change so for the sake of this video

[23.199 - 28.279000000000003] I've had a few people ask what do I mean

[25.16 - 30.32] when I say AGI now my personal

[28.279 - 32.519999999999996] prediction is that any definition have

[30.32 - 35.2] of AGI will be satisfied within the next

[32.52 - 37.399] year so what I mean by that is the

[35.2 - 38.96] compounding returns of multimodal AI

[37.399 - 41.2] plus robotics research plus everything

[38.96 - 43.52] else going on means that basically

[41.2 - 46.239000000000004] anything that people any definition that

[43.52 - 48.64] I've seen of AGI um with the exception

[46.239 - 50.919999999999995] of a few like really extreme ones uh

[48.64 - 52.559] will be satisfied within the next year

[50.92 - 54.399] so but what do I mean by that so what

[52.559 - 55.919999999999995] are what are some of the examples so

[54.399 - 58.44] first uh

[55.92 - 60.84] multimodal we're already we're seeing

[58.44 - 63.718999999999994] models that are trained with uh image

[60.84 - 65.96000000000001] text embodiment data all kinds of stuff

[63.719 - 67.04] uh so basically every kind of data we

[65.96 - 69.759] can throw at models they're being

[67.04 - 72.60000000000001] trained with uh and maybe there's even

[69.759 - 75.08] going to be more kinds of modalities uh

[72.6 - 76.439] greater than median human uh cognitive

[75.08 - 78.439] abilities is another thing that we

[76.439 - 80.83999999999999] should expect already depending on how

[78.439 - 84.27999999999999] you measure it uh the IQ of some of

[80.84 - 86.52000000000001] these models is far above most humans uh

[84.28 - 88.68] particularly when you consider how fast

[86.52 - 90.88] they work uh now of course some people

[88.68 - 92.28] argue well it's not even and it's like

[90.88 - 95.119] I'm not going to get lost in the cuz

[92.28 - 97.07900000000001] that's a red herring like you know like

[95.119 - 99.479] look at its functional capacities look

[97.079 - 101.67999999999999] at what is it what it is able to achieve

[99.479 - 103.479] and what it is not able to achieve not

[101.68 - 105.07900000000001] saying like well it's not human

[103.479 - 106.67999999999999] therefore it's not thinking because

[105.079 - 108.79899999999999] those are just semantic word games and

[106.68 - 110.399] they're not actually helpful um and it's

[108.799 - 113.0] to me that looks like just a defensive

[110.399 - 116.24] mechanism just some some coping um

[113.0 - 119.68] anyways so another aspect is going to be

[116.24 - 122.24] autonomy so uh the rumors are swirling

[119.68 - 125.039] the that open Ai and others are training

[122.24 - 126.91999999999999] autonomous AI uh and they worked very

[125.039 - 128.28] very hard to make chat GPT not

[126.92 - 130.72] autonomous they made they worked very

[128.28 - 133.56] hard to make it passive uh like I am a

[130.72 - 135.519] helpful assistant um so okay sure

[133.56 - 137.04] whatever uh another thing that we should

[135.519 - 140.12] expect to see is continual

[137.04 - 143.519] self-improvement um once we get AGI and

[140.12 - 145.84] so this is kind of something that uh is

[143.519 - 147.87900000000002] debatable because the thing is is these

[145.84 - 149.879] models already have the capacity for in

[147.879 - 151.599] context learning so that means the

[149.879 - 154.28] accumulation of information that it can

[151.599 - 157.56] just use in real time still counts as

[154.28 - 160.64000000000001] learning however uh what we want to see

[157.56 - 162.879] is also the ability to update its models

[160.64 - 165.76] update its underlying models and

[162.879 - 168.0] software as well and of course uh you

[165.76 - 170.0] know GPT models already have the ability

[168.0 - 173.04] to code they already have the ability to

[170.0 - 175.68] synthesize data so there's no reason

[173.04 - 178.12] right now today there are no barriers to

[175.68 - 179.48000000000002] having uh architectures that can modify

[178.12 - 180.59900000000002] themselves except maybe the context

[179.48 - 182.35999999999999] window

[180.599 - 184.319] and the level of sophistication of some

[182.36 - 186.959] of these models but all we need is the

[184.319 - 188.67999999999998] architecture and it is there uh and I

[186.959 - 192.12] suspect that we will have all of this

[188.68 - 193.56] within the next 12 calendar months so

[192.12 - 195.28] what we're coming for is a paradigm

[193.56 - 198.84] shift and what I mean by a paradigm

[195.28 - 201.4] shift is that the impact of AGI is going

[198.84 - 203.12] to have a greater impact than both

[201.4 - 205.28] mechanization of the Second Industrial

[203.12 - 207.84] Revolution and the digital revolution of

[205.28 - 210.239] the third Industrial Revolution combined

[207.84 - 213.12] this is basically inventing all of human

[210.239 - 214.76] history and doubling that um it's going

[213.12 - 216.56] to have a huge social impact it's going

[214.76 - 218.79899999999998] to have a huge economic impact it's

[216.56 - 220.48] going to be a technological Leap Forward

[218.799 - 222.28] it's going to have huge geopolitical

[220.48 - 225.159] ramifications and finally it's also

[222.28 - 228.0] going to impact the government AGI is

[225.159 - 230.599] literally going to impact every pillar

[228.0 - 232.879] of society across the entire

[230.599 - 236.0] planet and the biggest threat here is

[232.879 - 238.159] normaly bias so the I guess the primary

[236.0 - 240.2] purpose of this video is let's address

[238.159 - 242.84] the normaly bias in the three main

[240.2 - 245.11999999999998] pillars uh on an individual basis on a

[242.84 - 247.76] governmental basis and on a business

[245.12 - 250.04] basis and so what is normaly bias

[247.76 - 252.519] normaly bias is a cognitive bias where

[250.04 - 255.35999999999999] you just basically say uh well the

[252.519 - 258.799] status quo is things are this way today

[255.36 - 260.84000000000003] and I'm looking at the past as uh as an

[258.799 - 262.4] anchoring and so I'm going to say okay

[260.84 - 264.23999999999995] well because of the past and the way

[262.4 - 266.15999999999997] things are today this is how it's always

[264.24 - 269.039] going to be and you always see this

[266.16 - 270.759] normaly bias in any kind of forecast and

[269.039 - 273.639] a lot of people fail to take into

[270.759 - 276.039] account just how fast AI has advanced

[273.639 - 278.039] just from a year ago so I I like

[276.039 - 280.0] pointing this out so think about where

[278.039 - 282.32] we were a year ago and think about where

[280.0 - 285.039] we are today and then extrapolate that

[282.32 - 286.68] out another year forward a year ago we

[285.039 - 288.88] weren't even talking about multimodal

[286.68 - 290.72] models and now we all have or at least

[288.88 - 293.24] most of us have access to multimodal

[290.72 - 295.759] models um a year ago you weren't even

[293.24 - 298.68] aware of chat GPT cuz it didn't exist

[295.759 - 301.47900000000004] yet we still had only gpt3 now we've got

[298.68 - 303.40000000000003] GPT 4 V Vision we've got mgpt we've got

[301.479 - 308.0] all kinds of other things out there

[303.4 - 310.4] we've got Google's RTX um like guys I

[308.0 - 313.28] can't tell you like how fast things are

[310.4 - 316.4] going so the reason that I bring up

[313.28 - 318.31899999999996] normaly bias as a major risk is because

[316.4 - 319.75899999999996] I'm seeing a lot of normaly bias out

[318.319 - 321.52000000000004] there in the world like obviously if

[319.759 - 323.639] you're watching this channel you are

[321.52 - 325.44] probably ahead of the curve because like

[323.639 - 328.0] you watch me you watch AI explain you

[325.44 - 330.44] might watch Matt wolf and a few others

[328.0 - 333.44] so like you're aware of The Cutting Edge

[330.44 - 335.6] is but as many of you comment a lot of

[333.44 - 337.68] people don't and I actually just saw a

[335.6 - 340.96000000000004] news article that only two out of five

[337.68 - 344.16] teenagers even know what chat GPT is so

[340.96 - 345.84] we're still in the minority um which is

[344.16 - 349.12] really crazy considering how much is

[345.84 - 351.31899999999996] going to change in the next year so this

[349.12 - 352.84000000000003] kind of complacency is a huge risk and

[351.319 - 355.36] this is why I'm making this video so

[352.84 - 357.35999999999996] like share this video or use it to you

[355.36 - 359.6] know get familiar with how to change

[357.36 - 362.319] people's minds whatever all right all

[359.6 - 365.59900000000005] right so first government what can the

[362.319 - 369.40000000000003] government do to adapt to the coming AI

[365.599 - 371.919] wave so there's two primary problems

[369.4 - 374.63899999999995] that I see in the government and this is

[371.919 - 376.4] not just me observing the news um some

[374.639 - 378.36] of my clients and other people that I

[376.4 - 381.19899999999996] work with either are government

[378.36 - 385.12] contractors or government employees um

[381.199 - 388.24] and so the the the the the first aspect

[385.12 - 389.639] is potential widespread unemployment

[388.24 - 391.56] obviously many governments around the

[389.639 - 395.199] world are experimenting with Ubi so

[391.56 - 397.44] that's good um but there's many people

[395.199 - 399.319] within the government that uh that are

[397.44 - 402.12] just not aware of what's coming they're

[399.319 - 403.68] not aware of the narrative or their

[402.12 - 405.96] their current narrative is it's just a

[403.68 - 408.40000000000003] new technology um the second thing that

[405.96 - 410.35999999999996] governments can can do to adapt is to

[408.4 - 411.599] just start adopting AI services for

[410.36 - 414.40000000000003] themselves and I've got examples for

[411.599 - 415.639] both of these but the first big thing is

[414.4 - 417.23999999999995] and this is what I've talked to some

[415.639 - 419.24] government researchers about all over

[417.24 - 422.16] the world I've talked to um government

[419.24 - 424.56] uh policy people in Europe Australia not

[422.16 - 427.36] as many in America for whatever reason

[424.56 - 429.36] but the basically one of the things one

[427.36 - 431.759] of the problems is that convincing

[429.36 - 433.12] people inside of governments of what is

[431.759 - 435.08000000000004] happening is really difficult because

[433.12 - 436.72] they look they like they have that very

[435.08 - 439.12] much normaly bias where they're like

[436.72 - 441.0] well I'm not hearing about you know

[439.12 - 442.72] massive unemployment from AI so what are

[441.0 - 445.24] you talking about it doesn't physically

[442.72 - 447.639] exist right now so I don't think that

[445.24 - 450.68] it's a thing and this is a cognitive

[447.639 - 452.28000000000003] bias that most humans have um so they're

[450.68 - 453.919] not doing much to prepare for post-

[452.28 - 455.84] labor economics they're not even

[453.919 - 459.15999999999997] studying it because the current economic

[455.84 - 461.0] Doctrine is neoliberalism which says you

[459.16 - 463.40000000000003] should aim for around 3 to 5%

[461.0 - 464.759] unemployment but otherwise let the free

[463.4 - 466.75899999999996] market you know make up its own

[464.759 - 469.759] decisions and so we're basically going

[466.759 - 471.639] to have an entirely new uh economic

[469.759 - 473.639] Paradigm that's coming that many people

[471.639 - 475.319] are not preparing for now the fact that

[473.639 - 477.72] some governments are experimenting with

[475.319 - 479.40000000000003] Ubi uh tells me that maybe they're

[477.72 - 482.96000000000004] trying to hedge their bets but they're

[479.4 - 484.71999999999997] not talking about it um and as many of

[482.96 - 487.12] uh you commenters have pointed out in

[484.72 - 490.0] the past the current system breaks down

[487.12 - 492.56] once you get above like 20 to 30 uh%

[490.0 - 494.08] unemployment I think what there's like a

[492.56 - 496.56] there's a predicted threshold I think

[494.08 - 498.52] it's like 35% unemployment once you get

[496.56 - 500.72] to that like you're basically like

[498.52 - 502.75899999999996] Society collapses or something now

[500.72 - 504.52000000000004] obviously it'll take a while to get to

[502.759 - 508.24] get there and right now unemployment is

[504.52 - 510.159] very low so post- labor economics as I'm

[508.24 - 512.039] talking about this this is basically

[510.159 - 515.24] it's not just automation it's not just

[512.039 - 516.36] saying okay well machines are able to do

[515.24 - 518.0] a couple things and they're going to

[516.36 - 520.479] display some stuff but humans are just

[518.0 - 522.88] intrinsically exceptional and or

[520.479 - 525.6800000000001] intrinsically different uh no post-

[522.88 - 528.4399999999999] labor economics is a fundamental shift

[525.68 - 530.8] in the way that labor is done and so

[528.44 - 533.839] there's I've characterized it into a

[530.8 - 536.1999999999999] couple of subcategories so it's a it's

[533.839 - 537.9590000000001] more than just automation automation is

[536.2 - 539.519] actually like that's one way you can

[537.959 - 543.1999999999999] characterize it but your basically

[539.519 - 545.36] automating almost everything away so one

[543.2 - 549.0] thing to keep in mind is human preferred

[545.36 - 551.32] jobs so this is a demand side look at it

[549.0 - 553.48] which is like okay supply side so if you

[551.32 - 555.44] look at Labor Supply labor Supply is

[553.48 - 556.64] what are the workers that are out there

[555.44 - 558.9590000000001] what is their training what is their

[556.64 - 561.36] capabilities and so on but if machines

[558.959 - 563.959] are able to satisfy all Supply if

[561.36 - 566.0790000000001] they're able to do uh work better faster

[563.959 - 569.7199999999999] cheaper and safer than humans then the

[566.079 - 571.04] supply side is fully uh fully occupied

[569.72 - 573.0] so then you have to look at the demand

[571.04 - 575.3199999999999] side which the demand side is what are

[573.0 - 577.56] people willing to pay for and so there

[575.32 - 580.24] will always be a few jobs that humans

[577.56 - 583.1999999999999] will just want other humans to be doing

[580.24 - 585.6] like influencers and content creators um

[583.2 - 588.0790000000001] if you're watching me and not watching

[585.6 - 590.0] one of the you know AI generated

[588.079 - 593.7199999999999] channels then it's like you prefer a

[590.0 - 597.04] real human so there will always be some

[593.72 - 598.5600000000001] some uh human preferred sectors there uh

[597.04 - 602.64] and what we're going to be seeing is a

[598.56 - 605.3599999999999] very skewed labor market so basically uh

[602.64 - 607.56] as as machines are able to do more human

[605.36 - 610.04] jobs companies are just going to prefer

[607.56 - 612.079] to use machines to do those jobs why it

[610.04 - 614.1999999999999] makes economic sense and that push for

[612.079 - 616.16] Innovation is actually part of

[614.2 - 617.76] neoliberalism so if you're a government

[616.16 - 619.7199999999999] employee or government contractor and

[617.76 - 621.88] you're watching this what you need to

[619.72 - 624.9200000000001] understand is that neoliberalism is a

[621.88 - 626.959] push for efficiency it is a system that

[624.92 - 629.24] is put in place that incentivizes

[626.959 - 632.56] increasing efficiency that's exactly

[629.24 - 634.839] what we're aiming for and so if you also

[632.56 - 636.16] in the government and you are trying to

[634.839 - 637.48] say like okay well what are we looking

[636.16 - 639.3199999999999] for you're going to look for high

[637.48 - 641.32] unemployment and you're also going to

[639.32 - 643.5600000000001] look for declining total labor force

[641.32 - 645.88] participation rate so if you look at

[643.56 - 648.7199999999999] charts total labor force participation

[645.88 - 651.24] rate has it peaked in America at around

[648.72 - 652.76] 66% and it has never recovered since the

[651.24 - 654.92] Great Recession I think is when it it

[652.76 - 657.48] kind of peaked um and then it dropped

[654.92 - 659.959] even further during the pandemic and it

[657.48 - 662.32] is kind of slowly recovering but I think

[659.959 - 664.0] that we are permanently on a downward

[662.32 - 666.24] Trend where toal total labor

[664.0 - 669.48] participation Force rates are just going

[666.24 - 671.0] to be slowly deflating from here on out

[669.48 - 673.6] and I think that unemployment is

[671.0 - 674.839] actually not going to be the best metric

[673.6 - 677.0] because some people are just checking

[674.839 - 679.12] out of the economy for good we're going

[677.0 - 682.12] to need a new social contract so what is

[679.12 - 685.279] a social contract the social contract is

[682.12 - 687.839] basically the uh the unspoken or spoken

[685.279 - 690.48] agreement between all the pillars of

[687.839 - 692.6] society so in the past the pillars of

[690.48 - 694.76] society might have been the church and

[692.6 - 697.12] the monarchy and The Peasants and

[694.76 - 699.24] whatever else today the three pillars of

[697.12 - 702.44] society are government business and the

[699.24 - 706.0] people so the power the balance of power

[702.44 - 708.519] is going to be disrupted uh because

[706.0 - 711.2] labor force is going to be on the

[708.519 - 714.079] decline uh labor power is going to be

[711.2 - 716.279] fully disrupted by this and so worker

[714.079 - 719.04] power is going to be completely thrown

[716.279 - 720.519] out the window uh or by and large turn

[719.04 - 722.5999999999999] out the window and you already see this

[720.519 - 725.12] with some of the Union stuff so the like

[722.6 - 727.2] the the Hollywood writer strike that is

[725.12 - 730.04] an early example of people kind of

[727.2 - 733.5600000000001] rebelling against the inevitable change

[730.04 - 735.56] um towards AI Hollywood writers like yes

[733.56 - 737.279] human writers are still better than AI

[735.56 - 738.8389999999999] in some respects although if you watch

[737.279 - 740.72] some movies coming out of Hollywood it's

[738.839 - 743.2790000000001] like yeah AI could do a better job than

[740.72 - 746.0790000000001] this so like fight it as you as you

[743.279 - 748.519] might but if we're going to have uh you

[746.079 - 751.76] know personalized movies and books and

[748.519 - 753.12] AI generated content all over and yes

[751.76 - 755.519] some people will still prefer to see

[753.12 - 759.32] Hollywood actors but many people won't

[755.519 - 761.88] um so that that uh that is evidence of

[759.32 - 763.6] the erosion of worker power and so what

[761.88 - 766.76] we're going to need to do is negotiate a

[763.6 - 768.839] new balance of power which is how do we

[766.76 - 771.12] uh reshape the government and business

[768.839 - 774.12] and everything else to ensure that we

[771.12 - 776.12] don't end up in a dystopian hellscape uh

[774.12 - 778.519] next up is government AI adoption so I

[776.12 - 780.68] mentioned this and that is that uh you

[778.519 - 783.68] know the government will need to adopt

[780.68 - 786.959] AI products and services and actually um

[783.68 - 790.4399999999999] I have a a connection on LinkedIn um who

[786.959 - 793.3599999999999] who runs a startup and this startup he

[790.44 - 795.72] has a really great track record of

[793.36 - 798.8000000000001] basically accelerating government uh

[795.72 - 800.5600000000001] programs and so in this one case uh he

[798.8 - 803.68] he had a contract with the Veterans

[800.56 - 806.5189999999999] Affairs office the VA and they were able

[803.68 - 809.3599999999999] to clear a three-year backlog of medical

[806.519 - 811.92] paperwork in a matter of weeks using

[809.36 - 813.44] generative Ai and so this is an example

[811.92 - 815.7199999999999] that I've been wanting to share for a

[813.44 - 818.9590000000001] while because what I want to do is show

[815.72 - 821.12] that like yes AGI is going to change

[818.959 - 823.8389999999999] everything but AI today can start

[821.12 - 825.88] changing stuff now and so this is why I

[823.839 - 827.8800000000001] created this graphic I was like imagine

[825.88 - 831.16] that the government is as lean and fast

[827.88 - 833.04] as a Ferrari but the idea is that uh

[831.16 - 835.36] sticking your head in the sand and

[833.04 - 838.0] ignoring Ai and keeping the government

[835.36 - 840.6] in the current status quo is a bad idea

[838.0 - 842.759] what we need to be doing is adopting AI

[840.6 - 845.8000000000001] so that the government becomes leaner

[842.759 - 848.32] faster less corrupt more transparent

[845.8 - 850.8389999999999] more representative of people's actual

[848.32 - 852.9200000000001] best interests the story from ask Sage

[850.839 - 855.0400000000001] is really good one I was able to chat

[852.92 - 857.16] with um with the the founder for a

[855.04 - 859.16] little bit and he said oh yeah like

[857.16 - 861.24] everything that we're doing none of it

[859.16 - 863.48] was possible just a couple of years ago

[861.24 - 866.24] because that's how fast AI has has

[863.48 - 869.279] advanced so first mover

[866.24 - 871.279] Advantage uh one thing that has uh been

[869.279 - 872.8] more and more talked about is Ubi so

[871.279 - 875.32] Universal basic income but also

[872.8 - 876.88] Universal basic services and what I

[875.32 - 878.72] realized after talking to a bunch of

[876.88 - 882.0] people is that we already actually have

[878.72 - 885.12] a a decent model of basic services in

[882.0 - 887.639] the form of K through2 public education

[885.12 - 889.44] so you know obviously like there's also

[887.639 - 890.72] roads are maintained by the state and

[889.44 - 893.0400000000001] there's there's lots of other things

[890.72 - 894.399] that are maintained by the state but uh

[893.04 - 897.68] the school system is the only thing that

[894.399 - 899.759] is universal we literally have a federal

[897.68 - 901.8389999999999] mandate that every child is entitled to

[899.759 - 903.88] an education and we figured that out we

[901.839 - 907.0400000000001] run the school system now obviously when

[903.88 - 909.8] you look at problems like um inequality

[907.04 - 911.4399999999999] and blooms to Sigma problems uh you know

[909.8 - 913.959] our public schooling system is not

[911.44 - 916.6800000000001] perfect um but it does serve every

[913.959 - 919.399] single child and so this is actually a a

[916.68 - 921.7199999999999] decent enough model for Universal basic

[919.399 - 923.8] services so how is this going to look

[921.72 - 925.9200000000001] now obviously a lot of people say like

[923.8 - 928.04] yeah good luck living on $1,200 a month

[925.92 - 929.319] from the government and I agree that's

[928.04 - 931.92] pretty slim

[929.319 - 934.319] uh I I would not want to live on only

[931.92 - 936.399] $1,200 a month from the government uh

[934.319 - 938.12] now however there are many levels of

[936.399 - 941.36] government there is federal there is

[938.12 - 944.199] State there is County there is Municipal

[941.36 - 947.44] and so what I anticipate actually is

[944.199 - 950.68] that we will have a multi-layered set of

[947.44 - 953.399] safety nets that are a a combination of

[950.68 - 955.56] Ubi and UBS and so we might have a

[953.399 - 958.8] federal Ubi basically you know federal

[955.56 - 960.5189999999999] taxes tax the companies that are uh

[958.8 - 964.7199999999999] churning out trillions of dollars worth

[960.519 - 967.399] of value with AI That's that would be

[964.72 - 969.279] appropriate um you tax and redistribute

[967.399 - 970.92] that uh then you also do the same thing

[969.279 - 973.279] at the state level but then also at the

[970.92 - 976.399] state and County and Municipal level you

[973.279 - 978.319] provide basic services like hospitals

[976.399 - 981.6] you provide basic services like power

[978.319 - 984.7199999999999] food uh maybe even housing in many cases

[981.6 - 986.16] and so uh this is one thing that that

[984.72 - 989.0400000000001] really gives me a lot of uh

[986.16 - 990.639] encouragement is because many of the

[989.04 - 992.8389999999999] experiments going on around the world

[990.639 - 994.839] about Ubi are actually being run by

[992.839 - 996.5600000000001] cities or states or counties or

[994.839 - 999.8800000000001] provinces they're not being run at the

[996.56 - 1002.04] federal level and so you combine AI

[999.88 - 1004.24] efficiency with some of these

[1002.04 - 1006.759] multi-layered approaches and I think

[1004.24 - 1009.88] that what we're going to have is H this

[1006.759 - 1012.0] this multi-layered safety net of Ubi and

[1009.88 - 1013.759] UBS and I think that we're actually

[1012.0 - 1015.36] going to end up in a much better place

[1013.759 - 1016.759] than we are today certainly here in

[1015.36 - 1019.6800000000001] America because in America it's like

[1016.759 - 1021.759] you're on your own scrub get good

[1019.68 - 1025.24] okay so let's move on to businesses what

[1021.759 - 1027.64] can businesses do to adapt to AI well

[1025.24 - 1030.6] first and foremost you have to recognize

[1027.64 - 1033.679] that this is the most significant uh

[1030.6 - 1036.1989999999998] paradigm shift in all of human history

[1033.679 - 1038.3190000000002] uh both economically and technologically

[1036.199 - 1040.64] this is nothing short of the fourth

[1038.319 - 1043.319] Industrial Revolution probably the

[1040.64 - 1045.0790000000002] biggest one yet this is going to change

[1043.319 - 1047.0] the competitive landscape which

[1045.079 - 1049.039] basically means if you're not adopting

[1047.0 - 1052.4] AI your competitors are so if you're not

[1049.039 - 1054.28] adopting AI good luck um I've actually

[1052.4 - 1056.5590000000002] heard I don't have any clients obviously

[1054.28 - 1057.84] if someone comes to me they want AI but

[1056.559 - 1060.6399999999999] I've heard of people out there that are

[1057.84 - 1062.1999999999998] like no AI is just a fad it's just a

[1060.64 - 1063.64] hype cycle we're not going to invest

[1062.2 - 1066.4] we're going to let it blow over and I'm

[1063.64 - 1068.48] like okay we'll see how that plays out

[1066.4 - 1071.52] uh now another thing about this this

[1068.48 - 1073.0] changing landscape is some businesses I

[1071.52 - 1074.6] think are going to see thinning margins

[1073.0 - 1077.0] and that is just because the marginal

[1074.6 - 1079.8799999999999] cost of using AI to do some things is

[1077.0 - 1082.0] near zero which means means if you know

[1079.88 - 1084.5200000000002] if the if your margin goes from 50% to

[1082.0 - 1087.28] 1% you can't skim anything off the top

[1084.52 - 1088.6399999999999] for yourself and so I think that um and

[1087.28 - 1091.32] we've already seen this with some

[1088.64 - 1094.2] sectors collapsing such as the creative

[1091.32 - 1096.36] sector such as uh customer service jobs

[1094.2 - 1098.96] and then also HR we've seen a lot of

[1096.36 - 1100.36] layoffs in the HR departments and and I

[1098.96 - 1102.559] think it was Sachi Nadella one of the

[1100.36 - 1104.52] CEOs or maybe it was um what's his name

[1102.559 - 1105.8799999999999] at IBM anyways he predicted that back

[1104.52 - 1109.1589999999999] office workers are going to be the first

[1105.88 - 1111.7990000000002] to go those are white collar jobs and so

[1109.159 - 1115.1200000000001] any companies that serve HR purposes any

[1111.799 - 1117.6399999999999] companies that serve customer service uh

[1115.12 - 1119.7199999999998] uh positions any companies that focus on

[1117.64 - 1122.24] copywriting and image and images and

[1119.72 - 1123.799] that sort of stuff they're all gone and

[1122.24 - 1126.159] so that's that's what thinning margins

[1123.799 - 1128.4] results in is companies collapsing just

[1126.159 - 1130.3200000000002] because there's no room left to scrape

[1128.4 - 1131.76] out this can also be called creative

[1130.32 - 1134.32] destruction where it's basically an

[1131.76 - 1136.52] entire sector is just invalidated by a

[1134.32 - 1138.76] new technology either way there's going

[1136.52 - 1141.24] to be a lot of companies that collapse

[1138.76 - 1143.0] uh and then investing in AI both in in

[1141.24 - 1144.84] terms of products and services and

[1143.0 - 1147.28] developing AI Talent which will unpack

[1144.84 - 1149.12] more right now so the first thing that

[1147.28 - 1152.96] you need to know as a business is adapt

[1149.12 - 1155.28] or die uh this is the same as companies

[1152.96 - 1158.2] that you know that were late to adopt uh

[1155.28 - 1159.76] electrification and the internet uh many

[1158.2 - 1161.88] of them went out of business and those

[1159.76 - 1164.6] that were able to Pivot and adapt some

[1161.88 - 1166.44] of them are still around like IBM what

[1164.6 - 1169.36] you might not realize about IBM is that

[1166.44 - 1171.76] they started as mechanical time clocks

[1169.36 - 1173.6789999999999] um and so obviously IBM now working on

[1171.76 - 1175.4] Quantum Computing they have pivoted

[1173.679 - 1178.3200000000002] several times which is why they've been

[1175.4 - 1181.159] around for more than 100 years uh many

[1178.32 - 1183.36] of their competitors no longer exist uh

[1181.159 - 1186.4] another example is borders borders

[1183.36 - 1188.1999999999998] failed to Pivot to adapt to the internet

[1186.4 - 1191.0] and so what were they replaced by Barnes

[1188.2 - 1193.24] & Noble and Amazon again we do have

[1191.0 - 1195.96] living memory of companies failing to

[1193.24 - 1198.52] adapt to the internet and therefore

[1195.96 - 1201.4] collapsing uh so I'm here to tell you AI

[1198.52 - 1203.12] is not hype it will change everything

[1201.4 - 1205.3200000000002] and even if you look at like well it's

[1203.12 - 1208.36] not capable of stuff today look at what

[1205.32 - 1210.4399999999998] it was capable of last year which in

[1208.36 - 1211.9189999999999] comparison AI was not capable of hardly

[1210.44 - 1214.64] anything this time last year and now

[1211.919 - 1217.679] it's like well a lot of us use AI every

[1214.64 - 1219.4] day all day to do our work and so this

[1217.679 - 1223.3600000000001] time next year you're not going to be

[1219.4 - 1225.44] able to get away without using AI so you

[1223.36 - 1227.84] you will adapt or you will die it's that

[1225.44 - 1230.44] simple and I'm also here to tell you

[1227.84 - 1232.12] that like this is kind of bad news but

[1230.44 - 1233.76] AI is going to destroy a lot of jobs and

[1232.12 - 1237.0] a lot of businesses and there's not much

[1233.76 - 1239.2] we can do about it uh another thing that

[1237.0 - 1241.679] we're that we're already seeing is an AI

[1239.2 - 1244.799] Talent crunch so the beginning of my

[1241.679 - 1246.6000000000001] career back in 2007 was as the tech

[1244.799 - 1249.48] industry was recovering from the.com

[1246.6 - 1251.32] Revolution and crash and so you know

[1249.48 - 1253.6] there was I remember growing up hearing

[1251.32 - 1254.84] like there are 50,000 unfilled IT jobs

[1253.6 - 1257.32] so I was like okay I guess I'll get an

[1254.84 - 1258.8799999999999] IT job and it was really easy because

[1257.32 - 1261.6789999999999] there was such a huge huge vacuum there

[1258.88 - 1263.48] was a huge need for it people it's like

[1261.679 - 1265.3200000000002] oh hey you can program a home router

[1263.48 - 1267.88] great you've got a job it wasn't quite

[1265.32 - 1269.36] that easy but it was close uh and so

[1267.88 - 1272.4] we're seeing the same thing with AI

[1269.36 - 1274.6399999999999] Talent right now ai is the new it AI is

[1272.4 - 1277.1200000000001] the new software engineering uh I

[1274.64 - 1280.24] remember my mom's boyfriend back in the

[1277.12 - 1282.08] uh like early 2000s like he came back

[1280.24 - 1283.72] from the Army got a Microsoft

[1282.08 - 1286.1999999999998] certification and immediately had it

[1283.72 - 1288.4] like back then man it was easy you get

[1286.2 - 1291.3600000000001] one Microsoft certification in instant

[1288.4 - 1294.3200000000002] job that pays 50,000 a year uh you get a

[1291.36 - 1295.84] handful of certifications 80,000 a year

[1294.32 - 1298.52] uh and your companies would pay for it

[1295.84 - 1300.32] too uh most of most of my training at

[1298.52 - 1302.96] the beginning of my career paid for by

[1300.32 - 1307.559] my company because there was so little

[1302.96 - 1309.08] Talent um now one thing that uh is not

[1307.559 - 1310.8799999999999] necessarily possible because a lot of

[1309.08 - 1313.3999999999999] the training programs don't even exist

[1310.88 - 1316.4] yet is you need to focus on your

[1313.4 - 1320.159] internal Talent Development so for the

[1316.4 - 1322.3600000000001] last 10 to 15 years we've had this glut

[1320.159 - 1324.679] of in the labor market in the technology

[1322.36 - 1326.279] space where pretty much any anything you

[1324.679 - 1328.48] need you know if you need a databased

[1326.279 - 1330.32] administrator you need uh you know C

[1328.48 - 1332.32] developer there was someone out there

[1330.32 - 1334.6] and so companies have gotten really lazy

[1332.32 - 1336.52] about Talent Development like I said 15

[1334.6 - 1338.799] years ago at the beginning of my career

[1336.52 - 1340.44] all of uh all of my talent development

[1338.799 - 1342.32] was paid for by my company because it

[1340.44 - 1343.919] wasn't there in the last 10 years

[1342.32 - 1345.6399999999999] companies have gotten really lazy and so

[1343.919 - 1348.4] what I've had to do with some of my

[1345.64 - 1350.2] clients is say hey the thing the the

[1348.4 - 1352.5590000000002] skills and experience that you want to

[1350.2 - 1354.64] hire for isn't out there you need to

[1352.559 - 1356.6789999999999] develop it so like one of my clients was

[1354.64 - 1358.3600000000001] basically saying like Hey how do we find

[1356.679 - 1359.72] someone who is a director who

[1358.36 - 1362.4399999999998] understands like cognitive architecture

[1359.72 - 1363.679] I'm like you don't because the only

[1362.44 - 1365.799] people who understand cognitive

[1363.679 - 1367.919] architecture are a few of us lunatics

[1365.799 - 1369.799] out here and then like phds that have

[1367.919 - 1371.159] been studying it since the 80s but

[1369.799 - 1374.32] they're not looking for a you know

[1371.159 - 1376.159] director of technology job so I was like

[1374.32 - 1378.48] what you do what you need to do then is

[1376.159 - 1379.919] you hire for personality and the two

[1378.48 - 1382.2] primary traits that I recommend that

[1379.919 - 1383.5590000000002] people hire for is hunger I don't mean

[1382.2 - 1386.6000000000001] like physical hunger I mean people that

[1383.559 - 1389.3999999999999] are hungry for AI and uh and and hungry

[1386.6 - 1391.84] for the job and then curiosity um

[1389.4 - 1394.64] because those are two inborn traits uh

[1391.84 - 1396.6789999999999] and then then you backfill in with the

[1394.64 - 1397.88] talent and skills and other things that

[1396.679 - 1400.1200000000001] can be trained in because you can't

[1397.88 - 1401.64] train someone to be hungry for their for

[1400.12 - 1404.4799999999998] their job you can't train someone to be

[1401.64 - 1407.64] curious those are just fixed personality

[1404.48 - 1410.039] traits so you focus on those two things

[1407.64 - 1413.039] and develop your own internal Talent

[1410.039 - 1415.08] companies that that go all in on AI

[1413.039 - 1417.36] people that companies that focus on the

[1415.08 - 1419.039] AI Talent people that develop their own

[1417.36 - 1422.039] internal AI Talent they're going to have

[1419.039 - 1424.2] a much better uh chance at surviving

[1422.039 - 1427.64] this coming AI wave than those that

[1424.2 - 1429.0800000000002] don't and mark my word there are plenty

[1427.64 - 1430.44] of companies out there that are

[1429.08 - 1433.4399999999998] basically sticking their head in the

[1430.44 - 1435.48] sand and ignoring uh the AI stuff so if

[1433.44 - 1438.0800000000002] you uh if you're watching this I know

[1435.48 - 1440.279] that there's a lot of uh seite people

[1438.08 - 1442.4399999999998] and Senior vice presidents and other

[1440.279 - 1444.44] folks that watch like if you're watching

[1442.44 - 1446.48] this you are ahead of the curve now I'm

[1444.44 - 1449.039] not saying that it's going to be easy CU

[1446.48 - 1451.76] like I said there are macroscopic

[1449.039 - 1454.0] economic macroeconomic forces at play

[1451.76 - 1456.0] here that could just cut out your legs

[1454.0 - 1457.84] from underneath you so I know that's a

[1456.0 - 1459.6] really Grim Outlook but like that's how

[1457.84 - 1462.12] it is and that is how every industrial

[1459.6 - 1463.279] revolution has happened up until this

[1462.12 - 1466.0] point and this one's going to be no

[1463.279 - 1468.279] different in that respect okay so

[1466.0 - 1471.44] finally on a personal level how can you

[1468.279 - 1472.72] brace for impact first and foremost is

[1471.44 - 1475.3200000000002] emotional

[1472.72 - 1478.0] adaptation it is Perfectly Natural to

[1475.32 - 1480.279] have a very wide range of emotions as

[1478.0 - 1481.799] you contemplate these changes just last

[1480.279 - 1483.48] night it took me like an hour and a half

[1481.799 - 1486.279] to get to sleep because I didn't know

[1483.48 - 1489.2] what I was feeling I was like am I angry

[1486.279 - 1492.08] about this am I hopeful am I worried

[1489.2 - 1494.0800000000002] like it was just like just this poper of

[1492.08 - 1496.24] random nebulous emotions because I'm

[1494.08 - 1498.08] like man like because I was making the

[1496.24 - 1499.919] slide deck last night and so I'm like

[1498.08 - 1503.24] you know I need to take my own advice

[1499.919 - 1505.8400000000001] here anyways this is all natural um when

[1503.24 - 1507.679] you're facing a huge paradigm shift it

[1505.84 - 1510.0] is natural to have the occasional

[1507.679 - 1512.24] existential crisis like this dude at the

[1510.0 - 1514.96] bar being served by a robot

[1512.24 - 1516.919] bartender uh the best analogy that I

[1514.96 - 1519.2] have is it's like you know that there's

[1516.919 - 1520.72] a big storm coming you can prepare for

[1519.2 - 1522.8400000000001] the storm you can brace for the storm

[1520.72 - 1525.48] but eventually the storm's going to get

[1522.84 - 1527.9189999999999] here and all you can do is ride it out

[1525.48 - 1529.96] um and so yeah like that's that and you

[1527.919 - 1532.24] have to acknowledge these emotions you

[1529.96 - 1534.32] have to accept them work with them talk

[1532.24 - 1536.64] about them and so back when I was still

[1534.32 - 1538.559] hosting AI meetups like a lot of people

[1536.64 - 1540.159] were like giddy like it was really

[1538.559 - 1542.1589999999999] validating to have a bunch of people

[1540.159 - 1543.5200000000002] like 20 to 30 people like at the bar

[1542.159 - 1545.8400000000001] together like talking about like yeah

[1543.52 - 1547.76] this is so exciting and so like it was

[1545.84 - 1549.9189999999999] actually very normalizing for us to like

[1547.76 - 1552.44] feel sane like yeah we're not making

[1549.919 - 1553.76] this up right like so many of the

[1552.44 - 1555.48] conversations were like we're checking

[1553.76 - 1557.399] in with each other like are we crazy or

[1555.48 - 1558.6] are we just like have the lunatics taken

[1557.399 - 1560.4399999999998] over the the assignment it could be a

[1558.6 - 1562.0] little bit of both but it was really

[1560.44 - 1565.1200000000001] validating to get together with a bunch

[1562.0 - 1568.12] of other people and say like hey this is

[1565.12 - 1570.2399999999998] happening right yes cool okay now

[1568.12 - 1572.399] what so and that's why I have an

[1570.24 - 1573.559] existential coping uh channel in my

[1572.399 - 1575.6] Discord

[1573.559 - 1577.48] server another thing you can do to

[1575.6 - 1580.32] prepare is start moving towards a

[1577.48 - 1582.2] forever job and so a forever job this is

[1580.32 - 1584.6399999999999] the other side of post labor economics

[1582.2 - 1586.799] which is there will always be some jobs

[1584.64 - 1589.3200000000002] that people just want other people to do

[1586.799 - 1592.36] now I'm not saying go start an only fans

[1589.32 - 1593.9189999999999] go start you know your YouTube career um

[1592.36 - 1596.08] I don't know what kind of jobs are going

[1593.919 - 1598.24] to be forever jobs but this is why I'm

[1596.08 - 1600.0] here on YouTube is because as long as

[1598.24 - 1601.36] there are humans I suspect there are

[1600.0 - 1603.12] going to be humans that want to see my

[1601.36 - 1606.32] dumb face talking about AI or whatever

[1603.12 - 1608.6399999999999] it is that I ultimately talk about um so

[1606.32 - 1610.039] but yeah so be be thinking about and

[1608.64 - 1612.3600000000001] talking about like okay what are some

[1610.039 - 1614.52] things that will stick around forever

[1612.36 - 1616.76] and these are the demand side jobs now

[1614.52 - 1618.679] obviously not everyone can become a a

[1616.76 - 1621.0] content creator not everyone wants to

[1618.679 - 1623.3990000000001] get into into child care and so that

[1621.0 - 1626.399] leads to another part which is um

[1623.399 - 1628.039] redefining how you achieve social status

[1626.399 - 1629.6] so I've been reading this book uh it was

[1628.039 - 1631.84] recommended to me by a good friend and

[1629.6 - 1635.6] Mentor called the status game by William

[1631.84 - 1638.9189999999999] store and this book is the single most

[1635.6 - 1642.48] profound uh book on human psychology I

[1638.919 - 1646.159] have ever read it explains so much about

[1642.48 - 1648.1200000000001] The Human Condition um it blows sapiens

[1646.159 - 1650.3200000000002] out of the water um and I know that

[1648.12 - 1652.12] that's like heretical to say but like

[1650.32 - 1655.12] this book is so well researched and it

[1652.12 - 1658.08] is so practical and it is so useful so

[1655.12 - 1661.399] the basically the tldr is that all

[1658.08 - 1664.9189999999999] humans across the entire planet all care

[1661.399 - 1667.4399999999998] about social status Above All Else

[1664.919 - 1670.159] everything else that we care about flows

[1667.44 - 1671.76] from this Central foundational desire

[1670.159 - 1675.1200000000001] and it makes sense we are a social

[1671.76 - 1677.76] species your survival is predicated on

[1675.12 - 1680.4399999999998] your social status and your success and

[1677.76 - 1683.2] happiness in life is predicated on your

[1680.44 - 1685.2] social status so I strongly recommend

[1683.2 - 1687.679] you read this G this this game read this

[1685.2 - 1690.32] book um but basically there are three

[1687.679 - 1691.919] kinds of status games that we all play

[1690.32 - 1694.36] um and these status games are also

[1691.919 - 1696.0] rooted in our ancient Evolution because

[1694.36 - 1699.1589999999999] we see them in bonobos we see it in

[1696.0 - 1701.679] chimps we even see it in um in in lions

[1699.159 - 1704.44] and other social uh mammals so there's

[1701.679 - 1707.6000000000001] the success game which is uh often

[1704.44 - 1709.3600000000001] material success or financial success

[1707.6 - 1711.6399999999999] and so this is associated with career

[1709.36 - 1713.08] and conspicuous consumption and one

[1711.64 - 1715.2] piece of evidence for this and everyone

[1713.08 - 1717.519] has seen this is that even toddlers are

[1715.2 - 1720.24] possessive so our Instinct for private

[1717.519 - 1722.519] property is inborn why because if you

[1720.24 - 1725.3990000000001] possess the neat things you have higher

[1722.519 - 1727.039] Social Status and this goes way way way

[1725.399 - 1729.5189999999998] before capitalism way before

[1727.039 - 1732.76] neoliberalism this is deeply embedded in

[1729.519 - 1735.1200000000001] our DNA so material success the the

[1732.76 - 1737.6] possession of cool things the nicer

[1735.12 - 1740.2399999999998] house the cool shells uh you know the

[1737.6 - 1742.6] fancy car this is an evolutionary

[1740.24 - 1745.3990000000001] artifact that is deeply embedded in us

[1742.6 - 1746.6789999999999] now obviously if careers go away like

[1745.399 - 1748.399] okay you're not going to have financial

[1746.679 - 1750.5590000000002] success through your career but there

[1748.399 - 1752.76] will still be ways to have that kind of

[1750.559 - 1755.24] material success next up is the

[1752.76 - 1757.6] dominance game so dominance game is you

[1755.24 - 1760.32] know uh showing your muscles uh

[1757.6 - 1762.48] intimidation fear physical prowess

[1760.32 - 1764.519] sexual prowess that sort of stuff part

[1762.48 - 1768.039] of the dominance game you see this more

[1764.519 - 1770.559] often with um militaries Sports

[1768.039 - 1772.279] um and you know people that like to pump

[1770.559 - 1774.279] iron and get on the beach and you know

[1772.279 - 1776.12] winning right you know dominance is all

[1774.279 - 1778.6] about winning it's about conquering your

[1776.12 - 1781.0] enemies um and then there the the third

[1778.6 - 1783.9189999999999] game is the virtue or Prestige game

[1781.0 - 1786.039] which this is more about like um people

[1783.919 - 1788.919] that are either spiritual or

[1786.039 - 1791.08] intellectual or emotional um and so

[1788.919 - 1793.96] there's depending on which kind of

[1791.08 - 1796.12] Virtues you prefer this is something

[1793.96 - 1798.44] that uh like I guess what I'm trying to

[1796.12 - 1800.4399999999998] say is like say for instance you are a

[1798.44 - 1802.519] very religious person part of your

[1800.44 - 1804.44] virtue is how well you adhere to your

[1802.519 - 1806.679] religious Doctrine that's another way to

[1804.44 - 1808.64] get social status or if you're an

[1806.679 - 1810.72] academic if you're a researcher part of

[1808.64 - 1812.8400000000001] the virtue or Prestige is about

[1810.72 - 1815.799] intellectual contribution so that's

[1812.84 - 1817.6399999999999] another example of the status game by

[1815.799 - 1819.799] recognizing that that your social

[1817.64 - 1821.44] standing comes from multiple Dimensions

[1819.799 - 1824.799] because everyone plays a little bit of

[1821.44 - 1827.6000000000001] all three of these games everyone does

[1824.799 - 1829.519] cares about some material success

[1827.6 - 1831.24] everyone cares about some dominance and

[1829.519 - 1833.039] everyone cares about some virtue or

[1831.24 - 1835.36] Prestige it's just a matter of which one

[1833.039 - 1837.64] you prefer and what your particular

[1835.36 - 1839.08] social tribe prefers as well but

[1837.64 - 1841.159] everyone does all three and so

[1839.08 - 1843.84] recognizing and learning about social

[1841.159 - 1846.1200000000001] status is one of the number one ways to

[1843.84 - 1848.32] prepare for and adapt for the

[1846.12 - 1850.4399999999998] possibility of post- labor economics and

[1848.32 - 1852.32] no longer having a career after reading

[1850.44 - 1854.519] this book I'm like oh man nobody needs a

[1852.32 - 1857.32] career there are so many other ways to

[1854.519 - 1858.44] establish your your social Rank and to

[1857.32 - 1861.36] um make sure that you have social

[1858.44 - 1864.0] standing in your tribe um yeah no we're

[1861.36 - 1866.84] going to be fine there another thing you

[1864.0 - 1868.679] can do is as you have more time because

[1866.84 - 1870.519] either you lose your job and there's

[1868.679 - 1871.96] plenty of people that I talk to some of

[1870.519 - 1873.24] the folks that have volunteered for some

[1871.96 - 1875.039] of the open source projects that I'm

[1873.24 - 1877.6] running they lost their job six months

[1875.039 - 1879.0] ago and like they don't know if they're

[1877.6 - 1881.039] ever going to have a job again and

[1879.0 - 1884.88] they're trying to find their way back in

[1881.039 - 1887.32] um but uh as we pivot to post- labor

[1884.88 - 1890.48] economics as people lose their jobs or

[1887.32 - 1893.399] only have gig work or whatever um one of

[1890.48 - 1896.3600000000001] the most powerful Frameworks for living

[1893.399 - 1898.32] happily on a day-to-day basis is um Dr

[1896.36 - 1901.12] Walsh's TLC therapeutic lifestyle

[1898.32 - 1903.6] changes it is a it is a framework of

[1901.12 - 1906.279] eight behaviors that you can engage in

[1903.6 - 1908.039] on a daily or weekly basis that really

[1906.279 - 1909.84] really helps you feel better but

[1908.039 - 1911.36] basically it's like okay so a TLC

[1909.84 - 1913.4399999999998] specialist is going to say all right

[1911.36 - 1915.799] let's fix your exercise let let's fix

[1913.44 - 1917.2] your your diet and nutrition let's fix

[1915.799 - 1918.8799999999999] your relationships let's make sure

[1917.2 - 1920.0] you're spending time in nature let's

[1918.88 - 1922.48] make sure that you're doing your

[1920.0 - 1924.08] recreational stuff your fun your hobbies

[1922.48 - 1926.6390000000001] let's make sure that you're getting some

[1924.08 - 1927.96] relaxation um and Stress Management

[1926.639 - 1929.8799999999999] let's make sure that you're engaging

[1927.96 - 1931.2] with your religious or spiritual side

[1929.88 - 1933.519] and then finally let's make sure that

[1931.2 - 1936.039] you are um engaging in some kind of

[1933.519 - 1938.2] community service or giving back so I

[1936.039 - 1939.84] learned about TLC many many years ago

[1938.2 - 1942.48] way back on the days of stumble upon

[1939.84 - 1945.039] which doesn't even exist anymore um but

[1942.48 - 1946.3600000000001] this framework is super helpful and it's

[1945.039 - 1948.2] just like a checklist you can just go

[1946.36 - 1950.6] through this every day and say like hey

[1948.2 - 1952.919] can I invest some time in this cool and

[1950.6 - 1955.32] you do this and you'll feel better I

[1952.919 - 1958.24] promise another thing and this has been

[1955.32 - 1960.36] kind of uh personal insight as I am

[1958.24 - 1964.039] acclimating to this change that we're

[1960.36 - 1965.799] facing is forget about tomorrow and you

[1964.039 - 1967.399] know you you might say like Dave you're

[1965.799 - 1968.9189999999999] always predicting like you know into the

[1967.399 - 1971.2399999999998] future you're obviously not forgetting

[1968.919 - 1973.519] about tomorrow and what I mean by this

[1971.24 - 1974.679] is no I'm not forgetting about tomorrow

[1973.519 - 1977.2] like not in that sense but I'm not

[1974.679 - 1979.039] worrying about tomorrow it's like okay I

[1977.2 - 1981.24] see everything that everyone is doing

[1979.039 - 1983.519] like I go on archive on a regular basis

[1981.24 - 1985.88] and I see that there are literally like

[1983.519 - 1988.24] thousands upon thousands of scientists

[1985.88 - 1991.72] hundreds of labs and universities all

[1988.24 - 1995.48] over the world working on AI alignment

[1991.72 - 1997.919] and bias and safety and and I'm like

[1995.48 - 1999.48] okay the collective brain power of those

[1997.919 - 2001.72] like literally tens of thousands of

[1999.48 - 2003.2] students and researchers so I'm pretty

[2001.72 - 2005.919] sure we're going to figure out safety

[2003.2 - 2009.039] like this is a solvable problem and we

[2005.919 - 2011.159] will solve it uh and then on the on the

[2009.039 - 2012.8799999999999] other side there is the geopolitical and

[2011.159 - 2015.3600000000001] governmental side there's the business

[2012.88 - 2017.5590000000002] side and it's like okay well you know as

[2015.36 - 2019.279] I've covered in some of my recent videos

[2017.559 - 2024.399] all the halls of Power are talking about

[2019.279 - 2027.0] AI the EU the UN NATO America like a Cen

[2024.399 - 2029.0] like literally every International

[2027.0 - 2030.84] Organization on the planet is aware of

[2029.0 - 2033.24] AI and they are doing something about it

[2030.84 - 2035.639] now I might not agree with their closed

[2033.24 - 2037.44] door meetings I might not agree with

[2035.639 - 2038.84] exactly how they're conducting it but

[2037.44 - 2040.8400000000001] that's why I'm commenting on it but the

[2038.84 - 2042.4399999999998] fact of the matter is there are adults

[2040.84 - 2044.84] in the room and they are working on this

[2042.44 - 2046.3600000000001] stuff and so it's like you know what I'm

[2044.84 - 2047.8799999999999] doing everything that I can so I'm going

[2046.36 - 2049.879] to let go of the worry and I'm just

[2047.88 - 2053.28] going to forget about tomorrow and enjoy

[2049.879 - 2055.359] today so like adopting a carpedm uh

[2053.28 - 2057.96] mentality and just trusting that it'll

[2055.359 - 2060.359] work out and I don't mean blind trust

[2057.96 - 2062.2] I'm I never one to uh to argue for blind

[2060.359 - 2063.96] trust I keep my ear to the ground but

[2062.2 - 2066.0] it's like yeah I think generally things

[2063.96 - 2068.2] are going in the right direction and

[2066.0 - 2069.639] then finally simplif your life this is

[2068.2 - 2072.24] something that I realize that I've been

[2069.639 - 2075.359] doing uh lately which is just you know

[2072.24 - 2077.919] the rat race is ending um you know the

[2075.359 - 2079.7599999999998] the the competition for a better job and

[2077.919 - 2081.919] you know the the fancy house on the

[2079.76 - 2085.8] culdesac like yeah we can still want

[2081.919 - 2088.7999999999997] those things but the current American

[2085.8 - 2090.7200000000003] dream has like it's it's it's on its

[2088.8 - 2093.2000000000003] deathbed and it's just circling the

[2090.72 - 2097.52] drain and it's going to go any minute

[2093.2 - 2099.52] now and like yes the status game will be

[2097.52 - 2102.16] forever you know so we're going to find

[2099.52 - 2105.0] new ways to compete but this this

[2102.16 - 2108.72] workaholic treadmill that we've been on

[2105.0 - 2110.2] that's what's going away and like the

[2108.72 - 2111.64] the question that my wife and I have

[2110.2 - 2113.0] been asking each other and asking

[2111.64 - 2115.7599999999998] ourselves for the last few months is how

[2113.0 - 2118.359] do we want to live with with all this

[2115.76 - 2121.4] cultural baggage of you know middle

[2118.359 - 2122.96] class America and you know and all that

[2121.4 - 2124.1600000000003] stuff it's going away it's dying and so

[2122.96 - 2127.56] we're like well how do we want to live

[2124.16 - 2129.96] instead and so you know we we have been

[2127.56 - 2131.88] kind of critically re-calibrating our

[2129.96 - 2133.92] own lives and so one thing that I do is

[2131.88 - 2135.48] like when I'm done with work for the day

[2133.92 - 2136.88] sometimes I'll go to see a movie some

[2135.48 - 2138.68] sometimes like we love going to Panera

[2136.88 - 2140.8] after we go hiking and it's just like

[2138.68 - 2143.359] we're engaging with life on our own

[2140.8 - 2145.52] terms and obviously like I recognize not

[2143.359 - 2148.2] everyone can do this yet but I certainly

[2145.52 - 2150.68] recommend that everyone try uh you know

[2148.2 - 2153.16] like whatever your dream is like why not

[2150.68 - 2156.0789999999997] start now if you can um and look forward

[2153.16 - 2157.7999999999997] to that in the future uh and this is why

[2156.079 - 2159.5600000000004] like I'm trying to to thread the needle

[2157.8 - 2162.079] when I talk about post- labor economics

[2159.56 - 2165.68] and preparing for these changes because

[2162.079 - 2167.88] if we all work together if we work hard

[2165.68 - 2170.04] to make sure that AI is implemented in

[2167.88 - 2171.88] the ways that are best for everyone if

[2170.04 - 2173.839] we work with the government and the

[2171.88 - 2176.079] businesses to make sure that you know

[2173.839 - 2178.56] it's not you know heavily skewed or

[2176.079 - 2181.8] lopsided and you know keeps everyone in

[2178.56 - 2183.7599999999998] a cyberpunk dystopian hellscape like

[2181.8 - 2185.88] then there's no re I don't see any

[2183.76 - 2188.28] reason no physical reason no economic

[2185.88 - 2191.1600000000003] reason no Game Theory reason that we all

[2188.28 - 2193.1600000000003] couldn't live in a much more harmonious

[2191.16 - 2195.72] way uh that would be closer to our

[2193.16 - 2197.839] preference very soon and I mean like

[2195.72 - 2199.0] within the next two to five years I

[2197.839 - 2202.24] could see some of these major

[2199.0 - 2204.04] transitions happening which you know the

[2202.24 - 2206.68] like call that accelerationism call it

[2204.04 - 2208.359] whatever you want but like it's not a

[2206.68 - 2211.04] foregone conclusion that it will work

[2208.359 - 2213.52] out that way but I don't see any reason

[2211.04 - 2215.56] that it can't there's no fundamental

[2213.52 - 2217.48] reason that this is not the natural flow

[2215.56 - 2219.44] of things except of course as some

[2217.48 - 2220.839] people will say like the billionaires

[2219.44 - 2222.16] are greedy and they of course want to

[2220.839 - 2226.72] make sure that they control everything

[2222.16 - 2230.359] and I'm like I think maybe some do but I

[2226.72 - 2232.48] I don't know I I see it playing out

[2230.359 - 2234.119] really well anyways thanks for watching

[2232.48 - 2236.4] I hope you got a lot out of this let me

[2234.119 - 2238.1600000000003] know what you think in the comments are

[2236.4 - 2239.76] like how are you preparing is there

[2238.16 - 2243.64] anything that I missed so on and so

[2239.76 - 2243.6400000000003] forth have a good one cheers