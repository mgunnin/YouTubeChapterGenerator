[0.06 - 3.06] is there anything that makes humans

[1.86 - 5.16] Irreplaceable

[3.06 - 7.4399999999999995] so are we getting into a stage where

[5.16 - 9.96] there's nothing that AI that humans can

[7.44 - 12.719000000000001] do that AI won't be able to do

[9.96 - 15.719000000000001] uh from a functional standpoint from a a

[12.719 - 17.698999999999998] an objective standpoint I don't think so

[15.719 - 19.14] um and that actually begs a very deep

[17.699 - 20.520000000000003] philosophical and spiritual question

[19.14 - 23.64] which is what is the point of living

[20.52 - 25.68] what is the point of being a human

[23.64 - 28.199] um and that uh is something that I've

[25.68 - 31.259999999999998] done some work on I've wrote a paper or

[28.199 - 33.78] a short book called post nihilism where

[31.26 - 36.18] what I suspect is that we are barreling

[33.78 - 37.32] towards uh what I call a nihilistic

[36.18 - 39.239] crisis or actually we're in the middle

[37.32 - 41.0] of a nihilistic crisis and it actually

[39.239 - 44.16] started with the Industrial Revolution

[41.0 - 46.980000000000004] if you look at a lot of poetry and

[44.16 - 49.44] literature works of fiction during the

[46.98 - 51.059] the rise of the Industrial Revolution a

[49.44 - 52.739] lot of people had a lot of existential

[51.059 - 55.86] anxiety about what was the point of

[52.739 - 57.36] Being Human in an era of machines and

[55.86 - 60.12] this kind of pops up every now and then

[57.36 - 62.899] right same thing happened with computers

[60.12 - 65.46] um with the Advent of you know uh

[62.899 - 67.92] high-speed computers nuclear weapons so

[65.46 - 69.29899999999999] on and so forth technological advances

[67.92 - 70.58] advancements tend to give us some

[69.299 - 73.26] existentialing

[70.58 - 75.96] but to your question about like okay

[73.26 - 77.88000000000001] what is the benefit of being a human in

[75.96 - 79.74] a world where from a product

[77.88 - 81.36] productivity standpoint or an economic

[79.74 - 83.22] standpoint machines can do everything

[81.36 - 86.34] that we can do better faster and cheaper

[83.22 - 88.439] what's the point and so that is where we

[86.34 - 91.259] have to change our orientation towards

[88.439 - 92.939] how we value our own life and our own

[91.259 - 94.92] subjective experience so that's a deeply

[92.939 - 97.5] deeply philosophical and religious

[94.92 - 99.96000000000001] perspective or a question and it's it's

[97.5 - 101.7] really interesting because depending on

[99.96 - 104.46] someone's spiritual upbringing or

[101.7 - 106.979] spiritual disposition the question lands

[104.46 - 108.65899999999999] very differently because many uh

[106.979 - 110.7] religious doctrines around the world

[108.659 - 113.52000000000001] basically say that humans have a soul

[110.7 - 116.46000000000001] and that sets us apart and so whether or

[113.52 - 118.79899999999999] not that's true uh people have a model

[116.46 - 122.759] for just saying my subjective experience

[118.799 - 126.54] of being is Matt is very meaningful and

[122.759 - 129.06] it is unique and so part of overcoming a

[126.54 - 131.16] nihilistic crisis is we all have to face

[129.06 - 133.26] that whether or not not we believe in

[131.16 - 136.319] souls or God or whatever

[133.26 - 139.01999999999998] and we have to kind of go back to basics

[136.319 - 140.57999999999998] and look at the subjective experience of

[139.02 - 142.44] our own being and so back to your

[140.58 - 144.0] question earlier about children I

[142.44 - 146.7] suspect that children who grow up with

[144.0 - 148.08] AI they will just intrinsically know oh

[146.7 - 149.879] yeah my experience is different from

[148.08 - 151.37900000000002] this machine and that's okay and that

[149.879 - 154.14] they won't have any existential anxiety

[151.379 - 155.76] about it I hope at least do you have are

[154.14 - 157.98] you hopeful for the future or do you

[155.76 - 161.34] have this anxiety

[157.98 - 163.44] um no I I am uh I think I'm biologically

[161.34 - 165.239] programmed to be optimistic I just I

[163.44 - 168.48] can't be cynical

[165.239 - 171.12] um and I I

[168.48 - 172.56] part of that is that I've done a

[171.12 - 174.72] tremendous amount of work to understand

[172.56 - 177.0] what the dangers and risks are and I've

[174.72 - 179.58] also tried to contribute to coming up

[177.0 - 181.379] with a more optimistic outcome the

[179.58 - 183.0] machines so we we all learned this from

[181.379 - 185.22] watching Scooby-Doo

[183.0 - 187.26] um the monsters are always humans right

[185.22 - 189.12] there's no such thing as as an evil

[187.26 - 192.17999999999998] monster out there the problem is always

[189.12 - 194.36] humans and so this is this is a big

[192.18 - 196.8] reason that I've done my work is because

[194.36 - 198.72000000000003] you know it's not it's not that a

[196.8 - 200.15900000000002] machine is going to replace you and

[198.72 - 202.319] that's a bad thing right we all

[200.159 - 203.879] fantasize about like hey I want to you

[202.319 - 205.79999999999998] know go live in the countryside and just

[203.879 - 208.14] go fishing every day we all know what we

[205.8 - 211.20000000000002] want to do if we don't have to work what

[208.14 - 213.35999999999999] we are truly afraid of is not being able

[211.2 - 215.33999999999997] to take care of ourselves is that if the

[213.36 - 217.26000000000002] machine takes our job we're gonna go

[215.34 - 219.36] hungry we're gonna lose our home we're

[217.26 - 222.06] gonna end up lonely and and whatever

[219.36 - 224.15900000000002] that's the actual fear

[222.06 - 226.44] um nobody actually wants to keep working

[224.159 - 229.07999999999998] right nobody want like I remember one of

[226.44 - 231.54] the advertisements for um for a like

[229.08 - 232.98000000000002] health insurance here in America was you

[231.54 - 234.35999999999999] get to keep your health insurance you

[232.98 - 237.23899999999998] like your health nobody likes health

[234.36 - 241.31900000000002] insurance it's a necessary evil right

[237.239 - 243.54] jobs occupations are a necessary evil of

[241.319 - 245.45899999999997] the economic environment that we're in

[243.54 - 247.5] and the technological limitations that

[245.459 - 248.94] we're in and so as these things progress

[247.5 - 251.099] this is this is I'm basically just

[248.94 - 252.72] unpacking why I'm optimistic as these

[251.099 - 254.459] things progress I hope that we're all

[252.72 - 256.26] going to be able to have kind of a Back

[254.459 - 257.579] to Basics moment where it's like you

[256.26 - 258.84] wake up one day and it's like how do you

[257.579 - 260.519] actually want to live right if you want

[258.84 - 262.38] to go fishing every day do it if you

[260.519 - 264.9] want to focus on being an opera singer

[262.38 - 267.12] go do that you know there's we all have

[264.9 - 269.34] stuff that we want to do but that we

[267.12 - 271.32] sacrifice for the sake of earning enough

[269.34 - 274.67999999999995] money to take care of ourselves and that

[271.32 - 276.12] is the reality for most of us today

[274.68 - 278.04] one of the reasons why we have this

[276.12 - 279.6] worry is because currently we live in

[278.04 - 282.06] sort of a negotiated environment right

[279.6 - 284.52000000000004] the successive labor movements was

[282.06 - 286.759] because labor was needed when humans are

[284.52 - 289.44] no longer needed

[286.759 - 290.88] there's sort of a worry that

[289.44 - 292.08] we're not going to have the opportunity

[290.88 - 293.34] to go fishing

[292.08 - 295.38] right where they're going to have

[293.34 - 296.82] nothing and I guess that's that's the

[295.38 - 298.62] the worry that you're pointing at what

[296.82 - 300.71999999999997] what do you think the first jobs are

[298.62 - 302.28000000000003] that are going to go

[300.72 - 303.36] well there's already been quite a few

[302.28 - 306.23999999999995] layoffs

[303.36 - 308.34000000000003] um various uh communities on Reddit or

[306.24 - 310.8] private communities on Discord

[308.34 - 312.71999999999997] um so for instance my uh fiance's uh

[310.8 - 314.699] were both writers but she's on a few uh

[312.72 - 316.199] private writing communities

[314.699 - 318.90000000000003] um copywriters have already been laid

[316.199 - 320.94] off and replaced by AI

[318.9 - 322.5] um uh marketing teams have been notified

[320.94 - 323.82] that you know they've got a year until

[322.5 - 326.16] they're all going to get laid off and

[323.82 - 328.68] replaced by you know AI generated images

[326.16 - 330.78000000000003] and AI generated emails

[328.68 - 333.72] um so it's happening

[330.78 - 337.02] um yeah that's that that's where we're

[333.72 - 339.0] at now I guess to to your your larger

[337.02 - 340.979] point of you know if we're all

[339.0 - 343.08] replaceable you know what's what's the

[340.979 - 345.36] bottom line and the fact of the matter

[343.08 - 346.979] is from a corporate perspective from uh

[345.36 - 349.91900000000004] from the perspective of neoliberalism

[346.979 - 352.44] human labor is one of the most expensive

[349.919 - 354.78] aspects of productivity and it's also

[352.44 - 356.94] the biggest constraint you look at a

[354.78 - 358.79999999999995] population decline in places like China

[356.94 - 360.9] and Japan because China just crested

[358.8 - 362.34000000000003] right so from here on out China's

[360.9 - 364.13899999999995] population is going down for at least

[362.34 - 366.11999999999995] the next Century Japan has been in

[364.139 - 368.34000000000003] Decline for a couple decades now uh

[366.12 - 370.919] ditto for Italy and a few other nations

[368.34 - 373.67999999999995] so their labor force is Contracting

[370.919 - 377.15999999999997] right and from an economic perspective

[373.68 - 379.68] that's really bad for for for Nations so

[377.16 - 382.02000000000004] AI hopefully will actually Shore up

[379.68 - 385.38] those Labor uh labor markets and

[382.02 - 387.29999999999995] actually replace lost human labor now

[385.38 - 390.0] because humans are so expensive right

[387.3 - 391.86] you can pay uh 20 a month for chat gbt

[390.0 - 393.84] and it can basically serve as an

[391.86 - 395.699] executive assistant and personal coach

[393.84 - 397.31899999999996] and every it can replace literally

[395.699 - 399.6] thousands of dollars worth of Labor and

[397.319 - 400.86] it costs 20 a month chat GPT is

[399.6 - 402.47900000000004] infinitely cheaper than most human

[400.86 - 403.62] employees

[402.479 - 405.539] um and that's only going to get better

[403.62 - 407.639] right because either the model is going

[405.539 - 409.02] to get more efficient and cheaper

[407.639 - 410.639] um or it's going to get smarter and more

[409.02 - 414.18] powerful and therefore more valuable or

[410.639 - 415.5] both in all likelihood so one one of the

[414.18 - 417.3] things that I predict

[415.5 - 420.06] is that we are going to have a post

[417.3 - 422.88] Labor uh market economy before too long

[420.06 - 425.52] and in that respect uh basically

[422.88 - 427.199] economic productivity will be decoupled

[425.52 - 429.0] from Human labor

[427.199 - 430.38] um and in that case you know you're

[429.0 - 432.84] going to see quadrillion dollar

[430.38 - 433.919] valuation uh for companies that have no

[432.84 - 435.84] employees

[433.919 - 438.0] and that might sound like that that

[435.84 - 440.88] could be an ingredient for a dystopian

[438.0 - 442.74] world that nobody wants to live in

[440.88 - 445.38] we'll get to like the regulation and

[442.74 - 448.44] stuff of that later but from a from a

[445.38 - 449.46] from a purely GDP perspective AI is

[448.44 - 453.3] going to be the best thing that ever

[449.46 - 456.0] happened to GDP to uh to uh economics

[453.3 - 457.5] because again it will decouple uh human

[456.0 - 458.94] labor from the constraint and that there

[457.5 - 462.12] there will still be a few constraints

[458.94 - 465.0] natural resources Rare Minerals uh fresh

[462.12 - 466.44] water arable land right there's going to

[465.0 - 468.599] be there's always going to be some

[466.44 - 471.0] physical constraints but we're going to

[468.599 - 473.88] remove human labor as one of the main uh

[471.0 - 475.38] constraints to economics and that is

[473.88 - 476.639] going to mandate kind of those things

[475.38 - 479.15999999999997] like you said like if you want to go

[476.639 - 481.56] fishing well how right if you don't have

[479.16 - 484.259] any economic power if you don't have any

[481.56 - 485.94] way to make a demand then that's a big

[484.259 - 487.08000000000004] problem which is what we're going to

[485.94 - 488.52] have to negotiate we're going to have to

[487.08 - 489.96] negotiate a new social contract

[488.52 - 491.58] basically

[489.96 - 493.85999999999996] what do you think the impact is going to

[491.58 - 496.44] be on births ultimately do you think

[493.86 - 498.96000000000004] people are going to just start having AI

[496.44 - 500.52] children because it's cheaper

[498.96 - 502.85999999999996] you know that's a really difficult

[500.52 - 504.96] question I could see it going either way

[502.86 - 507.90000000000003] um there's plenty of books and and and

[504.96 - 509.75899999999996] fiction out there and research papers

[507.9 - 512.64] um people have predicted you know the

[509.759 - 514.14] population uh explosion you know the

[512.64 - 515.58] Earth will become uninhabitable because

[514.14 - 517.62] we'll have billions and billions of

[515.58 - 518.94] people that we can't feed other people

[517.62 - 520.979] are worried that you know the population

[518.94 - 522.3000000000001] is going to collapse

[520.979 - 524.339] um and I actually had a pretty long

[522.3 - 526.56] conversation about this just to kind of

[524.339 - 528.12] clarify my own ideas uh again with chat

[526.56 - 530.9399999999999] GPT

[528.12 - 534.54] um and so there's a few driving factors

[530.94 - 536.72] that cause uh birth rates to decline

[534.54 - 538.8] um uh women entering the workforce

[536.72 - 541.82] education and empowerment for women

[538.8 - 545.279] access to birth control so it turns out

[541.82 - 547.98] when a society advances and becomes a

[545.279 - 550.38] little bit more uh sophisticated or or

[547.98 - 552.36] gains more access or some you know Ginny

[550.38 - 555.06] coefficient goes up whatever metrics you

[552.36 - 555.899] use education goes up fertility rates go

[555.06 - 557.76] down

[555.899 - 560.04] some of that has to do with the choices

[557.76 - 561.36] of Family Planning you know men and

[560.04 - 563.64] women decide to have fewer children

[561.36 - 564.839] women have more control over their own

[563.64 - 566.8199999999999] fate

[564.839 - 568.86] um and so fertility rates tend to go

[566.82 - 570.839] down and this is a very very reliable

[568.86 - 572.0] Trend globally

[570.839 - 574.74] um you know

[572.0 - 577.14] regardless of culture regardless of

[574.74 - 580.74] other economic conditions as education

[577.14 - 582.779] rates go up as uh uh women in the

[580.74 - 584.88] workforce goes up fertility rates goes

[582.779 - 587.82] down this is a global thing with no

[584.88 - 589.98] exceptions right so if you extrapolate

[587.82 - 592.9200000000001] that out then you can probably make a

[589.98 - 595.5600000000001] relatively safe assumption that as AI

[592.92 - 597.36] spreads around the world and economics

[595.56 - 599.6999999999999] and education and everything goes up

[597.36 - 603.0600000000001] that fertility rates will continue to go

[599.7 - 604.62] down around the whole world South Korea

[603.06 - 607.8] I believe has the lowest fertility rate

[604.62 - 610.14] on the planet at 0.8 births per woman

[607.8 - 612.66] which is uh like

[610.14 - 615.48] um just uh just above a third of the

[612.66 - 618.959] replacement rate so it's entirely

[615.48 - 620.94] possible that under these trends that a

[618.959 - 623.88] population collapse is actually the most

[620.94 - 626.0400000000001] real danger that we face so well what do

[623.88 - 629.22] you do about that one thing that I think

[626.04 - 632.519] is going to happen is that AI will lead

[629.22 - 635.58] to Medical breakthroughs and I suspect

[632.519 - 637.5600000000001] that we are close if not already at uh

[635.58 - 640.44] the the place of what's called Longevity

[637.56 - 641.9399999999999] escape velocity which is that the

[640.44 - 643.98] medical breakthroughs that happen every

[641.94 - 646.5600000000001] year extend your life by more than a

[643.98 - 648.54] year so basically

[646.56 - 651.1199999999999] hypothetically if you're healthy enough

[648.54 - 653.64] today if you're not about to die and you

[651.12 - 655.8] have access to decent enough uh health

[653.64 - 658.1999999999999] care then that the compounding returns

[655.8 - 659.76] of medical research and AI means that

[658.2 - 661.86] you and I could live to be several

[659.76 - 663.66] centuries old which means that the

[661.86 - 667.019] population of the planet will stabilize

[663.66 - 668.9399999999999] as birth rates continue to decline now

[667.019 - 671.579] whether I I do think that some people

[668.94 - 673.6800000000001] will ultimately choose like AI

[671.579 - 675.3] companions as they become more realistic

[673.68 - 677.04] certainly a lot of people have seen

[675.3 - 677.76] shows like Westworld

[677.04 - 679.3199999999999] um you know one of my favorite

[677.76 - 681.0] characters of all time is data from Star

[679.32 - 682.44] Trek and I would love to have data as a

[681.0 - 685.82] friend right

[682.44 - 688.44] um so I absolutely suspect that um that

[685.82 - 690.48] anthropomorphic machines will be part of

[688.44 - 692.22] our Lives before too long

[690.48 - 693.779] um whether what form they take you know

[692.22 - 696.6] whether it's a robotic dog that never

[693.779 - 698.9399999999999] dies or you know a walking talking

[696.6 - 701.339] friend that is always there to hang out

[698.94 - 702.7790000000001] or if it's a romantic partner like uh

[701.339 - 704.82] you know in the movie Her

[702.779 - 706.56] um with Joaquin Phoenix and Scarlett

[704.82 - 709.1400000000001] Johansson there's lots of possibilities

[706.56 - 710.8199999999999] uh for how life is going to be but like

[709.14 - 713.16] I said I think one of the most reliable

[710.82 - 715.32] durable Trends is fertility rates go

[713.16 - 717.06] down so the question is will that be

[715.32 - 719.7600000000001] offset by longevity

[717.06 - 722.2199999999999] so in other words rather than sort of

[719.76 - 724.92] the the dangerous Skynet that some

[722.22 - 727.38] people Envision we might just get out

[724.92 - 729.7199999999999] competed sexually uh into Extinction

[727.38 - 731.64] something along those lines yeah that's

[729.72 - 733.38] that's entirely possible especially when

[731.64 - 735.8389999999999] you consider that um actually there was

[733.38 - 738.36] a line from Terminator two it was when

[735.839 - 739.74] Sarah Connor was watching uh you know

[738.36 - 742.5600000000001] the Terminator Arnold Schwarzenegger

[739.74 - 745.019] play with John and she realized that the

[742.56 - 747.06] machine has infinite patience and will

[745.019 - 749.279] always be there because John was his

[747.06 - 750.959] mission and I realized that from a

[749.279 - 753.779] philosophical standpoint one reading of

[750.959 - 755.2199999999999] that is that the machine could be a

[753.779 - 757.8] better parent than a human parent could

[755.22 - 759.839] ever be because for a child from a

[757.8 - 761.64] child's perspective they should be their

[759.839 - 763.5600000000001] Prime their parents primary Mission but

[761.64 - 765.42] that's never the case right parents are

[763.56 - 767.8199999999999] humans too and they have their own needs

[765.42 - 770.639] their own desires their own plans but

[767.82 - 772.5] when you have a machine that it's if it

[770.639 - 774.42] is designed that you are its primary

[772.5 - 777.36] Mission whether you're an adult or a

[774.42 - 780.019] child like that could be like

[777.36 - 781.86] from some perspectives a better outcome

[780.019 - 783.72] obviously some people are probably

[781.86 - 785.519] cringing which is understandable that's

[783.72 - 787.62] a perfectly healthy reaction to the idea

[785.519 - 789.6] of replacing children and parents with

[787.62 - 792.68] machines but it's possible Right

[789.6 - 792.6800000000001] hypothetically possible

[798.3 - 810.0] [Music]

[807.0 - 810.0] foreign