[0.299 - 7.140000000000001] before we get started I need to address

[2.82 - 10.2] the elephant in the room so after uh

[7.14 - 12.719] several months of wearing my Star Trek

[10.2 - 14.639] uniform t-shirt I got quite a few

[12.719 - 16.08] comments some people were rather

[14.639 - 18.48] triggered by this

[16.08 - 20.759999999999998] they said that it was cringe and

[18.48 - 21.66] unprofessional and ultimately I have to

[20.76 - 23.520000000000003] agree

[21.66 - 25.98] that uh the Pips were not in the right

[23.52 - 29.22] place and it was just a T-shirt and so

[25.98 - 32.04] in response I have upgraded to a

[29.22 - 33.839999999999996] handmade costume grade Sovereign class

[32.04 - 36.059] uniform I hope

[33.84 - 39.18000000000001] that this resolves all the complaints

[36.059 - 43.14] about my last cringe outfit now

[39.18 - 46.26] moving on to today's topic the United

[43.14 - 49.980000000000004] States Senate recently had a closed door

[46.26 - 52.019999999999996] session with more than 20 CEOs and

[49.98 - 54.059] researchers and other stakeholders about

[52.02 - 58.32] artificial intelligence

[54.059 - 60.839999999999996] so the full list of this uh this closed

[58.32 - 62.28] door Senate session includes all kinds

[60.84 - 64.86] of people and I picked out a few people

[62.28 - 67.2] that I was particularly interested in

[64.86 - 68.939] having seen them now Dr room and

[67.2 - 71.88] Chowdhury is someone that I follow on

[68.939 - 74.58] LinkedIn and she advocates for uh

[71.88 - 77.88] amongst many other things red teaming uh

[74.58 - 80.15899999999999] particularly by third party agencies so

[77.88 - 82.32] she's very articulate very intelligent

[80.159 - 84.659] and I appreciate the way that she

[82.32 - 87.65899999999999] approaches AI safety another one is

[84.659 - 89.58000000000001] Tristan Harris who is the CEO of the

[87.659 - 92.22] center for Humane technology so he

[89.58 - 93.42] focuses on alignment with Humanity uh

[92.22 - 96.06] one of the ones that was most

[93.42 - 98.52] interesting was Alex carp so Alex carp

[96.06 - 101.159] is the CEO of palantir and if you're not

[98.52 - 104.34] familiar with palantir they are a

[101.159 - 106.43900000000001] basically for law enforcement and

[104.34 - 110.159] intelligence agencies they're they're a

[106.439 - 112.979] like big data Gathering uh agency and

[110.159 - 115.68] they focus on graph Theory actually and

[112.979 - 118.799] so most of what they do do is scrape

[115.68 - 121.56] public and some private data sources to

[118.799 - 123.06] correlate times events transactions and

[121.56 - 125.57900000000001] all kinds of other things they have

[123.06 - 127.92] quite a few products and projects out

[125.579 - 130.2] there but the fact that someone who is

[127.92 - 131.94] involved with law enforcement

[130.2 - 133.73899999999998] counterterrorism and Military

[131.94 - 136.319] Intelligence was involved in this was

[133.739 - 138.18] really interesting and so back when I

[136.319 - 140.879] first heard about palantir I listened to

[138.18 - 143.52] a few interviews with Alex carp and he

[140.879 - 145.379] has very very specific and very clear

[143.52 - 148.5] beliefs about the way that the world

[145.379 - 151.2] works and in particular you know he

[148.5 - 154.26] explicitly refuses to sell his services

[151.2 - 156.0] to totalitarian regimes like China North

[154.26 - 158.7] Korea and Russia

[156.0 - 161.4] because he says that that their goals

[158.7 - 164.11999999999998] and their Ambitions are intrinsically

[161.4 - 166.86] destructive and run directly counter to

[164.12 - 169.31900000000002] the Western way of life the American way

[166.86 - 171.42000000000002] of life and the European way of life

[169.319 - 172.98] um so very principled person and I know

[171.42 - 174.83999999999997] not everyone's going to agree with him

[172.98 - 176.94] and I certainly don't agree with him on

[174.84 - 179.28] every point but this is a highly

[176.94 - 181.26] principled person who has a Global

[179.28 - 182.64000000000001] Perspective and that's actually why he

[181.26 - 185.519] created his company

[182.64 - 187.07999999999998] uh another one was Deborah Raji uh who

[185.519 - 190.14000000000001] focuses on algorithmic bias and

[187.08 - 192.65900000000002] accountability uh Janet mcguia I hope

[190.14 - 195.11999999999998] I'm saying her now her name right

[192.659 - 196.739] um civil rights activist interestingly

[195.12 - 199.739] there's a few creatives so Charles

[196.739 - 201.72] rivkin and Meredith uh Stein were

[199.739 - 204.72] representing the film industry and the

[201.72 - 206.819] writing industry so probably they were

[204.72 - 209.34] invited in order to represent the

[206.819 - 211.92] interest of uh people that are affected

[209.34 - 214.44] by generative AI art we had Elizabeth

[211.92 - 216.72] Schuler for labor rights

[214.44 - 218.28] we had teachers represented and then

[216.72 - 220.799] finally a second person representing

[218.28 - 222.12] human and civil rights so I know that

[220.799 - 223.92000000000002] there's been a lot of people that

[222.12 - 227.58] criticize this meeting for being too

[223.92 - 229.55999999999997] many CEOs but some of those CEOs are

[227.58 - 233.28] CEOs of non-profits and research and

[229.56 - 236.22] advocacy organizations so look past the

[233.28 - 238.44] headlines and recognize that there there

[236.22 - 241.68] are a lot of Heavy Hitters here okay

[238.44 - 243.239] so here's one image of the uh of the of

[241.68 - 245.70000000000002] the event so you get a little bit of

[243.239 - 248.09900000000002] context you can see the format

[245.7 - 250.85999999999999] um I think that's uh Elon Musk right

[248.099 - 253.67999999999998] there and then there's Alex carp I think

[250.86 - 256.32] there's a tsundar pishai of um Google

[253.68 - 258.66] and a few others so I think there's

[256.32 - 261.18] Chuck Schumer there there's Dr Roman

[258.66 - 263.28000000000003] Chaudhary so you can see that it's like

[261.18 - 266.639] this almost like this kind of Crescent

[263.28 - 268.32] table and I guess these are the uh

[266.639 - 271.62] observers I'm not sure maybe other

[268.32 - 273.59999999999997] Senators that were watching uh anyways

[271.62 - 276.12] so definitely a lot of Pomp and

[273.6 - 279.84000000000003] Circumstance but there's also kind of

[276.12 - 282.24] this sense of equality or egalitarian uh

[279.84 - 285.06] people being side by side I believe this

[282.24 - 286.74] is the CEO of Microsoft sorry the the

[285.06 - 287.82] resolution of this picture is a little

[286.74 - 289.74] bit too low

[287.82 - 292.199] Okay so

[289.74 - 294.66] super high level overview this was a six

[292.199 - 296.22] hour session I was uh kind of surprised

[294.66 - 298.62] by that so they probably had a break for

[296.22 - 300.72] lunch uh but that was that's a long time

[298.62 - 302.759] to be sitting and talking it was hosted

[300.72 - 304.38000000000005] by Senator Chuck Schumer which if you're

[302.759 - 306.96000000000004] not familiar with what he's done with AI

[304.38 - 309.0] recently he uh just a couple months ago

[306.96 - 312.96] proposed the safe Innovation framework

[309.0 - 314.759] which is a four-point uh like safety uh

[312.96 - 316.79999999999995] AI safety Innovation framework which

[314.759 - 319.5] prior tries to balance safety and

[316.8 - 321.84000000000003] Innovation I did read it at least the

[319.5 - 324.139] initial kind of press release

[321.84 - 326.46] um as well as some of the uh the initial

[324.139 - 328.74] literature that came out about it and it

[326.46 - 330.539] seemed okay but it's not very specific

[328.74 - 333.24] uh but again that's why they're having

[330.539 - 336.12] these these meetings now this was a

[333.24 - 339.3] closed door meeting in order to uh you

[336.12 - 341.28000000000003] know reduce uh press uh visibility and

[339.3 - 343.02000000000004] also to facilitate candid conversations

[341.28 - 345.23999999999995] and I'll get into that in just a moment

[343.02 - 347.28] because this part the fact that it was

[345.24 - 349.919] closed door caused a lot of heartburn

[347.28 - 351.29999999999995] and even I will admit that even when I I

[349.919 - 352.979] first heard about that I was like wow

[351.3 - 355.139] really like they're going closed door

[352.979 - 357.36] sessions already uh with all these

[355.139 - 360.3] industry insiders but I wanted to wait

[357.36 - 361.91900000000004] and see what information came out and

[360.3 - 363.72] then give my honest appraisal after

[361.919 - 366.0] having some time to reflect on it and

[363.72 - 367.259] sing uh seeing kind of what trickled out

[366.0 - 369.539] of this meeting

[367.259 - 371.34000000000003] so one thing that I want to point out is

[369.539 - 373.62] that the Senate the the United States

[371.34 - 376.38] Senate has already had many public

[373.62 - 379.919] hearings around everything from Ai and

[376.38 - 382.5] copyright AI safety regulation and all

[379.919 - 385.79999999999995] sorts of stuff and so it is time for

[382.5 - 387.6] them to move on to having uh closed-door

[385.8 - 389.639] meetings in order to have those more

[387.6 - 392.52000000000004] candid conversations

[389.639 - 394.919] um that so basically the idea is you

[392.52 - 397.38] reduce the risk profile for CEOs

[394.919 - 399.0] speaking candidly because what you'll

[397.38 - 400.979] see what happens when they speak

[399.0 - 403.86] publicly is that they have to have

[400.979 - 406.86] highly highly sanitized uh corporate

[403.86 - 409.74] speak in order to say like you basically

[406.86 - 411.84000000000003] give canned responses to things however

[409.74 - 414.0] if you have if you have a little bit

[411.84 - 416.21999999999997] more privacy that gives them permission

[414.0 - 417.66] to say things that may not sit well with

[416.22 - 419.759] the public and of course I can hear some

[417.66 - 421.199] people saying well if they're going to

[419.759 - 423.18] say stuff that doesn't sit well with the

[421.199 - 424.979] public maybe it should be public maybe

[423.18 - 427.62] these people with all this money and

[424.979 - 429.65999999999997] privilege and power and influence should

[427.62 - 431.1] not have these closed-door privileges

[429.66 - 433.56] however

[431.1 - 436.02000000000004] there is a reason for this format

[433.56 - 438.06] and I'm not going to say that this is

[436.02 - 439.979] the correct format I'm just saying the

[438.06 - 442.44] reasons there are reasons for this

[439.979 - 445.25899999999996] format and basically it has it comes

[442.44 - 449.28] down to the neoliberal approach to

[445.259 - 451.38] government and economics versus the

[449.28 - 454.08] Communist totalitarian Soviet version

[451.38 - 457.86] which the the key difference here is

[454.08 - 461.21999999999997] Central management versus uh pre-market

[457.86 - 463.08000000000004] capitalism and third-party experts so

[461.22 - 465.24] basically the idea the two underpinning

[463.08 - 467.52] print principles of neoliberalism here

[465.24 - 469.62] are free market advocacy and minimal

[467.52 - 471.539] State intervention and so one of the

[469.62 - 474.539] things one of the downstream effects of

[471.539 - 476.34] this is that rather than having only the

[474.539 - 478.259] government making decisions the

[476.34 - 480.0] government is supposed to consult with

[478.259 - 481.44] outside experts

[480.0 - 483.12] um this is something that America does

[481.44 - 485.099] very differently from the way that you

[483.12 - 487.08] know the Soviet Communists used to do

[485.099 - 488.71999999999997] things and this is of course still the

[487.08 - 491.15999999999997] way that uh China does things today

[488.72 - 493.139] where the government makes all decisions

[491.16 - 496.259] more or less unilaterally

[493.139 - 498.539] by Consulting with industry experts who

[496.259 - 500.34000000000003] have different agendas and different uh

[498.539 - 503.12] stakes and things the idea is that

[500.34 - 505.61999999999995] you'll have better decision making uh

[503.12 - 507.24] processes over time that you'll get

[505.62 - 508.979] information from different stakeholders

[507.24 - 511.319] with different agendas and different

[508.979 - 513.18] motivations and that everything will

[511.319 - 516.839] kind of get pulled towards some

[513.18 - 518.5799999999999] semblance of the truth or uh maybe not

[516.839 - 519.719] truth maybe that's the wrong word but

[518.58 - 521.399] the things will get kind of pulled

[519.719 - 524.7600000000001] towards the center

[521.399 - 526.68] okay so the whole thing the the comment

[524.76 - 529.62] that kicked off all of this research on

[526.68 - 531.42] my part was uh Dr chowdery's comment on

[529.62 - 534.0600000000001] LinkedIn as I mentioned I follow her on

[531.42 - 536.04] LinkedIn I am uh one step away from

[534.06 - 537.959] connecting with her

[536.04 - 541.019] um but I follow her and plenty of other

[537.959 - 544.1999999999999] policy uh Advocates uh around the world

[541.019 - 546.48] not just in America but so the the I'll

[544.2 - 549.1800000000001] skip the first part but the the meat of

[546.48 - 551.519] this was in seriousness I was PR I was

[549.18 - 554.16] impressed with the quality and Candor of

[551.519 - 555.839] conversation the panelists and senators

[554.16 - 558.0] were engaged in direct and real

[555.839 - 561.12] conversation about the realistic harms

[558.0 - 563.16] of AI and what government can do I

[561.12 - 565.44] advocated for increased funding and

[563.16 - 567.3] access for independent researchers and

[565.44 - 569.0400000000001] investment in a full ecosystem of

[567.3 - 570.899] governance not just industry and

[569.04 - 573.18] government but independent third parties

[570.899 - 576.36] we talked about open source education

[573.18 - 578.459] access sustainable in Innovation guard

[576.36 - 580.26] rails benchmarks and more it's a

[578.459 - 582.42] positive step forward so I literally

[580.26 - 585.06] just copied this directly for from her

[582.42 - 587.5799999999999] LinkedIn this is not me editorializing

[585.06 - 589.68] this is one for one copy from her

[587.58 - 592.08] LinkedIn unfortunately I wasn't able to

[589.68 - 594.12] find her saying anything else so this is

[592.08 - 596.58] probably this is probably a statement

[594.12 - 599.16] that uh you know her PR team helped her

[596.58 - 601.32] craft in order to just say like hey this

[599.16 - 602.399] was a positive thing moving forward

[601.32 - 604.7] um I don't know that she said anything

[602.399 - 606.779] else publicly about it but fortunately

[604.7 - 608.58] plenty of other people have commented

[606.779 - 610.92] and I have injected some of that

[608.58 - 613.2] information uh from that so here's

[610.92 - 615.36] here's another angle so we have Chuck

[613.2 - 616.9200000000001] Schumer that we have Dr Chowdhury here

[615.36 - 618.9590000000001] and a few other people that I don't

[616.92 - 622.26] recognize that's definitely the CEO of

[618.959 - 624.4799999999999] Microsoft there's CEO of Google

[622.26 - 625.92] so they separated some people out so

[624.48 - 628.5] that they weren't necessarily side by

[625.92 - 630.4799999999999] side although there was one pairing that

[628.5 - 632.459] makes me a little nervous which we'll

[630.48 - 635.4590000000001] get to at the end

[632.459 - 636.959] um okay so after sifting through all of

[635.459 - 639.42] the bits of information that are

[636.959 - 641.5189999999999] trickling out from various sources I

[639.42 - 643.38] found that there's basically only three

[641.519 - 645.72] points of agreement that really came out

[643.38 - 647.76] of this uh this event

[645.72 - 650.22] um so first there was unanimous

[647.76 - 652.68] agreement that the government needs to

[650.22 - 655.9200000000001] uh participate in regulation and

[652.68 - 657.42] development of AI uh basically at the my

[655.92 - 659.519] understanding is what happened is near

[657.42 - 662.279] the beginning of the thing uh Senator

[659.519 - 664.44] Chuck Schumer just asked like buy a show

[662.279 - 666.6] of hands who agrees that the government

[664.44 - 668.519] should intervene and so everyone raised

[666.6 - 670.38] their hands and there's a reason that I

[668.519 - 672.18] wanted to include this as well as the

[670.38 - 673.98] earlier slide about neoliberalism which

[672.18 - 676.4399999999999] one of the principles of neoliberalism

[673.98 - 678.9590000000001] is government non-intervention and so

[676.44 - 680.7] that leads to the uh the unsubstantiated

[678.959 - 681.959] quotation earlier which is that the

[680.7 - 684.1800000000001] government seems like they're ready to

[681.959 - 687.4799999999999] rip up the existing Playbook and move

[684.18 - 690.18] towards a different Paradigm of direct

[687.48 - 691.86] government oversight regulation and

[690.18 - 695.16] development of AI

[691.86 - 697.8000000000001] which is a very powerful pivot away from

[695.16 - 699.959] the minimalist approach of neoliberalism

[697.8 - 702.899] which basically creates a wild west

[699.959 - 704.279] environment of of corporatism so

[702.899 - 706.74] hopefully

[704.279 - 708.48] what this is signaling is actually a

[706.74 - 709.98] fundamental paradigm shift in the way

[708.48 - 713.04] that the government is going to approach

[709.98 - 714.24] things and basically what I'm hoping is

[713.04 - 717.3] that the government will start to

[714.24 - 720.0600000000001] Advocate more zealously on the interests

[717.3 - 722.2199999999999] of people not just consumers but all

[720.06 - 724.5] citizens all civilians

[722.22 - 725.5790000000001] this is me reading the tea leaves I

[724.5 - 727.44] don't know if that's actually how it's

[725.579 - 730.019] going to play out and we'll unpack this

[727.44 - 732.899] in a little bit uh more depth in in the

[730.019 - 734.339] coming slides another thing that was

[732.899 - 736.26] agreed on was that International

[734.339 - 738.36] coordination is required so remember

[736.26 - 740.459] this is layer six of the gato framework

[738.36 - 744.12] that I proposed which is that we need

[740.459 - 746.8199999999999] International bodies not necessarily

[744.12 - 749.16] Regulators or researchers they just said

[746.82 - 752.1800000000001] International coordination they likened

[749.16 - 754.4399999999999] it to nuclear Regulators like the iaea

[752.18 - 756.8389999999999] there was General consensus about

[754.44 - 759.0600000000001] International regulation there was not

[756.839 - 761.4200000000001] as much consensus about creating an

[759.06 - 764.579] international research body like CERN

[761.42 - 767.459] and personally what I would really like

[764.579 - 769.079] to see is all the foundation maybe not

[767.459 - 772.8] all but many of the foundation and

[769.079 - 776.04] Frontier models being trained by by

[772.8 - 777.8389999999999] Public Public Funding by tax dollars and

[776.04 - 780.06] international efforts and then being

[777.839 - 782.7600000000001] accessible to everyone all companies all

[780.06 - 784.7399999999999] people maybe open source Maybe be not

[782.76 - 787.139] open source I'm still undecided I very

[784.74 - 789.779] lean heavily towards open source because

[787.139 - 791.7] it's like python is open source like

[789.779 - 793.86] eventually here's the way I see it

[791.7 - 795.4200000000001] eventually we're going to look at AI as

[793.86 - 798.1800000000001] just like any other programming language

[795.42 - 800.519] or operating system or or computer

[798.18 - 803.399] program it's just another tool in the

[800.519 - 806.22] toolbox so I've I am very much in favor

[803.399 - 807.66] of open sourcing for all AI because

[806.22 - 809.76] that's going to make it more transparent

[807.66 - 811.3199999999999] more explainable and over over the long

[809.76 - 814.3199999999999] run more secure

[811.32 - 817.9200000000001] but more importantly on principle open

[814.32 - 819.6] source AI is going to enable for more

[817.92 - 822.4799999999999] democratic access which actually Mark

[819.6 - 824.1] Zuckerberg asked advocates for pretty

[822.48 - 827.22] vociferously and he seems like he's

[824.1 - 830.5790000000001] alone in this in terms of advocating for

[827.22 - 832.5] for just like 100 open source

[830.579 - 835.1999999999999] um and then finally the third point that

[832.5 - 837.18] that everyone seemed to agree on was uh

[835.2 - 840.3000000000001] Public Funding and development of

[837.18 - 842.04] expertise around AI so basically there

[840.3 - 844.0999999999999] seems to be General agreement that they

[842.04 - 848.3389999999999] need to increase government investment

[844.1 - 851.22] not just in uh in terms of University

[848.339 - 853.62] grants and other things like that but in

[851.22 - 856.62] in terms of Workforce Development

[853.62 - 858.839] now there was a lot of disagreement

[856.62 - 861.48] so one of the most contentious points of

[858.839 - 864.12] disagreement was the role of Open Source

[861.48 - 866.22] so as I just mentioned Mark Zuckerberg

[864.12 - 867.6] seems like he was more or less alone out

[866.22 - 870.6] there and just saying that like open

[867.6 - 873.0600000000001] source will democratize access I fully

[870.6 - 874.6800000000001] agree but then of course meta and Mark

[873.06 - 877.0189999999999] Zuckerberg don't have the best access

[874.68 - 879.54] when it comes to civil rights and uh

[877.019 - 881.88] being responsible users of artificial

[879.54 - 884.3389999999999] intelligence so what's their angle we'll

[881.88 - 886.62] unpack that in just a moment Tristan

[884.339 - 889.3800000000001] Harris from the um Center for Humane

[886.62 - 891.54] technology he basically said meta

[889.38 - 893.88] unilaterally decided what was safe and

[891.54 - 895.5] that that was not cool because he and

[893.88 - 897.959] his team found that llama could be used

[895.5 - 899.94] to produce harm but then again like you

[897.959 - 902.6389999999999] can use Python to do harm but no one's

[899.94 - 904.74] requiring you know python to be licensed

[902.639 - 907.5] you know or check with Regulators every

[904.74 - 909.42] time uh Bill Gates also pushed back

[907.5 - 910.62] against Mark Zuckerberg saying that

[909.42 - 912.3] there's a really big difference between

[910.62 - 914.88] just certain searching for information

[912.3 - 917.76] that is passively available online and

[914.88 - 920.459] interacting with an AI system what I'm

[917.76 - 922.56] reading into this is is based on Bill

[920.459 - 925.8599999999999] Gates comments when he was first shown

[922.56 - 928.199] gpt4 he released a statement through his

[925.86 - 931.74] online platform talking about how he

[928.199 - 934.26] perceives software agents AI agents as

[931.74 - 936.0600000000001] being transformative to the workforce so

[934.26 - 939.3] I think that Bill Gates is thinking in

[936.06 - 942.7199999999999] terms of autonomous AI moving forward

[939.3 - 944.639] and so obviously if some people are

[942.72 - 946.8000000000001] starting to arm up with either

[944.639 - 948.72] commercial or weaponized autonomous AI

[946.8 - 949.9799999999999] That's a really big concern because just

[948.72 - 952.98] searching for information on the

[949.98 - 954.6] Internet is a far cry from being able to

[952.98 - 957.779] build something that is fully autonomous

[954.6 - 959.279] and agentic uh again I'm reading into

[957.779 - 961.019] that a little bit that is my personal

[959.279 - 962.9399999999999] editorializing so I want to be very

[961.019 - 964.44] clear that I don't have any evidence

[962.94 - 967.5600000000001] that Bill Gates is thinking about

[964.44 - 969.6] weaponized AI but just based on his

[967.56 - 971.6389999999999] track record of what he has said because

[969.6 - 974.0400000000001] he was one of the first people that Sam

[971.639 - 976.5] Altman and open eye open AI showed

[974.04 - 979.38] demonstrated the capabilities of gpt4

[976.5 - 982.139] too so he is in a privileged position to

[979.38 - 983.76] on to better understand what these tools

[982.139 - 985.74] are capable of and what the risks are

[983.76 - 987.8389999999999] also he's a prolific reader he reads

[985.74 - 989.94] like one to two books a day

[987.839 - 991.9200000000001] um so he's he's a pretty sharp dude and

[989.94 - 993.36] I know that like as one of the world's

[991.92 - 994.92] richest people

[993.36 - 996.779] um he is under a lot of scrutiny and a

[994.92 - 998.699] lot of people don't like him that's fine

[996.779 - 1000.139] it is really suspicious that he's buying

[998.699 - 1002.3] a lot of farmland and on a lot of

[1000.139 - 1003.98] forests but you know what if I were a

[1002.3 - 1005.7199999999999] billionaire I'd probably buy be buying

[1003.98 - 1007.1] real estate too so I can't really blame

[1005.72 - 1009.5] him for that

[1007.1 - 1012.259] um and then finally Sam Altman seemed to

[1009.5 - 1014.18] um interestingly Sam Altman seemed to

[1012.259 - 1016.759] um kind of soften his tone from previous

[1014.18 - 1018.56] meetings because he said some things are

[1016.759 - 1019.88] okay to be open source but there are

[1018.56 - 1022.16] probably some things that you don't want

[1019.88 - 1024.14] to be fully public

[1022.16 - 1025.04] um and I when I why I say he softened

[1024.14 - 1026.8390000000002] his tone

[1025.04 - 1029.959] is because at the previous Senate

[1026.839 - 1032.0] hearings he my interpretation was that

[1029.959 - 1035.059] he was pretty vociferously advocating

[1032.0 - 1036.5] that like pretty much anything that is

[1035.059 - 1038.72] above a certain threshold of power

[1036.5 - 1040.4] should be licensed

[1038.72 - 1042.8600000000001] um and one I don't agree with that

[1040.4 - 1045.26] because again it take you fight fire

[1042.86 - 1048.02] with fire right if you want if you want

[1045.26 - 1049.58] only the most wealthy companies to have

[1048.02 - 1052.22] Frontier models and that they can be

[1049.58 - 1054.1999999999998] closed sourced and licensed what about

[1052.22 - 1055.94] The Regulators what about the cyber

[1054.2 - 1057.98] security firms what about the people who

[1055.94 - 1059.8400000000001] cannot afford those and have to just

[1057.98 - 1062.179] trust that that a closed Source tech

[1059.84 - 1064.22] company is providing them the best tools

[1062.179 - 1066.02] no I don't I don't agree with that

[1064.22 - 1069.26] because I'm looking at this from a cyber

[1066.02 - 1072.32] security perspective where you you

[1069.26 - 1074.96] better well have all the best cyber

[1072.32 - 1077.1789999999999] security firms have access to the most

[1074.96 - 1079.3400000000001] powerful Frontier models and can tear

[1077.179 - 1081.799] them apart which they would be able to

[1079.34 - 1085.039] to do with open source models in order

[1081.799 - 1086.96] to find those vulnerabilities now that

[1085.039 - 1089.419] being said I think that probably a

[1086.96 - 1090.8600000000001] compromise is what Dr Chowdhury and

[1089.419 - 1094.8200000000002] others have advocated for which is

[1090.86 - 1096.26] giving privileged access to researchers

[1094.82 - 1098.12] um but again that's keeping it all

[1096.26 - 1100.46] behind closed doors which I'm not

[1098.12 - 1101.7199999999998] necessarily in favor of like if we want

[1100.46 - 1103.88] a free market we should have a free

[1101.72 - 1105.98] market if we both if we believe in free

[1103.88 - 1107.96] market capitalism we need to have a free

[1105.98 - 1109.58] market including the intellectual aspect

[1107.96 - 1111.44] and another thing that I like to point

[1109.58 - 1113.6599999999999] out to people is that all of these

[1111.44 - 1115.76] Frontier models are almost entirely

[1113.66 - 1118.1000000000001] based on Open Source papers that are

[1115.76 - 1120.44] provided by universities and funded by

[1118.1 - 1121.8799999999999] public dollars that's not to say that

[1120.44 - 1123.74] they don't produce some of their own

[1121.88 - 1125.8400000000001] internal research in particular meta

[1123.74 - 1128.299] they have a very robust research Wing

[1125.84 - 1130.6999999999998] Nvidia also has a very robust research

[1128.299 - 1132.26] Wing but even still many of the

[1130.7 - 1133.4] Innovations are publicly funded

[1132.26 - 1136.039] Innovations

[1133.4 - 1138.6200000000001] uh okay so that's my Spiel there another

[1136.039 - 1140.48] point of disagreement was they really

[1138.62 - 1143.12] can't agree on whether they're just

[1140.48 - 1145.4] going to appropriate existing regulatory

[1143.12 - 1147.1999999999998] bodies or create a brand new one I

[1145.4 - 1149.66] believe that Elon Musk was one of the

[1147.2 - 1152.48] ones who was advocating for creating

[1149.66 - 1154.7] entirely new regulatory bodies and the

[1152.48 - 1157.16] analogy that he used repeatedly was

[1154.7 - 1159.44] talking about automobile safety when car

[1157.16 - 1161.44] safety became a big thing there was not

[1159.44 - 1164.38] an existing body and now we have several

[1161.44 - 1166.88] regulatory bodies specifically for

[1164.38 - 1169.2800000000002] Transportation safety uh in America it

[1166.88 - 1170.9] was it NTSB National Transportation

[1169.28 - 1173.36] safety board as well as a few other

[1170.9 - 1175.3400000000001] safety Regulators that look at the the

[1173.36 - 1179.1789999999999] you know Automotive manufacturing

[1175.34 - 1181.58] Industry Road laws that sort of things

[1179.179 - 1184.039] and then finally there was also no no

[1181.58 - 1185.96] agreement on the impact that AI is going

[1184.039 - 1188.299] to have on the workforce

[1185.96 - 1190.1000000000001] um do you just train up do you skill up

[1188.299 - 1192.98] are there going to be job losses or not

[1190.1 - 1195.74] the tech execs interestingly seems to be

[1192.98 - 1198.08] more uh seem to close ranks and say

[1195.74 - 1199.94] we're not really worried this is going

[1198.08 - 1202.84] to be great for productivity we're going

[1199.94 - 1205.3400000000001] to turn the economy up to 11. but

[1202.84 - 1207.32] everyone else was not so sure about that

[1205.34 - 1210.9189999999999] narrative which I'm really glad and

[1207.32 - 1213.1399999999999] having watched a lot of interviews with

[1210.919 - 1215.0] various stakeholders not necessarily

[1213.14 - 1216.5] some of the not necessarily some of the

[1215.0 - 1218.24] people at this meeting but other

[1216.5 - 1220.16] stakeholders and influencers like Andrew

[1218.24 - 1223.34] aung and others

[1220.16 - 1226.28] um there is General consensus amongst

[1223.34 - 1228.5] many people including many entrepreneurs

[1226.28 - 1231.9189999999999] that AI is just going to basically trash

[1228.5 - 1234.08] all conventional understanding of uh

[1231.919 - 1236.179] jobs and economics as we know it and I

[1234.08 - 1238.8799999999999] call this post-labor economics

[1236.179 - 1240.98] okay so that's kind of what has fallen

[1238.88 - 1242.9] out of this so let's unpack this a

[1240.98 - 1244.76] little bit further the biggest thing

[1242.9 - 1246.44] that I and other people are worried

[1244.76 - 1249.3799999999999] about is regulatory capture so

[1246.44 - 1251.96] regulatory capture is when a regulatory

[1249.38 - 1254.0590000000002] body ends up being so heavily influenced

[1251.96 - 1256.28] by the industry that it is regulating

[1254.059 - 1258.9189999999999] that it becomes completely ineffective

[1256.28 - 1261.86] and com and totally corrupt and so this

[1258.919 - 1263.7800000000002] image comes from the infamous big

[1261.86 - 1267.6789999999999] tobacco Senate hearings back in I think

[1263.78 - 1270.44] 1994 where with a straight face they set

[1267.679 - 1272.72] these CEOs said that tobacco is not

[1270.44 - 1275.2] addictive and it's just like okay that

[1272.72 - 1277.94] that lost all credibility and so

[1275.2 - 1280.94] probably you know even though this is

[1277.94 - 1282.6200000000001] almost 30 years old now I remember this

[1280.94 - 1284.539] even though I was a little kid because

[1282.62 - 1287.299] of how angry people were they're like

[1284.539 - 1289.039] this is absolute nonsense and my mom was

[1287.299 - 1290.9] a smoker at the time and she even said

[1289.039 - 1293.24] this is nonsense

[1290.9 - 1295.159] um she's like this is absolute baloney

[1293.24 - 1297.38] this is total BS

[1295.159 - 1299.679] um so

[1297.38 - 1302.179] there's a few other aspects of

[1299.679 - 1303.26] regulatory captures such as closed-door

[1302.179 - 1304.94] meetings closed door meetings are

[1303.26 - 1306.98] actually one of the ways that that

[1304.94 - 1309.679] industries that are being regulated can

[1306.98 - 1311.9] end up getting undue influence and so

[1309.679 - 1315.2] this is why those those public hearings

[1311.9 - 1317.3600000000001] uh were much more popular but one thing

[1315.2 - 1319.52] that a lot of people didn't realize is

[1317.36 - 1321.86] that a lot of the Senators before those

[1319.52 - 1324.08] public hearings had private dinners with

[1321.86 - 1326.059] many of the stakeholders and and other

[1324.08 - 1328.22] people beforehand

[1326.059 - 1331.3999999999999] so the the back door meetings are always

[1328.22 - 1332.96] happening and again it's like damned if

[1331.4 - 1334.179] you do damned if you don't so I don't

[1332.96 - 1336.6200000000001] really know what the best way is

[1334.179 - 1337.76] honestly I would prefer if everything

[1336.62 - 1339.799] was public

[1337.76 - 1343.1589999999999] um but the thing is is the American

[1339.799 - 1344.96] public as all proletariat and and

[1343.159 - 1347.3600000000001] unwashed masses

[1344.96 - 1349.88] many people tend to react very poorly to

[1347.36 - 1352.1] uh the halls of power saying anything so

[1349.88 - 1355.159] I understand the need for for closed

[1352.1 - 1357.559] door meetings and expert conferences uh

[1355.159 - 1359.48] but again in an Ideal World and we don't

[1357.559 - 1361.3999999999999] live in an ideal world everything would

[1359.48 - 1363.02] be public and you know everyone would

[1361.4 - 1365.3600000000001] get along and digest the information

[1363.02 - 1367.039] like mature adults but not everyone is a

[1365.36 - 1368.36] mature adult so unfortunately I

[1367.039 - 1369.679] understand the need for closed-door

[1368.36 - 1371.78] meetings

[1369.679 - 1373.1000000000001] and finally the devil is in the details

[1371.78 - 1375.9189999999999] I'm not going to read all this to you

[1373.1 - 1377.36] but the the uh summary of another source

[1375.919 - 1378.919] basically said overall the meeting

[1377.36 - 1380.7199999999998] showed agreement on the need to regulate

[1378.919 - 1382.76] AI given its transformative potential

[1380.72 - 1385.159] but details remain to be worked out

[1382.76 - 1387.62] through industry and expert input

[1385.159 - 1389.2990000000002] um so basically there's no agreement on

[1387.62 - 1391.9399999999998] licensing I tried to find if there was

[1389.299 - 1394.8799999999999] any specific talk about licensing

[1391.94 - 1397.76] and again uh there was there seemed to

[1394.88 - 1400.8200000000002] be uh no consensus amongst any

[1397.76 - 1403.34] particular group about licensing

[1400.82 - 1405.6789999999999] um but one thing that the senators were

[1403.34 - 1408.3799999999999] most concerned about was you know

[1405.679 - 1410.659] watermarking AI or licensing AI that can

[1408.38 - 1413.3600000000001] be used for election interference so

[1410.659 - 1415.7600000000002] again you know Tech exec CEOs big Tech

[1413.36 - 1417.74] CEOs they have one set of concerns

[1415.76 - 1419.24] people that are advocating for teachers

[1417.74 - 1420.919] and civil rights and human rights they

[1419.24 - 1422.36] have another set of concerns the

[1420.919 - 1423.919] Senators themselves they want to get

[1422.36 - 1427.82] reelected they don't want to get blamed

[1423.919 - 1430.22] if you know any more uh lunatics take

[1427.82 - 1433.1] over the Asylum so to speak

[1430.22 - 1435.6200000000001] um but so this is kind of one of the key

[1433.1 - 1438.1999999999998] key takeaways is that they have not yet

[1435.62 - 1440.9799999999998] worked out details but at least we have

[1438.2 - 1443.8400000000001] surfaced uh through these conversations

[1440.98 - 1446.48] the differences the agreements and

[1443.84 - 1447.9189999999999] disagreements about what details remain

[1446.48 - 1449.84] to be hashed out

[1447.919 - 1450.679] uh and so I believe this is my last

[1449.84 - 1452.6589999999999] slide

[1450.679 - 1454.76] what I want to point out is that even

[1452.659 - 1456.3200000000002] within the tech industry there are

[1454.76 - 1457.8799999999999] different business models which can

[1456.32 - 1459.6789999999999] underpin

[1457.88 - 1462.3200000000002] um some of the different behaviors that

[1459.679 - 1464.3600000000001] will that we see from from these people

[1462.32 - 1466.8799999999999] and this actually came from some help

[1464.36 - 1469.039] some very helpful comments in my YouTube

[1466.88 - 1471.0200000000002] uh section as I've posted various news

[1469.039 - 1473.72] articles and stuff so thanks everyone

[1471.02 - 1475.34] for for participating I read pretty much

[1473.72 - 1477.74] all comments

[1475.34 - 1479.72] um and I react I try I try and respond

[1477.74 - 1482.539] to the to the good ones and and the most

[1479.72 - 1484.159] thoughtful ones uh but so what I want to

[1482.539 - 1485.72] point out is that Microsoft is

[1484.159 - 1488.179] fundamentally a business software

[1485.72 - 1490.88] company and a cloud company yeah they

[1488.179 - 1494.179] have Xbox and a few other services uh

[1490.88 - 1496.3400000000001] but they're business software AI is just

[1494.179 - 1498.5] a tool that they offer and Azure

[1496.34 - 1500.8999999999999] provides both open source and closed

[1498.5 - 1502.52] Source AI models Microsoft is hedging

[1500.9 - 1504.02] their bets they're not going all in on

[1502.52 - 1505.6399999999999] open source or closed Source they're

[1504.02 - 1507.2] like you know what if you want to use a

[1505.64 - 1510.14] model we'll provide it to you through

[1507.2 - 1512.48] Azure great have fun go nuts

[1510.14 - 1513.8600000000001] uh Nvidia likewise they're a hardware

[1512.48 - 1516.38] and they're working on some Cloud

[1513.86 - 1518.6589999999999] platforms but again they don't care

[1516.38 - 1520.7] about the underlying models closed

[1518.659 - 1522.679] Source open source whatever it's running

[1520.7 - 1525.44] on their gpus that's all they care about

[1522.679 - 1527.539] uh meta so this was this was one of the

[1525.44 - 1528.559] key insights that came from uh my

[1527.539 - 1530.419] audience

[1528.559 - 1531.6789999999999] meta they are fundamentally a social

[1530.419 - 1533.779] media company and of course they're

[1531.679 - 1536.0590000000002] working on VR they're not trying to sell

[1533.779 - 1538.64] AI directly they just want AI to enable

[1536.059 - 1541.3999999999999] their products to be better uh in order

[1538.64 - 1543.98] to you know have basically realize the

[1541.4 - 1545.24] Ready Player One Oasis universe

[1543.98 - 1547.039] which

[1545.24 - 1548.779] want to point out that ready player one

[1547.039 - 1550.82] was a cautionary tale about what not to

[1548.779 - 1552.919] do so maybe meta maybe Mark Zuckerberg

[1550.82 - 1555.62] misread the the room when that movie

[1552.919 - 1558.38] came out but anyways whatever I do agree

[1555.62 - 1560.8999999999999] even if he's got potentially dubious

[1558.38 - 1563.5390000000002] motives I do agree with his him

[1560.9 - 1565.8200000000002] advocating for like just fully open

[1563.539 - 1567.799] source AI I'm glad that someone is in

[1565.82 - 1569.059] the room advocating for that

[1567.799 - 1572.0] um and that it's coming out in this

[1569.059 - 1573.62] dialogue now open AI

[1572.0 - 1575.559] if you've watched my channel for a long

[1573.62 - 1577.76] time you know that I have a

[1575.559 - 1579.32] love-hate relationship with open AI

[1577.76 - 1581.419] because on the one hand they've done

[1579.32 - 1584.4189999999999] some really cool things and where they

[1581.419 - 1587.0590000000002] started was was very much in line with

[1584.419 - 1588.8600000000001] my personal values where they have gone

[1587.059 - 1590.96] they've seemed to have drifted and as

[1588.86 - 1593.12] Elon Musk pointed out when openai

[1590.96 - 1594.679] started it was both open source and

[1593.12 - 1596.36] non-profit and neither of those things

[1594.679 - 1599.0590000000002] are true anymore

[1596.36 - 1601.1] um so it's definitely misnamed

[1599.059 - 1603.799] um they're a one-trick pony though they

[1601.1 - 1605.1789999999999] literally have one product gpt3 and now

[1603.799 - 1607.52] gpt4

[1605.179 - 1611.1200000000001] everything that they do is predicated on

[1607.52 - 1612.3799999999999] the success of a single model and so of

[1611.12 - 1615.3799999999999] course they're the most vocal about

[1612.38 - 1618.8600000000001] licensing this is why Ilya sutskiver and

[1615.38 - 1621.0800000000002] uh Jan Lakey and Sam Altman are

[1618.86 - 1622.279] constantly talking about licensing why

[1621.08 - 1624.1999999999998] because they're trying to pull up the

[1622.279 - 1626.679] ladder behind themselves they want to

[1624.2 - 1629.779] they want to protect on and wall off

[1626.679 - 1631.4] their ability to make money now I know

[1629.779 - 1633.5] that Sam Altman is constantly talking

[1631.4 - 1635.9] about like he's like oh well I have I

[1633.5 - 1637.039] have no no Financial incentive to to do

[1635.9 - 1639.0800000000002] this

[1637.039 - 1640.76] yes he's got plenty of money already but

[1639.08 - 1643.1589999999999] he's got plenty of social incentive to

[1640.76 - 1645.34] do this Sam Altman is at the most

[1643.159 - 1647.9] powerful he's ever been and All Humans

[1645.34 - 1649.52] particularly in that space want more

[1647.9 - 1651.38] power there's a book called the status

[1649.52 - 1653.539] game that a friend of mine and a good

[1651.38 - 1655.5200000000002] Mentor recommended that I read I haven't

[1653.539 - 1657.14] fully read it yet but basically the

[1655.52 - 1659.059] status game says that one of the primary

[1657.14 - 1661.5200000000002] motivations of humans once everything is

[1659.059 - 1663.32] else is taken care of is to achieve more

[1661.52 - 1665.24] status whether that's scientific

[1663.32 - 1667.82] standing social standing commercial

[1665.24 - 1670.88] standing or whatever people want more

[1667.82 - 1673.6399999999999] Fame more power and so

[1670.88 - 1675.2600000000002] whenever Sam Altman says that he has no

[1673.64 - 1678.26] personal stake in this I don't really

[1675.26 - 1679.7] believe it now that being said he does

[1678.26 - 1681.98] say some things that I agree with he

[1679.7 - 1683.419] does some things I disagree with he's a

[1681.98 - 1686.0] complicated person like everyone else

[1683.419 - 1687.98] look at me making YouTube videos wearing

[1686.0 - 1692.659] a Star Trek outfit we're all flawed

[1687.98 - 1695.779] wonderful creatures IBM likewise is a uh

[1692.659 - 1697.94] is a ancient software uh business

[1695.779 - 1699.08] software and hardware company they

[1697.94 - 1701.059] actually I don't know if you know this

[1699.08 - 1702.98] but IBM actually started with mechanical

[1701.059 - 1704.72] time clocks that was their one of their

[1702.98 - 1706.46] first products and it was literally over

[1704.72 - 1707.9] a hundred years ago

[1706.46 - 1709.82] um they have obviously evolved since

[1707.9 - 1712.159] then they are now doing artificial

[1709.82 - 1714.4399999999998] intelligence with Watson X and they're

[1712.159 - 1718.22] also doing Quantum Computing so they are

[1714.44 - 1719.72] pivoting to next-gen stuff again AI it's

[1718.22 - 1721.88] part of what they research but it's only

[1719.72 - 1724.52] a component of their broader portfolio

[1721.88 - 1727.46] of products and services Google again

[1724.52 - 1728.84] Google and Microsoft and openai they're

[1727.46 - 1731.24] all kind of in slightly different places

[1728.84 - 1732.86] Google is still the preeminent search

[1731.24 - 1735.32] engine uh pretty much on the entire

[1732.86 - 1737.4799999999998] planet they're somewhat vulnerable but

[1735.32 - 1739.7] uh particularly if you watched AI

[1737.48 - 1741.2] explained it looks like Gemini is is

[1739.7 - 1742.64] coming down the pipeline and it looks

[1741.2 - 1744.3400000000001] like Gemini is gonna

[1742.64 - 1747.679] blow the roof off of everything

[1744.34 - 1748.9399999999998] hopefully it remains to be seen but

[1747.679 - 1751.4] considering that Google literally

[1748.94 - 1753.6200000000001] started everything with embeddings and

[1751.4 - 1756.02] vectors and Transformers I have a

[1753.62 - 1757.82] sneaking suspicion that that Google once

[1756.02 - 1759.32] they set their mind to dominating AI

[1757.82 - 1760.7] they're gonna get really good at it

[1759.32 - 1762.32] really fast

[1760.7 - 1766.64] so I'm personally not worried about

[1762.32 - 1768.6789999999999] Google and even the CEO of Google has uh

[1766.64 - 1771.5] hinted at the fact that he's not worried

[1768.679 - 1774.38] about open ai's capabilities also Google

[1771.5 - 1775.88] has way more Engineers than openai

[1774.38 - 1778.3990000000001] um and they they hire some smart people

[1775.88 - 1779.96] I've talked to some people at Google

[1778.399 - 1781.8799999999999] um many of them are some of the most

[1779.96 - 1785.1200000000001] friendly and people that I've ever met

[1781.88 - 1786.74] but also damn are they smart holy

[1785.12 - 1789.5] mackerel the people at Google are really

[1786.74 - 1791.96] smart same with Nvidia Nvidia hires the

[1789.5 - 1793.159] best algorithm people on the planet

[1791.96 - 1796.1000000000001] um so they set their mind to something

[1793.159 - 1799.22] they'll figure it out SpaceX Tesla and X

[1796.1 - 1801.1999999999998] all under Elon Musk again Elon Musk is

[1799.22 - 1804.26] afraid of AI but that's not his primary

[1801.2 - 1806.48] uh business model he's focusing on space

[1804.26 - 1807.98] and Rockets and cars and social media

[1806.48 - 1810.5] since he bought Twitter which is a Hot

[1807.98 - 1812.179] Flaming mess right now but AI is just a

[1810.5 - 1814.279] component of all the all of these things

[1812.179 - 1816.5590000000002] he's more concerned about you know

[1814.279 - 1817.64] long-termism in the future of humanity I

[1816.559 - 1819.1399999999999] don't necessarily agree with that

[1817.64 - 1821.24] philosophy but it's at least it's a a

[1819.14 - 1823.94] philosophy and it's a morally consistent

[1821.24 - 1826.52] philosophy and then finally palantir

[1823.94 - 1828.14] they are they're a lot fundamentally a

[1826.52 - 1831.08] law enforcement and intelligence agency

[1828.14 - 1834.0800000000002] and again AI is just a tool for them so

[1831.08 - 1836.059] the point here is pay attention to the

[1834.08 - 1838.1] intrinsic motivations of all of these

[1836.059 - 1840.44] companies and the threats and

[1838.1 - 1843.559] opportunities that AI regulation

[1840.44 - 1845.1200000000001] presents to them in order to know how to

[1843.559 - 1847.1] read the tea leaves when talking about

[1845.12 - 1850.279] regulatory capture

[1847.1 - 1854.0] um so again I am vehemently opposed to

[1850.279 - 1856.22] requiring licensing I am like fully on

[1854.0 - 1858.32] board with open source because like

[1856.22 - 1860.84] here's the thing nobody's trying to

[1858.32 - 1862.48] regulate Linux right but Linux is a

[1860.84 - 1866.1789999999999] powerful operating system with a

[1862.48 - 1867.8600000000001] comprehensive ecosystem of software that

[1866.179 - 1870.74] can be used for hacking and can be used

[1867.86 - 1872.84] for biohacking and can be used for all

[1870.74 - 1875.659] sorts of illegitimate means but nobody

[1872.84 - 1877.4599999999998] cares why because it is out there in a

[1875.659 - 1880.1000000000001] comprehensive cyber security ecosystem

[1877.46 - 1882.559] and the cyber security ecosystem is how

[1880.1 - 1884.6] you get safety not government regulation

[1882.559 - 1887.539] I'm sorry okay

[1884.6 - 1889.8799999999999] this is the cringe-worthy picture that I

[1887.539 - 1892.399] mentioned at the beginning this is what

[1889.88 - 1895.279] really kind of gives me bad vibes

[1892.399 - 1897.7399999999998] because this is Alex carp of the

[1895.279 - 1901.039] intelligence company palantir and Elon

[1897.74 - 1903.32] Musk and again I don't necessarily have

[1901.039 - 1904.58] anything personal against either of

[1903.32 - 1907.279] these guys like I said they're

[1904.58 - 1909.86] complicated humans they they have very

[1907.279 - 1912.74] strong moral convictions and and future

[1909.86 - 1914.6589999999999] oriented goals about the world but it's

[1912.74 - 1916.64] like why would you put them side by side

[1914.659 - 1918.2] and it just doesn't make any sense to me

[1916.64 - 1919.3400000000001] now again it doesn't matter because I'm

[1918.2 - 1920.659] sure they have each other's phone number

[1919.34 - 1923.059] and they can talk on the phone all the

[1920.659 - 1924.7990000000002] time but like I don't know there's just

[1923.059 - 1928.1589999999999] something about this picture that that

[1924.799 - 1930.5] sets me on edge and I'm not I I don't

[1928.159 - 1932.1200000000001] know talk talk to me in the comments how

[1930.5 - 1933.74] do you feel about this picture and this

[1932.12 - 1935.12] whole meeting I'm sure I'll get plenty

[1933.74 - 1937.1] of comments anyways

[1935.12 - 1938.899] thanks for watching thanks for sticking

[1937.1 - 1941.0] around to the end like subscribe support

[1938.899 - 1943.299] me on patreon if you like I also have a

[1941.0 - 1946.34] new upwork so if you want to consult on

[1943.299 - 1948.32] autonomous AI safety whatever reach out

[1946.34 - 1950.799] I'm also on LinkedIn have a good one

[1948.32 - 1950.799] cheers