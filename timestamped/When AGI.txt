[1.02 - 6.2989999999999995] morning everybody David Shapiro here

[3.24 - 10.46] with a video the topic of this video is

[6.299 - 13.019] quite simply when AGI uh simple question

[10.46 - 15.540000000000001] but the underlying answer is a little

[13.019 - 16.8] bit less straightforward and that is uh

[15.54 - 18.56] we're going to talk about exponential

[16.8 - 21.060000000000002] growth

[18.56 - 24.659999999999997] well before we get started we have to

[21.06 - 26.698999999999998] talk about what even is Agi uh the

[24.66 - 31.52] answer is quite simple no one agrees

[26.699 - 36.84] uh when you say AGI it is uh latent with

[31.52 - 40.14] uh assumptions and implications and uh

[36.84 - 42.899] there are people that think AGI is has

[40.14 - 45.18] to be a super intelligent Skynet that

[42.899 - 47.7] can control everything there are people

[45.18 - 49.8] that think AGI has to be embodied that

[47.7 - 51.0] has to be in a robotic form like you and

[49.8 - 53.879] me

[51.0 - 56.1] um so there's no one definition there's

[53.879 - 58.98] plenty of assumptions

[56.1 - 62.160000000000004] um one definition that gets thrown

[58.98 - 64.199] around is that it an AGI is a machine

[62.16 - 66.14] that is capable of ending intellectual

[64.199 - 68.39999999999999] activity of any human

[66.14 - 70.43900000000001] but that's not general intelligence

[68.4 - 72.06] that's super intelligence because if you

[70.439 - 74.15899999999999] have a machine that the one machine that

[72.06 - 75.72] can do anything that eight billion

[74.159 - 77.93900000000001] different people can do

[75.72 - 79.619] excuse me that's uh super intelligence

[77.939 - 83.1] sorry

[79.619 - 85.38] um uh another question is it autonomous

[83.1 - 87.479] or is it reactive only this is another

[85.38 - 89.46] thing that people like if if a machine

[87.479 - 92.03999999999999] just sits there and waits for a human to

[89.46 - 93.78] use it can that even be qualified as AGI

[92.04 - 96.60000000000001] because isn't part of general

[93.78 - 99.24] intelligence the ability to create it

[96.6 - 101.579] and follow its own goals

[99.24 - 103.14] um so to me autonomy is part of it but

[101.579 - 104.77999999999999] people aren't even talk are not even

[103.14 - 106.92] talking about autonomous machines

[104.78 - 110.28] another thing that a lot of people

[106.92 - 112.56] assume is spontaneous learning

[110.28 - 114.72] um and so for humans learning is

[112.56 - 116.399] completely automatic yes you can engage

[114.72 - 117.96] in behaviors which increase learning

[116.399 - 119.93900000000001] like you can go choose to go to school

[117.96 - 120.899] or you can choose to read a book but you

[119.939 - 122.82] don't even need to do that because

[120.899 - 124.68] learning is part of our Hardware it is

[122.82 - 127.55999999999999] part of our intrinsic underlying

[124.68 - 129.179] hardware and so the uh many people think

[127.56 - 130.8] that AGI has to have spontaneous

[129.179 - 131.94] learning I don't know that I agree with

[130.8 - 134.28] that

[131.94 - 136.02] um but again there's no agreement on any

[134.28 - 137.76] of this stuff and then other people say

[136.02 - 140.34] well it's not AGI until it's conscious

[137.76 - 142.379] what even is consciousness Consciousness

[140.34 - 144.3] is an intrinsically subjective

[142.379 - 145.85999999999999] experience so how do you measure a

[144.3 - 147.959] subjective experience for something else

[145.86 - 149.52] you can't from a neuroscience

[147.959 - 151.44] perspective Consciousness is basically

[149.52 - 154.62] whether or not you're awake

[151.44 - 157.26] um or or there's a few other criteria

[154.62 - 159.78] um but it again it mostly subjective so

[157.26 - 162.06] it's very difficult to measure

[159.78 - 164.58] um then the tongue-in-cheek joke is Agi

[162.06 - 166.08] is whatever machines can't do yet which

[164.58 - 168.239] there will always be something that

[166.08 - 170.81900000000002] humans do differently or better or

[168.239 - 173.879] whatever from machines

[170.819 - 175.98] um and if there is a future where there

[173.879 - 179.22] is literally nothing that a human can do

[175.98 - 180.959] that a machine can't also do better well

[179.22 - 183.54] then we're in for a different different

[180.959 - 186.78] type of existence

[183.54 - 188.28] uh personally because of all this AGI is

[186.78 - 190.8] a useless term

[188.28 - 192.84] um I prefer ACOG so fans of my channel

[190.8 - 195.54000000000002] for a while will know that some of my if

[192.84 - 197.22] you go back in in my videos

[195.54 - 199.26] um what I focus on is artificial

[197.22 - 202.5] cognition how do we approximate

[199.26 - 204.78] cognition which is general purpose

[202.5 - 207.48] thought and then it's just a matter of

[204.78 - 209.159] measuring how intelligent it is and of

[207.48 - 211.92] course even intelligence is not a single

[209.159 - 213.54] metric there are many many kinds of

[211.92 - 215.28] intelligence

[213.54 - 217.92] um that are very difficult to measure

[215.28 - 220.98] because uh again it comes down to

[217.92 - 222.0] definitions so with all that said who

[220.98 - 226.01899999999998] cares

[222.0 - 228.599] like AGI is an arbitrary uh goal post

[226.019 - 231.12] that really isn't helpful and doesn't

[228.599 - 233.57999999999998] really mean anything but

[231.12 - 236.159] the world is moving fast so let's move

[233.58 - 240.239] on from the idea of AGI

[236.159 - 243.0] uh but let's let's stick with that idea

[240.239 - 245.09900000000002] of human level of intelligence when will

[243.0 - 246.9] we have machines that are human level of

[245.099 - 248.879] intelligence

[246.9 - 249.84] um let's look at some thermodynamics

[248.879 - 253.439] first

[249.84 - 254.42000000000002] so the human body uses roughly 100 watts

[253.439 - 257.82] of juice

[254.42 - 261.0] and the brain uses roughly 20 of our

[257.82 - 262.68] total oxygen consumption therefore you

[261.0 - 267.18] can estimate that the brain uses about

[262.68 - 269.94] 20 watts of energy that's not much right

[267.18 - 271.62] that's uh that's less than or actually

[269.94 - 274.259] that's more it's a little bit more than

[271.62 - 279.479] the light that's behind my camera

[274.259 - 281.16] um but it's roughly what a 50th of of uh

[279.479 - 284.88] the computer that I'm recording this on

[281.16 - 286.56] right so our brains are really efficient

[284.88 - 289.86] um so that's that's the energetic part

[286.56 - 291.72] but then how uh how many flops how many

[289.86 - 294.06] floating Point operations per second

[291.72 - 296.16] does the human brain do or how much

[294.06 - 298.68] would it take to approximate the human

[296.16 - 300.54] brain obviously our brains are massively

[298.68 - 302.04] parallel

[300.54 - 303.84000000000003] um each neuron is actually pretty slow

[302.04 - 305.94] they operated around a thousand

[303.84 - 307.85999999999996] kilohertz or

[305.94 - 310.44] one kilohertz

[307.86 - 313.74] right our neurons operate at about one

[310.44 - 316.8] kilohertz sorry one thousand Hertz

[313.74 - 318.66] um whereas uh CPUs and computers operate

[316.8 - 320.1] in the gigahertz range but they're much

[318.66 - 322.8] smaller

[320.1 - 324.84000000000003] um so by having a diffuse parallel

[322.8 - 327.72] Network our brains technically operate

[324.84 - 329.63899999999995] much faster but we'll get into that more

[327.72 - 332.1] more details about why this is a bad

[329.639 - 333.84000000000003] analogy but one thing I wanted to point

[332.1 - 336.06] out is that

[333.84 - 337.73999999999995] um estimates about the the flops of the

[336.06 - 340.139] human brain goes up every time we invent

[337.74 - 342.18] new computers if you go back there's

[340.139 - 344.46000000000004] like I think there's an article in wired

[342.18 - 346.08] when deep blue came out and it's like

[344.46 - 347.58] everyone's like deep blue is is as

[346.08 - 351.18] powerful as a human brain and it was

[347.58 - 355.19899999999996] like in the in the uh petaflops or

[351.18 - 357.0] gigaflops uh range and then you know the

[355.199 - 358.56] first the you know the gigaflops

[357.0 - 361.199] computers came out and then the and then

[358.56 - 363.18] the petaflops and tariffs or teraflops

[361.199 - 364.86] and exit flops and every time we come up

[363.18 - 367.38] with a new computer we're like this is

[364.86 - 369.139] as powerful as a human brain same trend

[367.38 - 371.52] for 25 years

[369.139 - 373.08] so either our brains have become

[371.52 - 375.419] exponentially more powerful over the

[373.08 - 377.69899999999996] last 25 years or we don't have the first

[375.419 - 380.639] freaking clue about how powerful our

[377.699 - 382.38] brains actually are so presently we

[380.639 - 384.12] think our brains are exascale computers

[382.38 - 385.86] but it could be beyond that we could

[384.12 - 387.9] have Yoda scale computers in our head

[385.86 - 391.02000000000004] which would be cool because they're only

[387.9 - 392.09999999999997] three pounds and and use uh 20 watts of

[391.02 - 392.75899999999996] power

[392.1 - 395.639] um

[392.759 - 397.97900000000004] nature do be crazy like that uh so by

[395.639 - 400.199] comparison an RTX 3090 weighs about four

[397.979 - 404.28] and a half pounds so our brains weigh

[400.199 - 406.08000000000004] less than a graphics card and yet

[404.28 - 408.02] um are more powerful than the most

[406.08 - 412.02] powerful super computers we have today

[408.02 - 415.31899999999996] so it's safe to assume that there are

[412.02 - 418.31899999999996] just from a pure entropic like physics

[415.319 - 419.94] uh level of looking at it that there are

[418.319 - 422.819] things that our brains can do that other

[419.94 - 424.38] machines cannot do yet just by sheer

[422.819 - 427.68] virtue of efficiency

[424.38 - 429.06] now that being said brains are not

[427.68 - 431.1] computers

[429.06 - 432.419] with some Nuance a lot of nuance so

[431.1 - 434.46000000000004] let's look at the the obvious

[432.419 - 437.21999999999997] differences first different architecture

[434.46 - 439.15999999999997] we don't have a CPU and memory register

[437.22 - 441.18] that are separate it's actually the same

[439.16 - 444.06] one of the ways that your brain

[441.18 - 448.259] maintains its its memory register is

[444.06 - 450.18] through a seven Hertz pulses through the

[448.259 - 453.24] brain and so it's actually kind of a

[450.18 - 456.0] memory state that is maintained along

[453.24 - 457.86] with the synaptic connections so one of

[456.0 - 460.199] the things that is

[457.86 - 462.41900000000004] um there's there's quite a few things

[460.199 - 464.16] about the way that our brains hold

[462.419 - 466.62] memories and store memories and retrieve

[464.16 - 470.16] memories that are different and when you

[466.62 - 471.72] when you uh look closer at neurons you

[470.16 - 474.06] realize that memory and processing are

[471.72 - 477.12] the same so the closest thing that we

[474.06 - 479.699] have in electronics is mem resistors

[477.12 - 482.22] which are not being used yet not widely

[479.699 - 484.88] so fundamentally different architecture

[482.22 - 487.259] two fundamentally different substrate

[484.88 - 491.58] computers all run on Silicon right now

[487.259 - 494.88] and we run on uh organic squishy mostly

[491.58 - 497.58] cholesterol and fat based materials as

[494.88 - 499.44] our substrate it's pretty gross

[497.58 - 501.96] um you know our brains are very very

[499.44 - 503.46] squishy uh let's see I've never touched

[501.96 - 505.44] a human brain I did touch a pig brain

[503.46 - 507.539] when we were uh in middle school when we

[505.44 - 508.86] did dissections

[507.539 - 510.539] um actually I couldn't touch it I was

[508.86 - 513.12] super grossed out

[510.539 - 516.24] um there's different signaling

[513.12 - 518.7] so CPUs use electrons our brains use

[516.24 - 520.52] neurotransmitters and no they don't even

[518.7 - 523.9190000000001] remotely do the same thing

[520.52 - 527.1] neurotransmitters bind to specific sites

[523.919 - 528.779] um and and there's ion channels and this

[527.1 - 532.38] that and the other whereas electrons

[528.779 - 535.5] pass through Gates through metal oxide

[532.38 - 537.8389999999999] Gates and so you might say okay well

[535.5 - 539.7] functionally you know a neurotransmitter

[537.839 - 541.32] is kind of like a gate because it

[539.7 - 542.72] activates a thing and then a signal goes

[541.32 - 546.6] through

[542.72 - 548.82] e-ish you know okay if you really want

[546.6 - 550.44] to break it down and and like squint at

[548.82 - 552.5400000000001] it sure

[550.44 - 555.0] um but then also they use a

[552.54 - 558.48] fundamentally different kind of energy

[555.0 - 562.38] um electromotive Force current versus uh

[558.48 - 565.26] electrochemical uh energy so when you

[562.38 - 567.48] take it all together the brain if it is

[565.26 - 570.0] a computer is a fundamentally different

[567.48 - 571.9200000000001] kind of computer now let's take a step

[570.0 - 573.24] back and just look at it from a raw

[571.92 - 575.0999999999999] physics perspective from a first

[573.24 - 577.26] principle's perspective

[575.1 - 580.9200000000001] brains and computers both process

[577.26 - 583.019] information this is true so you know you

[580.92 - 584.64] say okay well if it if it's a if it's a

[583.019 - 587.22] self-contained object that processes

[584.64 - 589.5] information that's a computer

[587.22 - 592.08] um but if that's your definition of

[589.5 - 594.0] computer then your entire body is a

[592.08 - 595.8000000000001] computer because you process information

[594.0 - 598.32] whether it's physical information or

[595.8 - 599.9399999999999] auditory information or all the

[598.32 - 602.6400000000001] processes going on inside of your body

[599.94 - 604.86] then your body is also a computer

[602.64 - 607.98] um but your your body can't do all the

[604.86 - 609.9590000000001] intellectual tasks on its own that we

[607.98 - 611.339] want computers to do so does it count is

[609.959 - 613.0189999999999] it like oh well that's not what I meant

[611.339 - 613.62] by computer

[613.019 - 616.92] um

[613.62 - 618.839] so the the definition of it processes

[616.92 - 621.5999999999999] information is not necessarily a useful

[618.839 - 623.8800000000001] definition of computer because

[621.6 - 625.6800000000001] if the definition of computer is any

[623.88 - 627.36] object or anything that processes

[625.68 - 629.0999999999999] information then everything in the

[627.36 - 631.14] universe like like Mike this this

[629.1 - 632.94] process is information this process is

[631.14 - 635.9399999999999] information on the quantum level

[632.94 - 637.6800000000001] it's processing the the mechanical wave

[635.94 - 639.7790000000001] of my voice as it passes through it it's

[637.68 - 641.3389999999999] processing the heat from my T that's

[639.779 - 643.38] traversing it

[641.339 - 646.0790000000001] um but again that's not useful to us in

[643.38 - 648.18] in terms of intelligence

[646.079 - 649.92] um and uh the entire universe is

[648.18 - 652.56] technically a computer from the

[649.92 - 655.079] perspective of physics so again like

[652.56 - 657.8389999999999] depending on your scale not really

[655.079 - 659.6999999999999] helpful so for the sake of this argument

[657.839 - 662.1] we're just going to say human brains are

[659.7 - 664.2] not actually computers

[662.1 - 666.12] um even though technically yes they

[664.2 - 668.1600000000001] process information but it's not the

[666.12 - 669.899] same they just the the way that they

[668.16 - 670.74] process information is fundamentally

[669.899 - 674.88] different

[670.74 - 676.5600000000001] okay so where are we AGI is undefinable

[674.88 - 678.66] but robots are real

[676.56 - 681.3599999999999] um so we could keep arguing over what

[678.66 - 683.459] AGI means and what the goals are but

[681.36 - 687.1800000000001] it's this imaginary goal post that keeps

[683.459 - 688.3199999999999] moving so let's look at what is real so

[687.18 - 690.66] what are some things that are actually

[688.32 - 693.0600000000001] happening Private Industry is going as

[690.66 - 695.04] fast as possible every single chip

[693.06 - 697.3199999999999] manufacturer and tech company out there

[695.04 - 700.14] is investing in AI as well as every

[697.32 - 704.1] military and every government they do

[700.14 - 705.42] not care what AGI is

[704.1 - 707.88] um there's a lot of money to be made

[705.42 - 709.4399999999999] right and where there's where there's

[707.88 - 711.54] money to be made you know the

[709.44 - 713.7] capitalists and the corporatists and the

[711.54 - 716.16] neoliberals are going to go in full

[713.7 - 718.8000000000001] force right why do you think that that

[716.16 - 721.019] crypto went really big is because there

[718.8 - 722.76] was money to be made and but it was the

[721.019 - 726.48] wild west and so there's no regulation

[722.76 - 728.3389999999999] and so then you had the FTX collapse

[726.48 - 730.44] um so where there's a will there's a way

[728.339 - 733.019] and there's a heck of a lot of willpower

[730.44 - 736.2600000000001] in AI right now

[733.019 - 738.66] um the only entity is even capable of

[736.26 - 741.18] slowing this down are governments and

[738.66 - 744.959] militaries but they want it too

[741.18 - 747.5999999999999] so like this is going as fast as humanly

[744.959 - 750.06] possible and I picked the Zoidberg meme

[747.6 - 751.98] because I can already hear some people

[750.06 - 753.779] in the comments saying blah blah you're

[751.98 - 755.279] wrong about all this

[753.779 - 756.72] um and you know you're allowed to have

[755.279 - 759.019] that opinion and you're also allowed to

[756.72 - 759.019] be wrong

[759.36 - 763.98] um the fact of the matter is AI is

[761.94 - 766.2600000000001] dangerous and disruptive long before we

[763.98 - 768.3000000000001] achieve anything remotely called AGI it

[766.26 - 771.899] doesn't matter it's a useless term

[768.3 - 774.24] um AI can make our lives better now it

[771.899 - 777.3] can also make our lives worse now look

[774.24 - 779.94] at how AI is being deployed for

[777.3 - 781.9799999999999] um the social credit system and and face

[779.94 - 784.9200000000001] tracking and and

[781.98 - 787.9200000000001] um even even dumber things basic machine

[784.92 - 790.5] learning uh regression models predicting

[787.92 - 791.9399999999999] um you know crime uh and and overly

[790.5 - 795.66] targeting

[791.94 - 798.0600000000001] um poor communities uh the real problem

[795.66 - 801.06] is not the machine the real problem is

[798.06 - 804.3] how we implement the machine whether we

[801.06 - 806.04] implement it with benevolent intent and

[804.3 - 808.9799999999999] benevolent uh

[806.04 - 810.12] uh actual uh deployment because there's

[808.98 - 813.72] always the law of unintended

[810.12 - 816.42] consequences or malevolent deployment do

[813.72 - 819.0] we put this stuff in uh weapons of war

[816.42 - 820.74] and and tools of control and

[819.0 - 823.44] manipulation

[820.74 - 826.98] um by the way this is why

[823.44 - 829.86] um companies like meta or Facebook have

[826.98 - 831.54] suddenly lost so much Faith with the

[829.86 - 833.339] entire market share is because

[831.54 - 834.779] everyone's like actually yeah like we

[833.339 - 837.24] don't like our attention and our

[834.779 - 839.82] emotions being gamified just for the

[837.24 - 843.36] sake of corporate profits

[839.82 - 844.9200000000001] uh that is at least that's why I deleted

[843.36 - 847.74] my accounts

[844.92 - 849.899] um so but you know who knows

[847.74 - 853.5] um so and the point is the problem

[849.899 - 856.32] exists today right now irrespective of

[853.5 - 858.42] AGI all right now I know you're probably

[856.32 - 860.399] like glazing over because I promised

[858.42 - 862.26] when are we going to have AGI I just

[860.399 - 865.26] needed to set the stage so let's move on

[862.26 - 867.6] so let's ignore the semantics

[865.26 - 870.5] and just let's look at the data let's

[867.6 - 873.899] look at the actual Trends to figure out

[870.5 - 875.76] what's happening and when

[873.899 - 877.2] um so the first question is since we're

[875.76 - 879.54] looking at data

[877.2 - 880.74] I want you to keep in mind that there

[879.54 - 883.139] are

[880.74 - 887.699] excuse me that there are two overarching

[883.139 - 891.48] kinds of graphs here now everyone has

[887.699 - 893.519] assumed that exponential growth

[891.48 - 894.86] um is is the way because of things like

[893.519 - 897.3] Moore's law

[894.86 - 899.6990000000001] but I want to call your attention to the

[897.3 - 902.2199999999999] possibility that until you get to the

[899.699 - 904.62] halfway point of a sigmoid curve it

[902.22 - 907.019] looks exponential and so over the last

[904.62 - 908.82] few years we've heard like oh more law

[907.019 - 910.8] is wearing out and no one can agree on

[908.82 - 913.98] whether or not that's true so if Moore's

[910.8 - 915.7199999999999] Law is in fact wearing out then maybe it

[913.98 - 917.94] was never an exponential growth curve

[915.72 - 921.72] maybe it was always a sigmoid growth

[917.94 - 924.24] curve and so maybe we're actually here

[921.72 - 926.1] where it's starting to slow down which

[924.24 - 928.92] if that's the case

[926.1 - 931.98] then maybe things aren't happening as

[928.92 - 934.3199999999999] quickly as we thought and maybe uh Ray

[931.98 - 935.82] Kurzweil was wrong and that the

[934.32 - 938.88] singularity is not near but maybe it's

[935.82 - 941.88] actually like a couple centuries away if

[938.88 - 944.16] we have diminishing returns

[941.88 - 946.92] then maybe it is much further away than

[944.16 - 949.68] we thought or

[946.92 - 951.7199999999999] if we have compounding returns it could

[949.68 - 954.899] be way closer than we think

[951.72 - 956.5790000000001] so let's explore the data and see which

[954.899 - 959.399] way it goes

[956.579 - 961.38] so first I apologize for the blurriness

[959.399 - 963.6] of this I couldn't find a larger higher

[961.38 - 967.8] quality version so this is the Energy

[963.6 - 971.399] Efficiency the the gigaflops per watt of

[967.8 - 974.459] AI technology and so you look here

[971.399 - 977.82] notice first the scale is logarithmic

[974.459 - 980.88] this this level is uh 10 gigaflops per

[977.82 - 984.24] watt then 100 then a thousand so each

[980.88 - 986.76] each major rank goes up by a factor of

[984.24 - 988.6800000000001] 10. so this is this is logarithmic

[986.76 - 990.42] growth this is the same thing this is

[988.68 - 992.579] the same way that decibels are measured

[990.42 - 995.459] and it's the same way uh same as the

[992.579 - 997.8] Richter scale for earthquakes so like

[995.459 - 1000.8] you know a three on the Richter scale is

[997.8 - 1003.92] 10 times weaker than a four and a five

[1000.8 - 1006.079] is ten times stronger than a four uh

[1003.92 - 1009.259] same thing for for decibels

[1006.079 - 1013.2399999999999] so in this case you see like okay in

[1009.259 - 1017.1800000000001] 2011 just 11 years ago we were at seven

[1013.24 - 1020.5] gigawatts per flops or uh uh yeah uh

[1017.18 - 1023.4799999999999] seven seven uh G flops per watt and then

[1020.5 - 1025.04] last year well I guess it's 2023 now so

[1023.48 - 1028.459] two years ago

[1025.04 - 1030.799] um we were uh hovering about about 70.

[1028.459 - 1033.14] so it went up by a factor of 10 but then

[1030.799 - 1034.699] we also started doing a lower uh

[1033.14 - 1036.439] floating point and we had new new

[1034.699 - 1038.419] technologies we had the tensor cores

[1036.439 - 1041.6000000000001] come out and so then we had this Quantum

[1038.419 - 1044.24] Leap so in the space of 10 years we went

[1041.6 - 1046.6989999999998] from less than 10 gigaflops per watt to

[1044.24 - 1049.94] over a thousand so in 10 years we went

[1046.699 - 1054.02] up by a factor of a thousand so what if

[1049.94 - 1055.94] 10 years from now are um our AI chips

[1054.02 - 1058.46] are a thousand times more efficient than

[1055.94 - 1061.3400000000001] they are today that's insane to think

[1058.46 - 1063.919] about because then then it's not a

[1061.34 - 1066.1999999999998] matter of like oh can you run gpt3 like

[1063.919 - 1068.96] on a big giant computer in the cloud you

[1066.2 - 1071.9] could run gpt3 on like an RTX 80 90

[1068.96 - 1073.1000000000001] right like when Nvidia comes you know

[1071.9 - 1075.5] just that like that's just a couple

[1073.1 - 1077.36] Generations like if if Nvidia comes out

[1075.5 - 1080.48] with a new generation every year and a

[1077.36 - 1083.84] half so that's what that's uh

[1080.48 - 1087.26] um that would be six six ish Generations

[1083.84 - 1090.4399999999998] so we're on the 40 90 right now right so

[1087.26 - 1093.2] then you go to like the the 10 the 190

[1090.44 - 1095.72] or the 90 90 or whatever right so then

[1093.2 - 1100.16] like it's conceivable that you could run

[1095.72 - 1103.34] these language models on uh on a on a on

[1100.16 - 1104.48] a home GPU within a few years within a

[1103.34 - 1106.1] decade

[1104.48 - 1108.559] um that's pretty crazy it's actually

[1106.1 - 1110.539] probably much less than that because I

[1108.559 - 1112.16] think that uh I think I did the math and

[1110.539 - 1113.36] it would take about I think it would

[1112.16 - 1117.679] take about

[1113.36 - 1119.9599999999998] um 90 uh mid-range gpus today

[1117.679 - 1123.38] so you know if we're if we're going up

[1119.96 - 1124.7] by a factor of 10 or a thousand in uh if

[1123.38 - 1125.96] we're going up by a factor of a thousand

[1124.7 - 1127.5800000000002] in 10 years

[1125.96 - 1129.559] um it'll be a lot less before we go up

[1127.58 - 1132.98] by a factor of 90. um it'll be about

[1129.559 - 1135.02] half that so you know the the Energy

[1132.98 - 1136.64] Efficiency is going up we're still a

[1135.02 - 1139.16] long ways off from the Energy Efficiency

[1136.64 - 1141.3200000000002] of our brain remember we might have we

[1139.16 - 1143.24] currently we think we have exascale

[1141.32 - 1146.12] computers in our head that run on 20

[1143.24 - 1147.6200000000001] watts of power and weigh three pounds

[1146.12 - 1150.5] um so we're a long ways off from that

[1147.62 - 1152.36] but remember the problem is today if

[1150.5 - 1154.16] it's already dangerous at these levels

[1152.36 - 1156.32] of efficiency how much more dangerous

[1154.16 - 1157.64] could it be at those at a thousand times

[1156.32 - 1158.8999999999999] more efficient

[1157.64 - 1161.3600000000001] okay

[1158.9 - 1165.02] so remember we talked about the

[1161.36 - 1167.12] possibility of diminishing returns

[1165.02 - 1169.1] um if we so so far on the hardware side

[1167.12 - 1170.84] we don't have diminishing returns we

[1169.1 - 1172.8799999999999] actually have accelerating returns as

[1170.84 - 1174.74] new technologies come out so this is

[1172.88 - 1177.8600000000001] this is a point in the column for

[1174.74 - 1180.26] exponential growth and maybe even

[1177.86 - 1182.0] um hyperbolic growth or parabolic growth

[1180.26 - 1183.32] parabolic growth is the one that goes to

[1182.0 - 1185.179] infinite

[1183.32 - 1189.1399999999999] um if we have parabolic growth I didn't

[1185.179 - 1191.3600000000001] even have a a a graph for that parabolic

[1189.14 - 1193.3400000000001] growth is even scarier and if I'm saying

[1191.36 - 1194.9599999999998] that wrong please let me know I'm sure

[1193.34 - 1196.6999999999998] someone will in the comments it's either

[1194.96 - 1198.799] hyperbolic or parabolic one of those

[1196.7 - 1200.72] goes to infinite the asymptote is like

[1198.799 - 1203.36] out there and it's like oh at 10 years

[1200.72 - 1204.2] you actually have infinite growth

[1203.36 - 1207.1999999999998] um

[1204.2 - 1208.64] so anyways so there's there's one point

[1207.2 - 1210.0800000000002] in the in the possibility that things

[1208.64 - 1211.76] are actually accelerating and the

[1210.08 - 1213.1399999999999] exponential growth might even be

[1211.76 - 1215.12] accelerating

[1213.14 - 1217.2800000000002] um so let's see if let's let's take a

[1215.12 - 1219.799] look at diminishing returns

[1217.28 - 1221.48] um for the last couple years the uh the

[1219.799 - 1222.44] the Mantra has been scale is all you

[1221.48 - 1225.02] need

[1222.44 - 1226.8200000000002] um a buddy of mine at open AI he

[1225.02 - 1228.32] messaged me this is a couple years ago I

[1226.82 - 1231.26] haven't heard from him in in like a year

[1228.32 - 1232.46] or two because because open AI has been

[1231.26 - 1233.78] very busy

[1232.46 - 1236.059] um but one of the one of the last

[1233.78 - 1238.7] messages he told me is like yeah as far

[1236.059 - 1240.62] as we can tell like all you need to do

[1238.7 - 1242.96] is just throw more data at it and more

[1240.62 - 1244.34] parameters and it just becomes more

[1242.96 - 1246.08] intelligent so we're going to see how

[1244.34 - 1249.02] far that takes us

[1246.08 - 1252.1999999999998] um and boy howdy has that worked so far

[1249.02 - 1254.6] um but here's the thing rumor has it

[1252.2 - 1256.3400000000001] that gpt4 has been trained on most of

[1254.6 - 1258.559] the internet like most of the data on

[1256.34 - 1259.1599999999999] the entire internet

[1258.559 - 1261.799] um

[1259.16 - 1265.039] which is just mind-blowing so if we run

[1261.799 - 1268.28] out of data to train it on what else is

[1265.039 - 1270.5] there like like okay that maybe uh

[1268.28 - 1274.039] someone one person had a hypothesis that

[1270.5 - 1275.6] that's why open AI trained whisper was

[1274.039 - 1276.679] so that they could get more data out of

[1275.6 - 1278.8999999999999] videos

[1276.679 - 1281.419] um and and podcasts uh because there's

[1278.9 - 1283.46] something like what was it like 90 years

[1281.419 - 1285.5590000000002] worth of videos on YouTube that seems

[1283.46 - 1287.1200000000001] low I think it's more than that but the

[1285.559 - 1290.72] point is is there are many many years

[1287.12 - 1293.12] worth of content just on YouTube Alone

[1290.72 - 1294.6200000000001] um that could be transcribed with a tool

[1293.12 - 1296.0] like whisper

[1294.62 - 1298.76] excuse me I don't know why I get so

[1296.0 - 1301.1] congested in the mornings um so that's

[1298.76 - 1303.3799999999999] one possibility then you transcribe all

[1301.1 - 1305.6] the podcasts right and then you go

[1303.38 - 1307.4] multimodal if you can describe what's

[1305.6 - 1310.2199999999998] happening on the videos and you you

[1307.4 - 1312.38] describe it with uh with language you

[1310.22 - 1315.2] can get even more data right but point

[1312.38 - 1317.8400000000001] is they're going to run out of data so

[1315.2 - 1320.3600000000001] there might be diminishing returns

[1317.84 - 1322.58] there's also some evidence

[1320.36 - 1325.34] that the larger the model you actually

[1322.58 - 1328.8799999999999] don't necessarily get the the returns

[1325.34 - 1331.28] for fully uh dense models that once you

[1328.88 - 1332.72] get larger that you probably have to go

[1331.28 - 1333.86] sparse

[1332.72 - 1336.559] um and so like once you get to the

[1333.86 - 1339.6789999999999] trillion parameter it's almost certain

[1336.559 - 1342.98] that uh you need to have a sparse model

[1339.679 - 1345.14] so my personal hypothesis is that gpt4

[1342.98 - 1347.72] it might be a thousand times bigger than

[1345.14 - 1349.7] gpt3 but it's probably a sparse model

[1347.72 - 1352.159] which means not all of it is going to

[1349.7 - 1355.1000000000001] activate at any given moment so maybe

[1352.159 - 1356.72] gpd4 could be just as efficient to run

[1355.1 - 1359.1789999999999] as gpd3 even though it could be

[1356.72 - 1362.0] hypothetically much much larger

[1359.179 - 1364.76] but this is all speculation we should

[1362.0 - 1366.559] know by the end of 2023 hopefully within

[1364.76 - 1369.26] the next few months when gpt4 comes out

[1366.559 - 1371.84] and we can add another another Branch to

[1369.26 - 1375.74] this but also take a look at this this

[1371.84 - 1378.559] also scales logarithmically right so uh

[1375.74 - 1381.98] one billion 10 billion 100 billion uh a

[1378.559 - 1384.1399999999999] trillion so if this trend continues then

[1381.98 - 1386.24] that's evidence for exponential but you

[1384.14 - 1388.8200000000002] look at this where where Megatron Turing

[1386.24 - 1390.86] kind of tapered off right so is that

[1388.82 - 1393.6789999999999] which way is the curve going is it gonna

[1390.86 - 1396.1999999999998] is it going to Plateau or is gpt4 going

[1393.679 - 1398.48] to be up here in like you know a couple

[1396.2 - 1400.76] ranks up and just show that actually no

[1398.48 - 1402.679] it's continuing to accelerate so this is

[1400.76 - 1403.4] like if they're running out of data I

[1402.679 - 1404.659] don't know what they're going to do

[1403.4 - 1405.26] about that

[1404.659 - 1410.419] um

[1405.26 - 1412.34] so uh yeah so this one's kind of a wash

[1410.419 - 1414.74] um we'll know more once gpt4 comes out

[1412.34 - 1416.299] or or any other competitors right I know

[1414.74 - 1417.919] Google is working on their own

[1416.299 - 1420.32] competitors

[1417.919 - 1423.26] um and you know Megatron this is um this

[1420.32 - 1424.9399999999998] is NVIDIA so we'll see um maybe meta

[1423.26 - 1426.86] will come out with something but I think

[1424.94 - 1430.159] the biggest players to look look at are

[1426.86 - 1432.1399999999999] going to be open AI Nvidia

[1430.159 - 1434.3600000000001] um and and actually those are the two

[1432.14 - 1436.5200000000002] biggest ones in Google probably so keep

[1434.36 - 1437.12] an eye on those three

[1436.52 - 1438.86] um

[1437.12 - 1441.7399999999998] so far

[1438.86 - 1444.02] it looks like exponential hasn't given

[1441.74 - 1448.28] out and it has been a durable

[1444.02 - 1450.08] trend for like a century so this um this

[1448.28 - 1451.58] was posted on Reddit and I think I

[1450.08 - 1452.96] believe it came from Source Time

[1451.58 - 1454.6999999999998] Magazine

[1452.96 - 1456.74] um so this was I think this was an

[1454.7 - 1459.2] article talking about

[1456.74 - 1462.5] um Ray kurzweil's uh prediction for the

[1459.2 - 1465.0800000000002] singularity so you look all the way back

[1462.5 - 1467.6] where we had electromechanical computers

[1465.08 - 1469.22] and then relay based computers and

[1467.6 - 1471.3799999999999] vacuum tubes and transistors and then

[1469.22 - 1474.6200000000001] integrated circuits and then who knows

[1471.38 - 1476.5390000000002] what comes out now but the prediction is

[1474.62 - 1479.4189999999999] that is that these computers that

[1476.539 - 1481.64] computers will surpass human brain power

[1479.419 - 1483.74] this year

[1481.64 - 1485.6000000000001] um we'll see you know but that's the

[1483.74 - 1488.539] other thing with with exponential growth

[1485.6 - 1490.3999999999999] is that very small changes in that

[1488.539 - 1491.299] growth trajectory could mean that it

[1490.4 - 1493.4] happens

[1491.299 - 1495.62] five years from now or it means that it

[1493.4 - 1496.96] might have already happened who knows

[1495.62 - 1500.6] so

[1496.96 - 1504.02] so far it seems like there's a lot of

[1500.6 - 1505.34] evidence that exponential uh is still uh

[1504.02 - 1509.059] the name of the game

[1505.34 - 1510.9189999999999] um and that it hasn't given out yet so

[1509.059 - 1513.3799999999999] um if if we are still growing

[1510.919 - 1516.919] exponentially and we're going up by a

[1513.38 - 1519.0800000000002] factor of like 10 every five years

[1516.919 - 1521.3600000000001] um that and and sometimes it feels like

[1519.08 - 1522.86] even less than that then it's really

[1521.36 - 1525.02] difficult to predict what will be

[1522.86 - 1527.6] possible even just two years from now I

[1525.02 - 1530.48] mean and honestly a year ago today who

[1527.6 - 1532.82] would have predicted GPT that chat GPT

[1530.48 - 1534.919] would have had the impact that it did so

[1532.82 - 1538.7] we'll see what happens

[1534.919 - 1540.7990000000002] now uh it's really difficult to wrap

[1538.7 - 1543.26] your head around exponential growth and

[1540.799 - 1545.72] we'll get into a little bit more why but

[1543.26 - 1548.419] the the biggest thing that people

[1545.72 - 1551.1200000000001] um have have witnessed or experienced in

[1548.419 - 1553.279] their lifetime is the exponential growth

[1551.12 - 1555.32] of human population now human population

[1553.279 - 1557.299] actually follows a sigmoid growth curve

[1555.32 - 1559.52] because we're approaching the carrying

[1557.299 - 1561.98] capacity of the planet which means the

[1559.52 - 1563.299] competition for resources is making life

[1561.98 - 1565.279] harder

[1563.299 - 1567.62] um so like I don't care about government

[1565.279 - 1569.36] policies the number one reason that

[1567.62 - 1571.52] housing prices are going up and food

[1569.36 - 1574.6399999999999] prices are going up it's not inflation

[1571.52 - 1577.7] it's it has to do with the thermodynamic

[1574.64 - 1579.5] limits of the entire planet

[1577.7 - 1581.6000000000001] um as we approach the carrying capacity

[1579.5 - 1584.179] of the planet just by virtue of

[1581.6 - 1586.4599999999998] everything is more constrained prices

[1584.179 - 1588.38] will go up life gets harder stress goes

[1586.46 - 1591.679] up etc etc

[1588.38 - 1593.38] um so overpopulation is actually uh to

[1591.679 - 1595.5800000000002] me a larger problem

[1593.38 - 1598.159] geopolitically than climate change

[1595.58 - 1600.74] because climate change just makes that

[1598.159 - 1602.1200000000001] worse the competition worse but humans

[1600.74 - 1604.94] have survived climate change before

[1602.12 - 1606.9189999999999] we've been through ice ages right

[1604.94 - 1609.6200000000001] um that's not the problem the problem is

[1606.919 - 1612.38] the additional stress that climate

[1609.62 - 1613.9399999999998] change puts on us at the same time that

[1612.38 - 1616.0390000000002] we're also approaching the carrying

[1613.94 - 1618.74] capacity of the planet that's when

[1616.039 - 1622.4] people get real mad anyways

[1618.74 - 1624.559] so I was born in 1986 so the world

[1622.4 - 1625.76] population was around 5 billion when I

[1624.559 - 1628.6399999999999] was born

[1625.76 - 1631.1589999999999] um and today it is we already passed 8

[1628.64 - 1632.779] billion so this is actually out of date

[1631.159 - 1636.159] um we're we're already above 8 billion

[1632.779 - 1639.2] so in my lifetime three billion humans

[1636.159 - 1642.2600000000002] additional humans this is net gain of 3

[1639.2 - 1644.3600000000001] billion has happened and the way that

[1642.26 - 1647.36] that looks is that like the city that

[1644.36 - 1648.62] I'm in constantly grows like where are

[1647.36 - 1650.1789999999999] all these people coming from I know that

[1648.62 - 1651.6789999999999] they're coming from somewhere and some

[1650.179 - 1655.1000000000001] places have populations that are

[1651.679 - 1656.539] declining but subjectively that's what

[1655.1 - 1657.799] it feels like and this is why it's

[1656.539 - 1659.84] difficult to wrap your head around it

[1657.799 - 1662.0] because the Earth is really big there's

[1659.84 - 1662.539] people all over the place

[1662.0 - 1664.52] um

[1662.539 - 1666.86] so why is it so hard for us to think

[1664.52 - 1668.6589999999999] exponentially why is it why is it that

[1666.86 - 1670.1589999999999] like you look at this and like we just

[1668.659 - 1671.72] can't even wrap our head around three

[1670.159 - 1673.279] billion people like that just that's

[1671.72 - 1675.14] just a number right what does three

[1673.279 - 1677.779] billion people even look like you can't

[1675.14 - 1679.1000000000001] you can't see it all in one place it's

[1677.779 - 1681.74] just too much

[1679.1 - 1685.1589999999999] so the reason is we evolved to think

[1681.74 - 1688.1] locally and geometrically because our

[1685.159 - 1690.5590000000002] our chimpanzee ancestors they pick up

[1688.1 - 1693.1399999999999] stick they hit thing that is bad or or

[1690.559 - 1695.539] food and then they eat it right and so

[1693.14 - 1697.7] they their their life is based on what

[1695.539 - 1699.74] they can see in their immediate uh

[1697.7 - 1701.779] vicinity and maybe things that they

[1699.74 - 1703.94] remember like oh the river is is just a

[1701.779 - 1706.58] little further away right

[1703.94 - 1708.26] um but then like hitting things throwing

[1706.58 - 1710.36] things running away hiding climbing up a

[1708.26 - 1711.86] tree these are all geometric problems

[1710.36 - 1713.059] right

[1711.86 - 1716.4189999999999] um it's something that you can interact

[1713.059 - 1719.0] with it's something that's tangible but

[1716.419 - 1721.94] where we live now is a global and

[1719.0 - 1724.94] exponential world like it's if if

[1721.94 - 1726.799] there's a million people working on AI

[1724.94 - 1728.6000000000001] and and they're all getting compounding

[1726.799 - 1730.4] returns and another million people

[1728.6 - 1732.1399999999999] working on better chips and they're

[1730.4 - 1734.299] getting compounding returns and another

[1732.14 - 1736.3600000000001] million people working on software and

[1734.299 - 1739.52] they're getting compounding returns

[1736.36 - 1741.9799999999998] we cannot wrap our heads around that we

[1739.52 - 1745.179] have no evolutionary

[1741.98 - 1748.88] um history that that has even primed us

[1745.179 - 1750.8600000000001] to think like this and even people who

[1748.88 - 1753.5590000000002] practice thinking like this it's still

[1750.86 - 1754.1589999999999] it's never intuitive

[1753.559 - 1757.82] um

[1754.159 - 1759.679] and so like a couple of of dumb examples

[1757.82 - 1761.779] I picked for things that like why it's

[1759.679 - 1764.419] not intuitive is because you never go

[1761.779 - 1766.64] from 30 miles an hour to 3000 miles an

[1764.419 - 1768.74] hour right in a car like the only people

[1766.64 - 1770.8400000000001] who can do that are like fighter pilots

[1768.74 - 1772.46] and the people who pilot the space

[1770.84 - 1773.6] shuttle well the space shuttle's been

[1772.46 - 1775.82] retired

[1773.6 - 1777.1999999999998] um but anyway now they're just riding in

[1775.82 - 1778.46] a capsule they're not even steering they

[1777.2 - 1780.5] can't even see where they're going it's

[1778.46 - 1781.7] just you Buckle in and you have g-forces

[1780.5 - 1784.64] right

[1781.7 - 1786.799] um and so and then also your microwave

[1784.64 - 1789.2] it doesn't go from like you know room

[1786.799 - 1791.48] temperature to 200 Fahrenheit to 2000

[1789.2 - 1793.22] Fahrenheit right so unless you're like a

[1791.48 - 1795.26] metal worker that you know where you've

[1793.22 - 1796.88] got a blast furnace that goes to 5000

[1795.26 - 1799.1589999999999] Centigrade or whatever

[1796.88 - 1801.14] um like you cannot think

[1799.159 - 1803.8990000000001] um you you have never even experienced

[1801.14 - 1806.24] something that is truly exponential

[1803.899 - 1808.4599999999998] um and so the fact that most of us never

[1806.24 - 1810.5] experience anything exponential means

[1808.46 - 1812.8990000000001] that it it with that without that

[1810.5 - 1815.12] experience we cannot intuitively think

[1812.899 - 1818.299] exponentially

[1815.12 - 1820.8799999999999] um so what then people fall back on is

[1818.299 - 1823.399] their gut response right when you think

[1820.88 - 1825.5] about AI it's like oh well humans are

[1823.399 - 1828.1399999999999] special so this is another thing humans

[1825.5 - 1830.539] love to romanticize human intelligence

[1828.14 - 1831.5590000000002] like we're we're just intrinsically

[1830.539 - 1832.82] special

[1831.559 - 1835.039] um but if you go back to that model

[1832.82 - 1836.84] where like okay what's special about the

[1835.039 - 1839.96] brain it's three three pounds of

[1836.84 - 1841.6999999999998] cholesterol and and neurotransmitters

[1839.96 - 1844.039] um what's actually special about that

[1841.7 - 1846.38] there's nothing there's there's no

[1844.039 - 1849.86] there's no uh Secret Sauce there's no

[1846.38 - 1852.919] magic substance that you know makes our

[1849.86 - 1855.1999999999998] brain unique it's not made of any uh

[1852.919 - 1856.8200000000002] original elements

[1855.2 - 1859.46] um there's no structures in the brain

[1856.82 - 1862.22] that um are impossible to either

[1859.46 - 1865.3400000000001] simulate or approximate

[1862.22 - 1867.38] so it's just so there there's the

[1865.34 - 1869.299] emotional the visceral reaction saying

[1867.38 - 1871.279] nothing will ever be like us because

[1869.299 - 1873.44] that's very comforting thought there's

[1871.279 - 1875.059] the there's the uh emotional rejection

[1873.44 - 1877.64] of oh it's nothing to worry about

[1875.059 - 1879.62] because the the actual possibility is it

[1877.64 - 1882.98] is so terrifying that you don't even

[1879.62 - 1884.779] want to engage with it right and so the

[1882.98 - 1887.32] combination of the emotional response

[1884.779 - 1891.26] and our inability to think exponentially

[1887.32 - 1893.96] combines to say like you know

[1891.26 - 1895.94] yeah like the The Singularity is much

[1893.96 - 1896.779] closer or whatever however you want to

[1895.94 - 1898.1000000000001] call it

[1896.779 - 1902.36] so

[1898.1 - 1907.1589999999999] if it is all exponential

[1902.36 - 1909.5] then we have already crossed the Rubicon

[1907.159 - 1912.5590000000002] and for those who are not history nerds

[1909.5 - 1915.98] the Rubicon is a tiny little river in uh

[1912.559 - 1918.98] in in now Italy where uh Julius Caesar

[1915.98 - 1920.6] took his army across because you're you

[1918.98 - 1923.419] were technically not allowed to take

[1920.6 - 1925.6999999999998] your army past that and uh so that was

[1923.419 - 1928.5200000000002] basically him like saying the dye is

[1925.7 - 1929.919] cast I am fully committed we are now

[1928.52 - 1932.84] fully committed

[1929.919 - 1934.1000000000001] the genie is out of the bottle and I

[1932.84 - 1936.3799999999999] remember a couple years ago I couldn't

[1934.1 - 1938.779] find this quote but I remember seeing

[1936.38 - 1940.94] Elon Musk in a uh interview he said

[1938.779 - 1944.84] things get really interesting by 2024 or

[1940.94 - 1946.22] 2025. and I think he was probably late I

[1944.84 - 1948.4399999999998] think things get really interesting this

[1946.22 - 1950.84] year I think that 2023 is the year that

[1948.44 - 1952.76] things get really interesting

[1950.84 - 1954.98] um so

[1952.76 - 1958.279] the short answer is are we going to have

[1954.98 - 1959.84] AGI this year I don't think so but I

[1958.279 - 1961.22] don't care things are already

[1959.84 - 1962.9599999999998] interesting and they're only going to

[1961.22 - 1965.539] get more interesting

[1962.96 - 1967.7] so you're probably just raging like just

[1965.539 - 1970.1589999999999] tell me when you know and I'm going to

[1967.7 - 1972.919] say that it's not a matter of when

[1970.159 - 1975.5] it's it's not even a matter of if it is

[1972.919 - 1978.919] happening now and the choice we have to

[1975.5 - 1982.279] make is utopian World on the left or

[1978.919 - 1985.279] dystopian World on the right and you

[1982.279 - 1988.1] know like we we everyone I don't mean

[1985.279 - 1989.659] like you me every you know every Joe or

[1988.1 - 1991.399] whatever I mean billionaires I mean

[1989.659 - 1995.1200000000001] politicians I mean military leaders

[1991.399 - 1997.399] everyone has a role to play in making

[1995.12 - 1999.3799999999999] this choice

[1997.399 - 2002.1999999999998] so how do we make this choice

[1999.38 - 2005.8600000000001] well first vote

[2002.2 - 2010.0] vote for politicians that uh are are not

[2005.86 - 2011.86] insane and and are not uh lunatics

[2010.0 - 2014.019] um think for yourself

[2011.86 - 2016.4189999999999] um but more than thinking learn

[2014.019 - 2019.6] uh go read

[2016.419 - 2021.1000000000001] go experiment go Tinker on your own talk

[2019.6 - 2023.74] to people

[2021.1 - 2027.519] um the best the social trust social

[2023.74 - 2031.539] proof and and social consensus is one of

[2027.519 - 2033.3990000000001] the best ways to participate I

[2031.539 - 2035.08] experiment that's why I have my YouTube

[2033.399 - 2037.059] channel is so that I can demonstrate

[2035.08 - 2039.039] these things proposed Solutions I've

[2037.059 - 2040.539] written a few books on this

[2039.039 - 2044.08] I have written books on the control

[2040.539 - 2047.44] problem benevolent by Design I wrote a

[2044.08 - 2050.379] solution for this because believe it or

[2047.44 - 2052.7200000000003] not the problem between utopian society

[2050.379 - 2055.06] and dystopian Society has nothing to do

[2052.72 - 2057.7799999999997] with technology and it has everything to

[2055.06 - 2060.7] do with our philosophical disposition

[2057.78 - 2062.9190000000003] towards each other but more importantly

[2060.7 - 2065.6789999999996] towards ourselves so I wrote a book

[2062.919 - 2067.96] called post nihilism because this on the

[2065.679 - 2071.56] left is post nihilism this on the right

[2067.96 - 2075.2200000000003] is nihilism and so we have to choose

[2071.56 - 2077.859] post nihilism if we want that you don't

[2075.22 - 2080.74] have to do anything but in my book post

[2077.859 - 2083.2599999999998] nihilism I outline that it that choosing

[2080.74 - 2085.1789999999996] a philosophy is an arbitrary choice so

[2083.26 - 2086.619] we could just arbitrarily choose the

[2085.179 - 2087.28] left one

[2086.619 - 2088.7200000000003] um

[2087.28 - 2089.98] so that's one of the solutions I

[2088.72 - 2092.2] proposed

[2089.98 - 2095.56] um either way buckle up

[2092.2 - 2098.56] it's happening it's happening faster and

[2095.56 - 2099.82] there's nothing we can do to stop it

[2098.56 - 2102.22] um don't stick your head in the sand

[2099.82 - 2105.04] don't pretend like it's not happening

[2102.22 - 2106.9599999999996] and uh the time to act is now and I

[2105.04 - 2110.02] added a little graphic of the of a bad

[2106.96 - 2112.66] alignment take Bingo which is pretty

[2110.02 - 2114.04] it's pretty cynical and pretty funny

[2112.66 - 2115.42] because it's like

[2114.04 - 2117.16] it's actually a really serious

[2115.42 - 2119.38] conversation

[2117.16 - 2120.5789999999997] but like AGI is too far away to worry

[2119.38 - 2122.619] about right now

[2120.579 - 2124.3] nobody who's actually working on this

[2122.619 - 2127.06] stuff believes that

[2124.3 - 2128.5600000000004] the only people who say AGI oh it's

[2127.06 - 2131.02] never going to happen it's decades away

[2128.56 - 2134.14] the people who actually work on large

[2131.02 - 2136.119] language models and and and and basic

[2134.14 - 2138.0989999999997] research like basic science and

[2136.119 - 2141.1600000000003] algorithmic improvements none of them

[2138.099 - 2143.7400000000002] think that they say oh hey we already

[2141.16 - 2146.0789999999997] have you know human level performance or

[2143.74 - 2148.359] superhuman level performance on a broad

[2146.079 - 2150.3590000000004] array of tasks and that number is going

[2148.359 - 2152.0989999999997] up by a factor of a hundred every year

[2150.359 - 2154.7799999999997] so

[2152.099 - 2157.48] what happens next

[2154.78 - 2160.119] um uh oh that's the that's the end I

[2157.48 - 2163.06] forgot to add a uh thank you and exit uh

[2160.119 - 2165.52] card so that's that

[2163.06 - 2168.82] um the short answer is will we ever see

[2165.52 - 2172.9] AGI maybe not but it's happening today

[2168.82 - 2175.48] it's exciting today 2023 is uh the year

[2172.9 - 2177.96] that it changes so get engaged thanks

[2175.48 - 2177.96] for watching