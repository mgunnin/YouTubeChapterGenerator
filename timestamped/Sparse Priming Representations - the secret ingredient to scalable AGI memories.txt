[0.84 - 4.859999999999999] hey everybody David Shapiro here with a

[3.36 - 6.899] video so

[4.86 - 9.179] um one I've been scarce and I apologize

[6.899 - 11.219000000000001] I am feeling better

[9.179 - 13.2] um recovering from burnout although I

[11.219 - 14.58] still need like some days just doing

[13.2 - 16.619] nothing

[14.58 - 19.8] um but anyways

[16.619 - 23.64] um so y'all are really clamoring for me

[19.8 - 24.9] to continue the um the Q a chat but not

[23.64 - 26.76] that one

[24.9 - 28.5] um and then the salience and

[26.76 - 31.199] anticipating

[28.5 - 34.38] um you know and auto Muse and all that

[31.199 - 37.32] fun stuff so all these chat Bots

[34.38 - 40.879000000000005] um I will continue working on them

[37.32 - 45.059] but I kind of got to a stopping point

[40.879 - 47.579] where uh basically the problem is memory

[45.059 - 50.64] right so whether you're looking at

[47.579 - 54.059] hundreds of scientific articles or an

[50.64 - 56.1] arbitrarily long uh chat conversation or

[54.059 - 57.959999999999994] an entire novel

[56.1 - 60.36] um semantic search is just not good

[57.96 - 62.1] enough breaking it up and chunking and

[60.36 - 65.7] and stuff so we need a more

[62.1 - 68.82000000000001] sophisticated a more organized uh memory

[65.7 - 71.46000000000001] system for AI for autonomous AI

[68.82 - 73.86] and so this is what I proposed

[71.46 - 76.619] um and so basically there's there's

[73.86 - 78.119] there's uh episodic memory there's two

[76.619 - 80.6] primary kinds of memory in the human

[78.119 - 83.159] brain there's episodic memory which is

[80.6 - 85.14] chronologically linear so that is the

[83.159 - 88.259] lived experience the live narrative that

[85.14 - 90.479] is the a linear account of the

[88.259 - 93.06] sensations you know your external senses

[90.479 - 94.38] and your internal thoughts

[93.06 - 96.9] um those are the two primary things that

[94.38 - 98.579] you got Sensations thoughts and then in

[96.9 - 100.74000000000001] thoughts are

[98.579 - 102.89999999999999] um decisions uh memories that have been

[100.74 - 104.46] recalled so on and so forth but you

[102.9 - 106.259] forget most of this most of this is

[104.46 - 108.0] noise right you don't need to remember

[106.259 - 109.92] that you remembered something at all

[108.0 - 111.36] times you just have like oh I'm thinking

[109.92 - 114.42] about you know that time I went to the

[111.36 - 116.28] beach right and then you know anyways

[114.42 - 118.799] so you don't necessarily need to record

[116.28 - 120.96000000000001] all your thoughts but you definitely

[118.799 - 124.02000000000001] need to record uh to a certain extent

[120.96 - 126.78] what's coming in and then you you slot

[124.02 - 128.51999999999998] that into some kind of framework

[126.78 - 131.64000000000001] um so

[128.52 - 134.22] this is going to be the underpinning uh

[131.64 - 136.61999999999998] work and I have written in all three of

[134.22 - 139.62] my books so far that like I was putting

[136.62 - 141.78] off memory systems because it is a super

[139.62 - 144.18] non-trivial problem and it turns out

[141.78 - 146.7] it's now the problem that like we all

[144.18 - 149.4] have to solve so I'm working with

[146.7 - 150.66] um a few people uh on various cognitive

[149.4 - 152.16] architectures and we're actually going

[150.66 - 153.42] to have some demos coming up in the

[152.16 - 155.4] coming weeks

[153.42 - 156.72] um because fortunately I'm no longer the

[155.4 - 158.459] only person working on cognitive

[156.72 - 160.85999999999999] architectures yay

[158.459 - 163.62] um the idea is catching on

[160.86 - 165.0] um so with that being said though

[163.62 - 167.459] um

[165.0 - 170.76] the this is this is a very difficult

[167.459 - 173.4] problem and so the idea is Okay so we've

[170.76 - 175.44] got raw data coming in right it's it's

[173.4 - 177.42000000000002] unstructured the only well it's it's

[175.44 - 180.239] semi-structured the only structure is

[177.42 - 181.73899999999998] you know what time series it has but

[180.239 - 183.239] other other than that you don't know

[181.739 - 184.92000000000002] what

[183.239 - 186.3] um what the topic is going to be and the

[184.92 - 187.98] topics are going to change right and

[186.3 - 191.34] there might be gaps in the time

[187.98 - 193.98] so what we do is we take a chunk of logs

[191.34 - 195.54] an arbitrary chunk of logs based on that

[193.98 - 198.72] are temporally bounded

[195.54 - 201.48] and you get an executive summary of that

[198.72 - 203.04] information and in this chunk so this is

[201.48 - 204.959] like going to be another Json file or

[203.04 - 206.64] whatever you have pointers back to the

[204.959 - 208.92000000000002] original log so that you can reconstruct

[206.64 - 211.14] the memory because using sparse pointers

[208.92 - 212.94] is actually a big thing that human

[211.14 - 215.39999999999998] brains do

[212.94 - 217.379] um and so then this is basically a a

[215.4 - 219.72] very sparse summary and I'll show you

[217.379 - 222.17999999999998] what I mean by sparse summary and then

[219.72 - 224.519] finally as you accumulate more of these

[222.18 - 227.15900000000002] summaries you eventually merge these

[224.519 - 231.31900000000002] into a knowledge graph or a cluster them

[227.159 - 234.06] and then use that clustering to make uh

[231.319 - 236.94] to make like Wiki articles or KB

[234.06 - 240.54] articles and give me just a second

[236.94 - 244.799] sorry I needed my coffee okay so anyways

[240.54 - 247.019] um yeah so this is the scheme and I

[244.799 - 250.5] spent a long time talking through this

[247.019 - 253.5] with chat gpt4 so you can see this is a

[250.5 - 255.659] whoops this is a come on

[253.5 - 258.0] Why is the

[255.659 - 260.03999999999996] why is the okay it doesn't want to

[258.0 - 262.019] scroll anyways you can see it is a very

[260.04 - 265.139] very long conversation I talked through

[262.019 - 266.88] code I talk through the math I talked

[265.139 - 270.0] through the concept

[266.88 - 271.62] and so anyways at the very end of it I

[270.0 - 273.06] said can you write an executive summary

[271.62 - 276.479] of the problem we're trying to solve

[273.06 - 277.919] here and so this is just taking a step

[276.479 - 282.24] back for a second

[277.919 - 285.06] I am using gpt4 to help solve the

[282.24 - 287.04] problems of AGI artificial general

[285.06 - 289.44] intelligence or what I call autonomous

[287.04 - 291.66] cognitive entities

[289.44 - 293.699] so the problem at hand involves

[291.66 - 295.62] designing an efficient memory system for

[293.699 - 297.78000000000003] an autonomous cognitive entity or an ace

[295.62 - 299.639] that can manage a large and constantly

[297.78 - 301.979] growing Corpus of text Data generated

[299.639 - 303.419] through thoughts inputs and outputs this

[301.979 - 305.21999999999997] data can accumulate to hundreds of

[303.419 - 307.919] gigabytes per year potentially reaching

[305.22 - 310.56] millions or billions of individual logs

[307.919 - 312.24] the primary challenge is to organize and

[310.56 - 314.16] compress these logs into a manageable

[312.24 - 315.6] set of knowledge-based Articles while

[314.16 - 317.28000000000003] retaining as much meaningful information

[315.6 - 319.56] as possible

[317.28 - 321.17999999999995] this is such a concise summary I could

[319.56 - 323.82] not have done better

[321.18 - 325.919] our proposed hour see it's it's already

[323.82 - 328.259] the the collective because where it

[325.919 - 330.18] understands that we're collaborating our

[328.259 - 332.039] proposed solution involves a multi-level

[330.18 - 333.96] approach with the first level being the

[332.039 - 337.4] consolidation of raw logs into roll-up

[333.96 - 337.4] summaries so that's this

[337.5 - 341.34] um these Roll-Ups serve as compressed

[339.6 - 343.03900000000004] representations of the original logs

[341.34 - 345.84] reducing the total number of Records

[343.039 - 347.699] then we employ a gating or threshold

[345.84 - 349.56] function to determine whether a roll-up

[347.699 - 352.44] is semantically similar enough to an

[349.56 - 355.139] existing KB articles or if it if it

[352.44 - 356.699] should be added as a new article this

[355.139 - 359.1] approach allows the KB to adapt

[356.699 - 360.84000000000003] organically to the evolving data while

[359.1 - 362.639] maintaining scalability

[360.84 - 364.79999999999995] the key aspects to consider in this

[362.639 - 366.36] solution are the choice of similarity

[364.8 - 367.979] threshold and semantic similarity

[366.36 - 369.91900000000004] measure as well as the balance between

[367.979 - 372.36] number of KB articles and their quality

[369.919 - 373.79999999999995] periodic evaluation and fine-tuning of

[372.36 - 376.08000000000004] the system will help ensure its

[373.8 - 380.1] continued Effectiveness as data grows

[376.08 - 383.419] okay so this is a very very condensed

[380.1 - 385.56] text summary of this system

[383.419 - 388.74] and then

[385.56 - 391.139] so I mentioned sparsity right so I've

[388.74 - 395.039] been reading this book

[391.139 - 398.94] behave so as always neuroscience and

[395.039 - 401.15999999999997] life inspires what I'm working on and

[398.94 - 402.6] one of the one of the experiments or

[401.16 - 404.639] actually several the experiments that he

[402.6 - 406.319] talks about in this book has to do with

[404.639 - 408.6] linguistic priming

[406.319 - 412.199] and so an example of linguistic priming

[408.6 - 415.319] in humans in Psychology is that if you

[412.199 - 417.96000000000004] use just a few words

[415.319 - 420.66] um kind of placed arbitrarily it will

[417.96 - 424.19899999999996] really change someone's cognition so one

[420.66 - 427.02000000000004] example was they did a test with Asian

[424.199 - 429.0] women and if you remind the Asian women

[427.02 - 430.979] of The Stereotype that Asians are better

[429.0 - 433.74] at math before giving them a math test

[430.979 - 436.31899999999996] they do better if you remind them of The

[433.74 - 438.66] Stereotype that uh that women are bad at

[436.319 - 439.86] math than they do worse and then of

[438.66 - 441.78000000000003] course if you just give them neutral

[439.86 - 443.819] priming they kind of you know perform in

[441.78 - 446.63899999999995] the middle and there's plenty of

[443.819 - 448.91900000000004] examples of priming um Darren Darren

[446.639 - 452.639] Brown the the British dude The Mentalist

[448.919 - 454.79999999999995] he used a lot of priming to get people

[452.639 - 456.539] to like do all kinds of cool stuff this

[454.8 - 458.52000000000004] was back in the 90s

[456.539 - 460.979] um but like one one experiment that he

[458.52 - 463.139] did was he had a bunch of like marketing

[460.979 - 465.36] guys and he put them in a car and drove

[463.139 - 467.759] them around town and he drove them by

[465.36 - 470.34000000000003] like a specific set of billboards

[467.759 - 472.74] and so they were primed with images and

[470.34 - 475.38] words and then he asked them to solve a

[472.74 - 477.3] particular marketing problem and he had

[475.38 - 479.58] almost exactly predicted what they were

[477.3 - 483.479] going to produce based on how they had

[479.58 - 486.35999999999996] been primed now I noticed that large

[483.479 - 488.52] language models can also be primed and

[486.36 - 490.38] so what I mean by primed is that by just

[488.52 - 492.79999999999995] sprinkling in a few of the correct words

[490.38 - 495.9] and terms it will then be able to

[492.8 - 497.40000000000003] reproduce or reconstruct whatever it is

[495.9 - 499.31899999999996] that you're talking about so what I want

[497.4 - 503.17999999999995] to do is I want to show you that because

[499.319 - 505.56] this this really high density

[503.18 - 507.72] way of compressing things is what I call

[505.56 - 510.74] sparse priming representations

[507.72 - 514.08] is going to be super important

[510.74 - 516.3] for managing uh artificial cognitive

[514.08 - 518.339] entities or AGI memories because here's

[516.3 - 520.38] the thing large language models already

[518.339 - 523.6800000000001] have a tremendous amount of foundational

[520.38 - 525.959] knowledge so all you need to do is prime

[523.68 - 527.12] it with just a few rules and statements

[525.959 - 531.42] and assertions

[527.12 - 533.399] that will allow it to um just basically

[531.42 - 535.0799999999999] kind of remember or reconstruct the

[533.399 - 536.399] concept so what I'm going to do is I'm

[535.08 - 539.7] going to take this

[536.399 - 542.04] and put it into a new chat and we're

[539.7 - 545.72] going to go to gpt4

[542.04 - 548.3389999999999] and I'll say the following is a sparse

[545.72 - 552.0] priming representation

[548.339 - 554.1600000000001] of a concept or topic

[552.0 - 558.42] um oh wow they they reduced it from 100

[554.16 - 560.16] messages to 50. I guess they're busy uh

[558.42 - 562.68] unsurprising

[560.16 - 567.98] um please reconstruct

[562.68 - 567.9799999999999] the topic or Concept in detail

[568.62 - 572.36] and so here's what we'll do

[573.0 - 577.32] so with just a handful of statements and

[576.3 - 580.4399999999999] assertions

[577.32 - 584.1] I will show you that gpt4

[580.44 - 587.58] in the form of chat gpt4 is highly

[584.1 - 590.94] capable of reconstituting this very

[587.58 - 593.1] complex topic just by virtue of the fact

[590.94 - 594.9590000000001] that it um it already has a tremendous

[593.1 - 597.9200000000001] amount of background knowledge and

[594.959 - 597.92] processing capability

[598.019 - 602.58] um okay

[599.88 - 603.959] so there we go so the autonomous uh

[602.58 - 605.399] cognitive entity is an advanced

[603.959 - 608.04] artificial intelligence system to design

[605.399 - 609.959] it yep okay there you go

[608.04 - 612.8389999999999] um

[609.959 - 614.8199999999999] so it's kind of it's It's reconstructing

[612.839 - 616.32] what this multi-level approach so what

[614.82 - 619.38] it's doing here is it's kind of re

[616.32 - 621.72] restating uh everything

[619.38 - 623.399] um but what you'll see is that it will

[621.72 - 626.7] be able to confabulate and kind of fill

[623.399 - 628.32] in the blanks and so by having a sparse

[626.7 - 630.0600000000001] representation

[628.32 - 632.6400000000001] it kind of guides how it's going to

[630.06 - 635.0999999999999] confabulate and this can be used for all

[632.64 - 636.72] kinds of tasks right so some of my

[635.1 - 638.1] patreon supporters I'm not going to give

[636.72 - 640.38] anything away because I respect my

[638.1 - 643.44] patreon supporters privacy but they ask

[640.38 - 645.0] me like how do I represent X Y or Z and

[643.44 - 647.2790000000001] what I'm going to say is this is a way

[645.0 - 649.38] to represent a lot of stuff

[647.279 - 652.98] um what whatever whatever your domain of

[649.38 - 655.079] expertise is you can ask it to do what I

[652.98 - 657.0] did in there which is say just give me a

[655.079 - 659.3389999999999] short list of you know statements

[657.0 - 662.399] assertions explanations such that a

[659.339 - 664.98] subject matter expert could re um could

[662.399 - 666.0] uh reconstitute it

[664.98 - 668.76] um

[666.0 - 672.0] there we go and so here here it's it's

[668.76 - 673.74] figuring this out as it goes periodic

[672.0 - 676.32] evaluation and necessary to continued

[673.74 - 678.24] efficiency this may be involve adjusting

[676.32 - 680.5790000000001] the similarity threshold refining

[678.24 - 681.9590000000001] semantic similarity measure modifying

[680.579 - 684.06] other aspects

[681.959 - 685.5] sparse priming representation is a

[684.06 - 687.3] technique using conjunction to fill

[685.5 - 689.64] acetate knowledge transfer and

[687.3 - 692.279] reconstruction spr concise statements

[689.64 - 695.16] are generated to summarize yeah so it

[692.279 - 697.32] even understands just by virtue of

[695.16 - 698.6999999999999] saying this is an spr and a brief

[697.32 - 700.5] definition it understands the

[698.7 - 704.1] implications

[700.5 - 707.94] um there you go so now that it has has

[704.1 - 710.22] um has reconstituted it we can say Okay

[707.94 - 712.5] um great thanks

[710.22 - 717.0] um can you discuss

[712.5 - 720.18] how we could uh go about implementing

[717.0 - 723.959] this for a chat bot

[720.18 - 723.959] and so again because

[724.2 - 731.22] um because this uh because gpt4 already

[728.399 - 733.62] knows a whole bunch of coding and data

[731.22 - 735.86] and stuff it's going to be able to talk

[733.62 - 739.68] through the process

[735.86 - 742.92] so this is going to

[739.68 - 744.779] okay I don't think it fully

[742.92 - 746.76] I gave it very simple instructions let's

[744.779 - 748.92] see where it goes because often what

[746.76 - 751.079] happens is and someone someone pointed

[748.92 - 752.9399999999999] this out to me is that it'll kind of

[751.079 - 755.04] talk through the problem and then give

[752.94 - 756.899] you the answer so I learned the hard way

[755.04 - 759.899] just be patient what it's basically

[756.899 - 762.06] doing is it's talking itself through

[759.899 - 764.579] um the the problem in the solution

[762.06 - 766.26] so anyways excuse me I don't know why

[764.579 - 769.019] I'm so hoarse

[766.26 - 770.76] um but yeah so this is this is what I'm

[769.019 - 773.399] working on right now and this is going

[770.76 - 775.86] to have implications for for all all

[773.399 - 777.839] chat Bots but also all autonomous AI

[775.86 - 779.399] because again

[777.839 - 781.5] um you know this is this is like the

[779.399 - 782.76] first two minutes of conversation but

[781.5 - 783.959] what happens when you have a million

[782.76 - 786.42] logs what happens when you have a

[783.959 - 789.42] billion logs so one thing that I suspect

[786.42 - 791.399] will happen is

[789.42 - 794.459] um the number of whoops

[791.399 - 798.06] nah come back no

[794.459 - 800.6999999999999] um I suspect that the number of logs

[798.06 - 805.56] will go up geometrically

[800.7 - 808.74] but what I also suspect is that the um

[805.56 - 812.76] is that the number of KB articles will

[808.74 - 815.9590000000001] actually go up and approach an asymptote

[812.76 - 815.959] how do you get it to stop

[816.959 - 820.38] there you go so I think I think that

[819.06 - 822.7199999999999] this is kind of how it'll look where

[820.38 - 825.24] like when you're when your Ace is new

[822.72 - 827.94] when it's young it'll be creating a

[825.24 - 830.16] bunch of new KB articles uh very quickly

[827.94 - 831.839] but then over time the number of KB

[830.16 - 833.88] articles will taper off because say for

[831.839 - 836.1600000000001] instance there's only a finite amount of

[833.88 - 838.8] information to learn about you and then

[836.16 - 840.92] there will be a very slow trickle as

[838.8 - 844.26] your life progresses right

[840.92 - 846.8389999999999] and we can also exclude KB articles

[844.26 - 849.54] about basic World Knowledge right all it

[846.839 - 852.0600000000001] needs all your Ace needs is KB articles

[849.54 - 854.519] about truly new novel and unique

[852.06 - 856.4399999999999] information it doesn't need to record a

[854.519 - 859.98] world model the world model is baked

[856.44 - 863.0400000000001] into gpt4 and future models now one

[859.98 - 866.22] other thing was because this is kind of

[863.04 - 867.54] incrementally adding the KB articles

[866.22 - 869.7] um let's see what it came up with okay

[867.54 - 872.579] so talk through the problem

[869.7 - 875.1600000000001] um one thing is that I asked it for the

[872.579 - 877.8] pros and cons so right here

[875.16 - 879.8389999999999] uh using a gating or and this is this is

[877.8 - 881.399] how sophisticated it is

[879.839 - 883.5600000000001] um using a gating or threshold function

[881.399 - 885.42] to compare Roll-Ups against existing KBS

[883.56 - 887.399] can be a viable alternative to

[885.42 - 889.38] clustering so basically what we were

[887.399 - 892.32] exploring was what if we use a

[889.38 - 894.24] clustering algorithm to to

[892.32 - 895.8000000000001] um figure out the chunks but then I was

[894.24 - 898.86] like okay but we're not gonna We're Not

[895.8 - 900.899] Gonna regenerate the uh the KB articles

[898.86 - 902.94] every single time because that's going

[900.899 - 906.24] to be prohibitively expensive so what if

[902.94 - 907.98] we treat it more incrementally

[906.24 - 909.54] um let's see this approach involves

[907.98 - 911.04] comparing semantic similarity between a

[909.54 - 912.06] new roll-up and existing KB articles if

[911.04 - 913.98] it doesn't meet a predetermined

[912.06 - 916.079] threshold okay so the pros it's simple

[913.98 - 917.519] this approach is conceptually simple and

[916.079 - 920.0999999999999] can be easier to implement compared to

[917.519 - 921.779] clustering algorithms yes scalability as

[920.1 - 923.88] new Roll-Ups are processed individually

[921.779 - 926.16] the computational complexity of updating

[923.88 - 927.3] KB articles grows linearly with the

[926.16 - 928.38] number of Roll-Ups making it more

[927.3 - 930.8389999999999] scalable

[928.38 - 932.22] Dynamic growth the number of KB articles

[930.839 - 935.519] can grow organically with the addition

[932.22 - 938.0400000000001] of new rollups and then the cons it very

[935.519 - 940.44] rightly identifies sub-optimal

[938.04 - 943.26] organization because we're not using

[940.44 - 945.6] Global clustering that's fine redundancy

[943.26 - 947.16] there's a risk of creating similar KB

[945.6 - 949.019] articles

[947.16 - 951.48] um depending on the semantic similarity

[949.019 - 954.54] and then parameter sensitivity so on and

[951.48 - 956.82] so forth now that being said there is a

[954.54 - 959.2199999999999] final step that we were that I was going

[956.82 - 962.22] to talk about which is every now and

[959.22 - 964.86] then we should do a re-indexing event

[962.22 - 966.72] and so basically what that says is when

[964.86 - 969.0600000000001] you're when your Ace is offline during

[966.72 - 971.22] the dream sequence right so real-time

[969.06 - 973.8599999999999] learning it can update the KB articles

[971.22 - 976.6990000000001] in real time but then the dream sequence

[973.86 - 979.1990000000001] it will delete all the KB articles

[976.699 - 981.779] cluster the chunks based on semantic

[979.199 - 984.4799999999999] similarity and then based on those

[981.779 - 985.62] chunks write a whole new set of KB

[984.48 - 987.48] articles

[985.62 - 989.579] and so every now and then your

[987.48 - 993.1800000000001] autonomous cognitive entity is going to

[989.579 - 995.9399999999999] update its entire internal Wiki and then

[993.18 - 999.4799999999999] these internal wikis are going to be the

[995.94 - 1001.519] primary source of information for your

[999.48 - 1004.639] uh for your for your cognitive entity

[1001.519 - 1006.32] and so instead of searching millions of

[1004.639 - 1009.0790000000001] logs you're going to be searching

[1006.32 - 1011.899] hundreds or maybe a couple thousand KB

[1009.079 - 1013.459] articles which is a much more tractable

[1011.899 - 1015.8] problem

[1013.459 - 1017.2399999999999] um to find the correct thing and also

[1015.8 - 1018.8599999999999] they can be cross-linked to each other

[1017.24 - 1020.1800000000001] right because these KB articles these

[1018.86 - 1022.1] wikis

[1020.18 - 1024.26] um can be nodes and a knowledge graph

[1022.1 - 1026.6] which means it's like so my fiance was

[1024.26 - 1028.8799999999999] like okay so I was explaining it to her

[1026.6 - 1032.8999999999999] and she's like so what if it has what if

[1028.88 - 1035.419] it has a um an article on me and an

[1032.9 - 1037.3390000000002] article on her would it link the two of

[1035.419 - 1038.98] us and say that like we're engaged and

[1037.339 - 1041.24] you know our relationship has been ex

[1038.98 - 1044.72] long and I'm like yes we could probably

[1041.24 - 1046.339] do that it might also topically

[1044.72 - 1048.799] um so in terms of the kinds of topics

[1046.339 - 1050.78] here's another important thing in terms

[1048.799 - 1053.66] of kinds of topics we're probably going

[1050.78 - 1055.8799999999999] to have have it focus on people

[1053.66 - 1059.059] events

[1055.88 - 1061.88] um things like objects

[1059.059 - 1063.98] um as well as Concepts so a concept

[1061.88 - 1066.3200000000002] could be like the concept of the

[1063.98 - 1071.179] autonomous cognitive entity so people

[1066.32 - 1073.6399999999999] events things and Concepts and included

[1071.179 - 1077.96] in things are like places right so like

[1073.64 - 1082.16] the year 1080 the the place Paris France

[1077.96 - 1084.08] right so those are all viable nodes for

[1082.16 - 1085.94] a Knowledge Graph so that's that's kind

[1084.08 - 1087.799] of where we're at

[1085.94 - 1089.96] um yeah I think that's all I'm going to

[1087.799 - 1091.6399999999999] do today because like this is a lot and

[1089.96 - 1093.44] you can see that this conversation was

[1091.64 - 1096.14] very long

[1093.44 - 1098.72] um and uh but yeah so let me know what

[1096.14 - 1100.94] you think in the comments we are

[1098.72 - 1102.919] continuing to work

[1100.94 - 1105.0800000000002] um I had a few other things that I was

[1102.919 - 1106.4] going to say but I forgot them this is

[1105.08 - 1107.84] the most important thing and this is

[1106.4 - 1110.8400000000001] this is the hardest problem I'm working

[1107.84 - 1112.9399999999998] on and once I unlock this it's going to

[1110.84 - 1115.1599999999999] unlock a lot more work because think

[1112.94 - 1117.3200000000002] about think about breaking what if these

[1115.16 - 1119.539] logs instead of like our conversation

[1117.32 - 1122.299] what if these logs are scientific papers

[1119.539 - 1125.0] or what if these logs are scenes in a

[1122.299 - 1128.299] book right pretty much everything can be

[1125.0 - 1129.5] represented this way I think and then

[1128.299 - 1131.9] once you have these higher order

[1129.5 - 1133.58] abstractions and all of them point back

[1131.9 - 1135.799] so here's another really important thing

[1133.58 - 1137.98] that I forgot to mention is that there's

[1135.799 - 1140.48] metadata attached with each of these

[1137.98 - 1142.039] entities that points back to the

[1140.48 - 1144.919] original so you can you can still

[1142.039 - 1146.48] reconstruct the original information so

[1144.919 - 1148.7] if you have like you know a topical

[1146.48 - 1151.76] article here it'll point to all the

[1148.7 - 1153.74] chunks that were in that cluster that um

[1151.76 - 1155.12] that helped create it and then each of

[1153.74 - 1156.86] those chunks will point back to the

[1155.12 - 1159.1999999999998] original logs so you have kind of a

[1156.86 - 1162.62] pyramid shape

[1159.2 - 1164.179] um yeah so that's what I'm working on uh

[1162.62 - 1166.539] that's it I'll call it a day thanks for

[1164.179 - 1166.539] watching