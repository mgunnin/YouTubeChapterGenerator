[1.319 - 6.4799999999999995] hey everyone David Shapiro here with

[4.2 - 8.639] today's video good morning

[6.48 - 10.679] today's topic is going to be a little

[8.639 - 13.62] bit more severe and a little bit more

[10.679 - 17.4] intense so the title of today's video is

[13.62 - 19.619] Agi Unleashed what happens when we lose

[17.4 - 22.979999999999997] control

[19.619 - 25.14] part one the coming storm

[22.98 - 27.359] as everyone is aware things are ramping

[25.14 - 30.779] up very quickly there are people calling

[27.359 - 33.0] for moratoriums on AI research and while

[30.779 - 35.1] some of us don't take it seriously there

[33.0 - 36.6] are very legitimate concerns about

[35.1 - 39.239000000000004] what's going on

[36.6 - 41.82] and in other words we're in the end game

[39.239 - 44.76] now we are in the ramp up towards AGI

[41.82 - 47.399] and the singularity whether or not you

[44.76 - 51.059] are emotionally ready for it

[47.399 - 53.36] so just as a tiny bit of evidence this

[51.059 - 55.5] paper from nature

[53.36 - 58.32] demonstrates that we are in an

[55.5 - 60.84] exponential ramp up on AI whatever else

[58.32 - 63.239] is true the investment is there the

[60.84 - 66.06] research is there it's happening and

[63.239 - 68.64] it's not slowing down

[66.06 - 70.5] now I'm probably preaching to the choir

[68.64 - 73.2] and rehashing old

[70.5 - 75.299] stuff that everyone knows but let's just

[73.2 - 78.659] briefly talk about the existential risks

[75.299 - 80.759] there are two overarching themes or

[78.659 - 86.10000000000001] categories of the existential risks

[80.759 - 89.06] risks for AGI one is the basically just

[86.1 - 92.939] the deliberate weaponization of AI

[89.06 - 95.46000000000001] some people are comparing AGI to nuclear

[92.939 - 98.52] weapons beyond that there's the

[95.46 - 103.69999999999999] potential of cyber warfare drone Warfare

[98.52 - 106.79899999999999] autonomous uh tanks artillery aircraft

[103.7 - 108.659] submarines so on and so forth many of

[106.799 - 110.659] these systems are already being

[108.659 - 114.36] developed and deployed

[110.659 - 116.28] so basically ai ai is already being

[114.36 - 117.24] weaponized it's just a matter of on what

[116.28 - 120.18] scale

[117.24 - 122.46] moreover the second category is

[120.18 - 125.159] accidental outcomes or accident or

[122.46 - 128.16] unintended consequences basically what

[125.159 - 130.5] if the AGI escapes but there's a few

[128.16 - 132.84] other possible avenues for this to

[130.5 - 133.86] happen for instance runaway corporate

[132.84 - 135.78] greed

[133.86 - 138.06] you bet your bottom dollar that the

[135.78 - 140.64000000000001] first corporation that can create AGI to

[138.06 - 143.81900000000002] automate profits is going to do so

[140.64 - 145.44] uh more uh beyond that there's political

[143.819 - 147.66] corruption and just straight up

[145.44 - 149.28] political incompetence this is something

[147.66 - 151.85999999999999] that has been uh discussed actually

[149.28 - 154.04] quite a bit in uh my comments section

[151.86 - 159.9] for my most recent videos which is that

[154.04 - 162.29999999999998] uh yes uh like Italy uh uh uh Britain

[159.9 - 164.22] and a few other places are doing their

[162.3 - 166.92000000000002] best to kind of get ahead of the curve

[164.22 - 169.019] but the fact of the matter is is that

[166.92 - 171.66] governments by and large are moving too

[169.019 - 174.3] slow and politicians just don't get it

[171.66 - 177.66] and then finally you can even have

[174.3 - 180.48000000000002] situations where otherwise benign agis

[177.66 - 183.18] collaborate and eventually turn on us

[180.48 - 185.94] so the existential risks are there

[183.18 - 187.8] and you guys most most of you know me I

[185.94 - 189.78] am very optimistic I'm very sanguine

[187.8 - 192.239] about all this and by the end of the

[189.78 - 194.519] video I hope that many of you will see

[192.239 - 198.12] where I'm coming from

[194.519 - 201.659] now even if AGI doesn't wipe us out

[198.12 - 203.87900000000002] there are still lots and lots of risks

[201.659 - 206.819] um job loss economic changes social

[203.879 - 210.48] upheaval so on and so forth mostly it

[206.819 - 212.57999999999998] comes down to economic shifts who gets

[210.48 - 216.06] to capture all of the tremendous wealth

[212.58 - 218.09900000000002] generated by AGI and even if AGI is safe

[216.06 - 219.84] and doesn't kill us there's still every

[218.099 - 221.94] possibility that the corporatists and

[219.84 - 224.4] capitalists will have us living in a

[221.94 - 227.4] dystopian hell before too long

[224.4 - 230.159] so that is also an intrinsic risk even

[227.4 - 231.72] if it is not an existential risk so we

[230.159 - 233.879] really need to be talking about what

[231.72 - 236.48] happens when we get these super powerful

[233.879 - 236.48] agis

[236.84 - 243.9] part two autonomous AI

[240.12 - 245.22] AGI is kind of a loaded term there's a

[243.9 - 248.28] lot of

[245.22 - 250.14] debate over what AGI even means so this

[248.28 - 252.599] is why I have started saying autonomous

[250.14 - 255.599] AI I don't care how smart it is

[252.599 - 257.639] the the point being is that AI becoming

[255.599 - 261.53999999999996] autonomous is really what we're talking

[257.639 - 264.24] about and as we talk more about what is

[261.54 - 267.72] the path to AGI autonomy is rapidly

[264.24 - 270.06] becoming part of the conversation now

[267.72 - 272.40000000000003] that being said there is a huge

[270.06 - 275.46] disconnect between what

[272.4 - 278.94] us proletariat are talking about and

[275.46 - 281.09999999999997] what is being released from the halls of

[278.94 - 283.08] power and what the academic

[281.1 - 285.78000000000003] establishment is talking about

[283.08 - 288.59999999999997] there is a lot of gatekeeping and a lot

[285.78 - 290.52] of this is cloistered and that I think

[288.6 - 292.5] is probably one of the biggest problems

[290.52 - 294.74] which is honestly why I'm making these

[292.5 - 294.74] videos

[294.96 - 300.18] autonomous AI is coming whether or not

[297.96 - 302.46] you like it whether or not you're

[300.18 - 304.74] emotionally ready for it autonomous AI

[302.46 - 306.419] is easier to make than you think people

[304.74 - 309.12] are starting to realize this especially

[306.419 - 310.79999999999995] with the release of chat GPT plugins and

[309.12 - 312.979] the ability for language models to use

[310.8 - 316.68] apis

[312.979 - 318.71999999999997] to be fair the Department of Defense and

[316.68 - 321.72] universities and tech companies are all

[318.72 - 323.66] working on autonomous AI systems they

[321.72 - 326.03900000000004] are kind of cloistering and and and

[323.66 - 329.82000000000005] closing their research it's not as open

[326.039 - 332.21999999999997] as it once was and in part I I totally

[329.82 - 334.38] understand that because I think that

[332.22 - 335.22] some of these folks realize how close we

[334.38 - 338.039] are

[335.22 - 339.84000000000003] and that scares them and so they're

[338.039 - 342.78] they're basically playing the CL playing

[339.84 - 345.539] the cards very close to their vest until

[342.78 - 348.23999999999995] they get a better read on the situation

[345.539 - 351.18] now taken one step further

[348.24 - 353.28000000000003] or or looking at at this problem no one

[351.18 - 355.62] has actually proposed a comprehensive

[353.28 - 357.55999999999995] framework uh there's been plenty of

[355.62 - 361.44] books on it there's been a few papers

[357.56 - 364.74] but there's not really anything that is

[361.44 - 366.71999999999997] a fully realized solution and I don't in

[364.74 - 369.72] my opinion nobody has even fully

[366.72 - 372.47900000000004] articulated the danger yet what's going

[369.72 - 374.16] on and I'm hoping that this video will

[372.479 - 375.96] advance that conversation just a little

[374.16 - 377.94] bit

[375.96 - 379.85999999999996] part of what happens in this comment

[377.94 - 381.419] comes up on my videos a lot is a lot of

[379.86 - 383.88] people are afraid to even talk about

[381.419 - 385.44] this out of fear of ridicule

[383.88 - 386.94] those of us that are paying attention

[385.44 - 389.28] those of us that are doing the work

[386.94 - 391.919] those of us that are working on it we

[389.28 - 393.84] all see what's possible but Society by

[391.919 - 396.29999999999995] and large is not ready for it they're

[393.84 - 398.15999999999997] not ready for the conversation and so

[396.3 - 400.56] there's a lot of ridicule whether it's

[398.16 - 402.84000000000003] in Private Industry whether it's in the

[400.56 - 404.24] halls of Academia government military

[402.84 - 408.17999999999995] and so on

[404.24 - 409.56] uh so this basically comes down to

[408.18 - 411.539] something that I call institutional

[409.56 - 412.8] codependence which is that the

[411.539 - 414.59999999999997] establishment believes that the

[412.8 - 417.319] establishment is always right and the

[414.6 - 420.66] establishment will use any tactic attack

[417.319 - 423.47900000000004] tactic or technique sorry uh to control

[420.66 - 427.8] the conversation such as shame bullying

[423.479 - 430.199] and otherwise silencing techniques uh to

[427.8 - 433.58] stymie the conversation again this is

[430.199 - 433.58] why I've created my YouTube channel

[433.88 - 439.099] instead who gets to dominate the

[436.68 - 442.319] conversation fiction and billionaires

[439.099 - 444.479] Skynet Elon Musk The Establishment has

[442.319 - 447.06] abdicated responsibility for this now

[444.479 - 451.199] that being said I am absolutely 100

[447.06 - 453.3] certain that the halls of power and and

[451.199 - 456.0] the establishment is doing the research

[453.3 - 458.88] behind closed doors

[456.0 - 461.099] but they have abdicated responsibility

[458.88 - 464.24] for controlling the narrative and

[461.099 - 464.24] guiding the conversation

[464.639 - 469.319] so let's talk about a couple aspects of

[467.039 - 471.599] the control problem so for those not in

[469.319 - 474.599] the know the control problem is the

[471.599 - 476.639] category of problems of basically once

[474.599 - 477.9] the AI becomes super intelligent how do

[476.639 - 480.12] you control it

[477.9 - 481.62] there's a few Concepts in here I'm not

[480.12 - 483.84000000000003] going to go into every single one of

[481.62 - 486.24] them but a few of them that you may or

[483.84 - 488.52] may not know about one is convergent

[486.24 - 491.16] instrumental values the idea is that any

[488.52 - 493.19899999999996] sufficiently intelligent agent will come

[491.16 - 496.199] up with certain instrumental values or

[493.199 - 497.28000000000003] goals in service to those other goals

[496.199 - 499.74] whatever the whether whatever whatever

[497.28 - 501.29999999999995] its primary purpose happens to be such

[499.74 - 503.639] as self-preservation resource

[501.3 - 505.979] acquisition and so on

[503.639 - 508.919] so basically in order to further any

[505.979 - 511.08] goal whatever it happens to be your AI

[508.919 - 512.8199999999999] might come to the conclusion that it

[511.08 - 515.039] needs to preserve itself in order to

[512.82 - 517.5] continue furthering its goal that's a

[515.039 - 519.959] pretty reasonable uh thought to have I'm

[517.5 - 521.88] not going to comment one way or another

[519.959 - 523.7399999999999] um on on whether or not I think that's

[521.88 - 525.48] going to happen ultimately I don't think

[523.74 - 527.279] it's relevant and you'll see why a

[525.48 - 529.32] little bit later second is the

[527.279 - 531.06] orthogonality thesis which basically it

[529.32 - 533.519] says very simply there is no correlation

[531.06 - 535.8599999999999] between an ai's level of intelligence

[533.519 - 538.08] and the values that it pursues

[535.86 - 540.66] the treacherous turn this is a

[538.08 - 545.279] hypothetic hypothetical situation in

[540.66 - 548.1] which a apparently benign AGI suddenly

[545.279 - 550.38] or apparently turns on its creators

[548.1 - 552.4200000000001] because it came to some conclusion that

[550.38 - 554.519] we don't understand

[552.42 - 557.459] the courage ability problem which

[554.519 - 560.339] basically says that uh your AI might

[557.459 - 562.4399999999999] might not remain open to correction or

[560.339 - 563.3000000000001] feedback or it might resist being shut

[562.44 - 566.0400000000001] down

[563.3 - 569.2199999999999] so basically you lose control of it just

[566.04 - 570.54] because uh it says I'm not I'm not down

[569.22 - 572.76] with that anymore

[570.54 - 574.86] and then finally the value loading

[572.76 - 577.4399999999999] problem which is how do you specify

[574.86 - 580.08] human wave values in such a way that the

[577.44 - 582.1800000000001] AI can understand and act on them and

[580.08 - 584.76] then the very next follow-up question is

[582.18 - 586.3199999999999] who gets to Define them anyways

[584.76 - 588.8389999999999] so again these are a few of the

[586.32 - 590.88] assertions and these and hypotheses this

[588.839 - 593.519] is not an exhaustive list but you kind

[590.88 - 595.38] of get an idea there are a lot of ideas

[593.519 - 596.76] out there and not a whole lot of

[595.38 - 599.9399999999999] solutions

[596.76 - 602.16] now speaking of there are some solutions

[599.94 - 603.6600000000001] out there and these are more like broad

[602.16 - 605.16] categories

[603.66 - 607.5] um rather than

[605.16 - 611.279] um rather than comprehensive Frameworks

[607.5 - 613.26] so one thing is kill switch Solutions

[611.279 - 615.42] um pretty self-explanatory this is a

[613.26 - 617.399] broad category of ideas

[615.42 - 621.36] um I just saw on the internet someone

[617.399 - 623.22] proposed that we we put uh bombs like

[621.36 - 625.74] remotely triggered bombs in every data

[623.22 - 628.32] center so that we can immediately shut

[625.74 - 631.279] down every data center if we need to

[628.32 - 634.08] okay sure that

[631.279 - 636.899] doesn't sound like a very reasonable uh

[634.08 - 639.779] direction to go for me but hey worst

[636.899 - 642.54] comes worst case scenario maybe we do

[639.779 - 644.16] uh courage ability which basically is

[642.54 - 647.3389999999999] just the idea of just make the machine

[644.16 - 649.14] responsive to feedback uh but what if

[647.339 - 651.4200000000001] the feedback mechanism one doesn't work

[649.14 - 654.18] or two the machine shuts it down or

[651.42 - 657.779] three the feedback mechanism

[654.18 - 659.459] um doesn't have the intended uh efficacy

[657.779 - 661.2] that we want it to have

[659.459 - 663.42] uh and then there's various kinds of

[661.2 - 665.6400000000001] reinforcement learning inverse

[663.42 - 668.8199999999999] reinforcement learning and passive and

[665.64 - 672.24] blah blah basically just include an

[668.82 - 674.5790000000001] algorithm so that the machine will

[672.24 - 676.92] automatically autonomously learn the

[674.579 - 678.7199999999999] values that we want one this is

[676.92 - 680.399] difficult to do in the first place and

[678.72 - 683.4200000000001] two what if the machine rewrites itself

[680.399 - 685.74] or accident or even accidentally

[683.42 - 687.4799999999999] nullifies uh those reinforcement

[685.74 - 689.82] learning signals

[687.48 - 692.04] finally values alignment what if you

[689.82 - 693.9590000000001] build it with the human friendly values

[692.04 - 694.68] in the first place

[693.959 - 696.7199999999999] um

[694.68 - 698.8199999999999] still you run into the same problem one

[696.72 - 700.8000000000001] how do you implement it two what if it

[698.82 - 703.1400000000001] changes its mind later and three what if

[700.8 - 705.3599999999999] the values that you gave it are poorly

[703.14 - 709.019] defined or intrinsically flawed or

[705.36 - 711.019] broken now that that being said there is

[709.019 - 713.22] a paper that literally just came out

[711.019 - 714.899] called the capacity for moral

[713.22 - 716.88] self-correction in large language models

[714.899 - 718.8] so I'm really glad to see that the

[716.88 - 721.2] establishment is

[718.8 - 723.18] at the very beginning of talking about

[721.2 - 725.5790000000001] this uh soberly

[723.18 - 726.959] so the link is here but you can just

[725.579 - 729.0] search that

[726.959 - 730.8599999999999] um I I believe this paper was published

[729.0 - 733.68] at least in part by the folks over at

[730.86 - 735.9590000000001] anthropic still these are not complete

[733.68 - 738.42] Solutions and we are literally months

[735.959 - 741.2399999999999] away from from fully autonomous AGI

[738.42 - 742.9799999999999] systems the conversation is not going

[741.24 - 745.44] fast enough

[742.98 - 747.24] so if you haven't heard found it or seen

[745.44 - 749.399] it just do a Google search for bad

[747.24 - 751.62] alignment take Bingo

[749.399 - 753.98] um these are these were circulated on

[751.62 - 756.899] Twitter probably more than a year ago

[753.98 - 758.7] these will address and and argue against

[756.899 - 760.32] many of the things that people say out

[758.7 - 762.3000000000001] there so I'm not gonna I'm not gonna

[760.32 - 764.519] rehash it or read all of them to you but

[762.3 - 766.019] I just wanted to show you that like some

[764.519 - 767.639] people are treating it like a joke it's

[766.019 - 769.26] not really a joke but this is a good way

[767.639 - 772.5] of just showing like yeah the thing that

[769.26 - 776.3389999999999] you thought has already been addressed

[772.5 - 778.2] part three the AGI landscape

[776.339 - 780.1800000000001] um the biggest takeaway here is that

[778.2 - 782.82] there's not going to be one AGI there's

[780.18 - 785.399] going to be a few features of how AGI is

[782.82 - 788.339] implemented so let's talk about that

[785.399 - 790.26] first and foremost intelligence is not

[788.339 - 792.24] binary it's not like you're going to

[790.26 - 794.18] flip the switch in one day it's AGI but

[792.24 - 797.76] the day before it wasn't

[794.18 - 800.0999999999999] basically intelligence uh and and the

[797.76 - 802.4399999999999] sophistication of AGI systems will

[800.1 - 804.98] evolve over time there are going to be

[802.44 - 808.2] various constraints such as time energy

[804.98 - 810.72] data and all of that basically means

[808.2 - 812.94] that the the level of power of your AGI

[810.72 - 815.76] system is going to be on a sliding scale

[812.94 - 818.22] so for instance even the most evil

[815.76 - 819.779] machines might not be that powerful and

[818.22 - 821.399] they're going to be constrained based on

[819.779 - 823.019] you know the processing power of the

[821.399 - 825.24] computers that they're running on the

[823.019 - 827.519] network speed that they have so on and

[825.24 - 829.019] so forth when you look at intelligence

[827.519 - 832.38] there are literally thousands of

[829.019 - 834.18] dimensions of intelligence that that the

[832.38 - 836.7] types of intelligence out there are huge

[834.18 - 839.279] and so AGI is not going to master all of

[836.7 - 841.32] them immediately it's going to take time

[839.279 - 843.48] and then as I just mentioned there are

[841.32 - 845.7] gonna there are going to be numerous

[843.48 - 847.32] limiting factors or constraints such as

[845.7 - 850.6800000000001] the underlying Hardware the training

[847.32 - 854.12] data and energy requirements of course

[850.68 - 856.56] that is going to change quickly as

[854.12 - 858.6] basically the underlying Hardware ramps

[856.56 - 861.4799999999999] up exponentially the amount of data that

[858.6 - 865.32] is available ramps up exponentially and

[861.48 - 867.4200000000001] then the underlying machine learning

[865.32 - 869.279] models the neural networks also get

[867.42 - 870.959] exponentially more sophisticated and

[869.279 - 873.24] larger

[870.959 - 876.0] now as I mentioned most importantly

[873.24 - 877.2] there won't just be one Skynet the

[876.0 - 878.88] reason that we think that there's going

[877.2 - 880.5790000000001] to be just one is because it is

[878.88 - 883.32] convenient from a narrative perspective

[880.579 - 886.56] in Terminator it's easy to just say

[883.32 - 888.48] there's one big bad there's one Skynet

[886.56 - 890.9399999999999] um but that's not how it's going to

[888.48 - 892.9200000000001] happen there's going to be hundreds

[890.94 - 894.1800000000001] thousands millions it's going to ramp up

[892.92 - 898.4399999999999] very quickly

[894.18 - 901.079] so what this results in is a sort of

[898.44 - 903.4200000000001] arms race amongst in between the agis

[901.079 - 904.8] themselves as well as the sponsors or

[903.42 - 907.4399999999999] the people trying to build and control

[904.8 - 910.199] them which results in a survival of the

[907.44 - 912.899] fittest situation or a race condition

[910.199 - 915.42] where basically the most aggressive and

[912.899 - 917.06] sophisticated and and Powerful agis are

[915.42 - 919.74] the ones who win

[917.06 - 921.959] which that could be bad because then

[919.74 - 925.019] you're basically selecting for the most

[921.959 - 927.779] aggressive and hostile agis

[925.019 - 931.199] the high velocity of AGI cyber warfare

[927.779 - 934.26] will probably require our AGI systems to

[931.199 - 936.66] be partially or fully autonomous

[934.26 - 939.66] so basically what that means is that in

[936.66 - 940.92] order to match the arms race in cyber

[939.66 - 943.26] warfare

[940.92 - 946.4399999999999] the agis that we built will probably

[943.26 - 947.9399999999999] need to be evolving which means that

[946.44 - 950.8800000000001] they'll spawn off copies of themselves

[947.94 - 954.1800000000001] they'll be polymorphic they will recode

[950.88 - 956.82] themselves so on and so forth and also

[954.18 - 959.0999999999999] when you look at AGI in the context of

[956.82 - 961.5790000000001] cyber warfare they will explicitly

[959.1 - 964.0790000000001] require adversarial objective functions

[961.579 - 965.9399999999999] this is what was explored in Skynet

[964.079 - 968.2199999999999] which basically the objective function

[965.94 - 971.1600000000001] of Skynet was probably like maximize

[968.22 - 973.6800000000001] military power or so on

[971.16 - 975.48] So In This Global AGR arms race there's

[973.68 - 977.16] going to be numerous copies they're all

[975.48 - 978.9590000000001] going to be changing which results in

[977.16 - 981.12] the Byzantine generals problem so the

[978.959 - 983.2199999999999] Byzantine generals problem is a cyber

[981.12 - 986.279] security thought experiment where

[983.22 - 988.1990000000001] wherein the idea is you have numerous

[986.279 - 989.639] generals and you don't know their

[988.199 - 991.8] allegiance you don't know their loyalty

[989.639 - 993.36] and you don't know their plans either so

[991.8 - 995.76] how do those how do those generals

[993.36 - 998.279] communicate with each other in such a

[995.76 - 1000.259] way that they can understand who's on

[998.279 - 1002.54] whose side and also come to consensus on

[1000.259 - 1005.36] what the plan is assuming that there are

[1002.54 - 1008.3] hostile or adversarial actors

[1005.36 - 1010.82] now thinking of this in terms of three

[1008.3 - 1012.079] to five entities is difficult enough but

[1010.82 - 1014.1800000000001] we're going to be talking about a

[1012.079 - 1017.3599999999999] situation where there are millions or

[1014.18 - 1019.7589999999999] billions of agis all of them with

[1017.36 - 1022.759] unknown objective functions

[1019.759 - 1024.679] as autonomous agents lastly they will

[1022.759 - 1027.079] form alliances with each other

[1024.679 - 1028.76] by some means or other they will

[1027.079 - 1031.28] communicate they will establish their

[1028.76 - 1033.439] intentions and allegiances

[1031.28 - 1035.36] um and they will spend more time talking

[1033.439 - 1038.0] with each other than they willed with us

[1035.36 - 1039.74] this was something that is um that

[1038.0 - 1042.079] people are starting to talk about some

[1039.74 - 1044.36] of the folks that I I'm working with on

[1042.079 - 1047.1789999999999] cognitive architecture we're realizing

[1044.36 - 1050.0] that the very instant that you create a

[1047.179 - 1052.76] cognitive architecture it can talk 24 7.

[1050.0 - 1054.62] we can't talk 24 7 so even just by

[1052.76 - 1056.36] virtue of experimenting with cognitive

[1054.62 - 1058.78] architectures it makes sense to have

[1056.36 - 1062.539] them talking with each other

[1058.78 - 1063.9189999999999] uh and having agis talk with each other

[1062.539 - 1065.539] and come to agreements and

[1063.919 - 1067.76] understandings

[1065.539 - 1070.16] um this is going to happen even with the

[1067.76 - 1073.4] most benign benevolent outcomes of AGI

[1070.16 - 1076.039] now what these what these autonomous AI

[1073.4 - 1078.679] systems agree and disagree on will

[1076.039 - 1081.02] likely determine the overall outcome of

[1078.679 - 1083.299] what happens with the singularity with

[1081.02 - 1086.66] AGI and with

[1083.299 - 1090.44] um the basically the fate of humanity

[1086.66 - 1091.88] part four AGI Unleashed now given

[1090.44 - 1093.5] everything that I've outlined the

[1091.88 - 1094.3400000000001] question remains how do you control the

[1093.5 - 1098.86] machine

[1094.34 - 1098.86] my answer is maybe you don't

[1099.2 - 1102.0800000000002] the reason that I believe this is

[1100.82 - 1104.8999999999999] because the genie is out of the bottle

[1102.08 - 1107.0] open source models are proliferating you

[1104.9 - 1109.3400000000001] can already run a 30 billion parameter

[1107.0 - 1111.44] model on a laptop with six gigabytes of

[1109.34 - 1113.12] memory that paper just came out what

[1111.44 - 1115.28] yesterday or today

[1113.12 - 1117.7399999999998] Global deployments of AI are rising

[1115.28 - 1119.4189999999999] Federal and Military investment globally

[1117.74 - 1121.64] is also Rising

[1119.419 - 1124.1000000000001] because of this centralized alignment

[1121.64 - 1126.7990000000002] research is completely irrelevant it

[1124.1 - 1129.1399999999999] doesn't matter how responsible the most

[1126.799 - 1130.52] responsible actors are there are hostile

[1129.14 - 1132.679] actors out there with malevolent

[1130.52 - 1134.78] intentions and they have lots of funding

[1132.679 - 1137.7800000000002] not only that the AI systems are

[1134.78 - 1140.48] becoming much more accessible

[1137.78 - 1143.1789999999999] because of that distributed cooperation

[1140.48 - 1145.34] is now required alignment is not just

[1143.179 - 1147.26] about creating an individual Ai and if

[1145.34 - 1149.9599999999998] you go look at the alignment the bad

[1147.26 - 1152.6] alignment take Bingo none of those talk

[1149.96 - 1154.76] about distribution collaboration or

[1152.6 - 1157.2199999999998] collective intelligence or Collective

[1154.76 - 1159.02] processing all of the all of the

[1157.22 - 1161.78] conversations today are still talking

[1159.02 - 1164.12] about individual agis as if they're

[1161.78 - 1166.8799999999999] going to exist in a vacuum so far as I

[1164.12 - 1170.1789999999999] know no one is talking about this in the

[1166.88 - 1172.5200000000002] context of Game Theory and competition

[1170.179 - 1174.2] so because of this we need an alignment

[1172.52 - 1175.58] scheme that can create open source

[1174.2 - 1178.7] collaboration amongst numerous

[1175.58 - 1181.039] autonomous AGI entities such a framework

[1178.7 - 1182.419] needs to be simple robust and easy to

[1181.039 - 1185.86] implement

[1182.419 - 1185.8600000000001] we'll get to that in just a minute

[1186.02 - 1189.74] so

[1187.82 - 1191.72] what I'm basically proposing is a

[1189.74 - 1193.28] collective control scheme which might

[1191.72 - 1195.6200000000001] sound impossible

[1193.28 - 1197.48] creating one benevolent stable super

[1195.62 - 1198.86] intelligence is hard enough and now I'm

[1197.48 - 1201.44] saying we need to create millions of

[1198.86 - 1203.6599999999999] them billions of them

[1201.44 - 1205.28] what I'm saying is not that we need to

[1203.66 - 1208.3400000000001] we might not have a choice in the matter

[1205.28 - 1210.2] this might be the only path forward

[1208.34 - 1213.26] now if you're familiar with the work of

[1210.2 - 1215.299] John Nash and Game Theory you might be

[1213.26 - 1217.4] able to think about this in terms of

[1215.299 - 1219.2] okay let's just imagine for a minute

[1217.4 - 1220.3400000000001] that there are millions of agis out

[1219.2 - 1222.28] there

[1220.34 - 1227.98] with many of them with unknown

[1222.28 - 1230.299] intentions given a game theory

[1227.98 - 1232.7] dilemmas like the prisoner's dilemma and

[1230.299 - 1235.28] so on if you think about this in that

[1232.7 - 1237.679] perspective it may be possible to devise

[1235.28 - 1240.26] rules or assumptions that enable the AI

[1237.679 - 1243.3200000000002] the agis to reach consensus on their

[1240.26 - 1245.24] behavior even with the presence of

[1243.32 - 1248.6] malicious and faulty actors

[1245.24 - 1251.24] so what kinds of rules or assumptions

[1248.6 - 1253.82] could we give our AGI systems that we're

[1251.24 - 1257.299] all going to be developing independently

[1253.82 - 1260.0] excuse me so that they arrive at this

[1257.299 - 1262.16] equilibrium this Nash equilibrium that

[1260.0 - 1264.02] we're looking for how do we ensure that

[1262.16 - 1266.3600000000001] this that the millions and billions of

[1264.02 - 1269.4189999999999] agis that are coming arrive at the

[1266.36 - 1270.6789999999999] consensus we want them to

[1269.419 - 1272.9] part five

[1270.679 - 1275.0590000000002] heuristic imperatives

[1272.9 - 1277.2800000000002] so now we're going to talk about the

[1275.059 - 1280.28] work that I have done on this problem

[1277.28 - 1282.08] and this is not just hypothetical these

[1280.28 - 1283.94] there are also experiments that I've

[1282.08 - 1286.28] done that are documented and I'll link

[1283.94 - 1288.02] to those as well

[1286.28 - 1290.4189999999999] so the heuristic imperatives that I have

[1288.02 - 1292.6399999999999] come up with are quite simply one reduce

[1290.419 - 1294.5590000000002] suffering in the universe two increase

[1292.64 - 1296.659] prosperity in the universe and three

[1294.559 - 1298.039] increase understanding in the universe

[1296.659 - 1301.5200000000002] and I've been I've been talking about

[1298.039 - 1303.5] these uh much more frequently lately

[1301.52 - 1304.7] so let's take a deeper dive into these

[1303.5 - 1306.98] imperatives

[1304.7 - 1308.96] so first what is a heuristic imperative

[1306.98 - 1311.299] it's a set of principles that can be

[1308.96 - 1313.039] embedded into autonomous AI that

[1311.299 - 1315.32] basically takes the place of intrinsic

[1313.039 - 1317.96] motivations now what I want to point out

[1315.32 - 1320.48] is that the gpt4 paper that Microsoft

[1317.96 - 1322.82] published did mention intrinsic

[1320.48 - 1324.14] motivation so again The Establishment is

[1322.82 - 1325.3999999999999] starting to come around and I'm sure

[1324.14 - 1327.74] they've had more conversations

[1325.4 - 1330.26] internally that they are not revealing

[1327.74 - 1332.659] yet but they are setting the stage to

[1330.26 - 1333.799] talk about what intrinsic motivations do

[1332.659 - 1335.5390000000002] we give them

[1333.799 - 1338.4189999999999] so in the case of the heuristic

[1335.539 - 1341.6] imperatives these are imperatives that

[1338.419 - 1343.2800000000002] are uh basically provided a moral and

[1341.6 - 1345.3799999999999] ethical framework as well as those

[1343.28 - 1347.1789999999999] intrinsic motivations because very early

[1345.38 - 1350.0590000000002] on in my research I realized that there

[1347.179 - 1351.8600000000001] is no difference between an intrinsic

[1350.059 - 1354.1399999999999] motivation and a moral and ethical

[1351.86 - 1356.6589999999999] framework you have to have some impetus

[1354.14 - 1360.5] some motivation behind and reasoning

[1356.659 - 1362.6000000000001] behind all behavior and all reasoning

[1360.5 - 1365.48] so why these three why suffering and

[1362.6 - 1368.36] prosperity and understanding first it's

[1365.48 - 1370.76] a holistic approach it uh it's a it's a

[1368.36 - 1373.6999999999998] flexible framework that provides a very

[1370.76 - 1376.34] Broad and yet simple to implement

[1373.7 - 1378.2] framework it also balances trade-offs

[1376.34 - 1380.72] remember these heuristic imperatives

[1378.2 - 1384.679] have to be implemented simultaneously

[1380.72 - 1386.539] and in lockstep so this forces the AI to

[1384.679 - 1388.039] reason through and balance trade-offs

[1386.539 - 1390.3799999999999] between

[1388.039 - 1392.72] um between these objectives

[1390.38 - 1395.419] they're also very adaptable and context

[1392.72 - 1397.4] sensitive and basically what I mean by

[1395.419 - 1401.24] that is that large language models today

[1397.4 - 1403.5800000000002] like gpt4 are very very aware of the

[1401.24 - 1405.32] fact that these that these general

[1403.58 - 1408.3799999999999] principles these heuristic imperatives

[1405.32 - 1411.1399999999999] are not the be-all end-all but they are

[1408.38 - 1412.5200000000002] guidelines they're they're uh they're

[1411.14 - 1414.679] shorthand

[1412.52 - 1416.84] um ways of basically implementing

[1414.679 - 1419.72] intuition in order to quickly make

[1416.84 - 1422.36] decisions uh that adhere to a general

[1419.72 - 1425.0] principle or a moral compass and then

[1422.36 - 1426.4599999999998] evaluate that uh based against the

[1425.0 - 1429.14] context that it's in

[1426.46 - 1431.179] there's two other things that emerged

[1429.14 - 1433.2800000000002] during my most recent experiments with

[1431.179 - 1435.14] the heuristic imperatives and that is

[1433.28 - 1438.32] that the heuristic imperatives promote

[1435.14 - 1441.8600000000001] individual autonomy uh basically chat

[1438.32 - 1443.96] gpt4 realized that in order to reduce

[1441.86 - 1446.8999999999999] suffering of people you need to protect

[1443.96 - 1448.88] individual autonomy ditto for Prosperity

[1446.9 - 1449.96] that if you control people they're not

[1448.88 - 1452.179] going to be happy and they're not going

[1449.96 - 1454.4] to be prosperous so that was an emergent

[1452.179 - 1456.919] quality of the heuristic imperatives

[1454.4 - 1461.48] that surprised me and made me realize

[1456.919 - 1464.9] that chat gpd4 is already capable of a

[1461.48 - 1466.64] very very highly nuanced reasoning the

[1464.9 - 1469.52] other emerging quality that I did

[1466.64 - 1472.7] anticipate was fostering Trust

[1469.52 - 1474.62] basically when you have an AI equipped

[1472.7 - 1477.6200000000001] with these heuristic imperatives it

[1474.62 - 1480.08] understands that um fermenting trust or

[1477.62 - 1482.4799999999998] fostering trust with people is actually

[1480.08 - 1484.82] critical as a subsidiary goal or an

[1482.48 - 1487.159] auxiliary goal of these because if if

[1484.82 - 1490.52] humans don't trust the AI the rest of

[1487.159 - 1493.46] its imperatives are made irrelevant

[1490.52 - 1495.44] finally there are a lot of what about

[1493.46 - 1497.0] isms yeah but what about there's a lot

[1495.44 - 1498.44] of protests which of course this is part

[1497.0 - 1500.84] of the conversation

[1498.44 - 1503.48] so the most con these are some of the

[1500.84 - 1505.1] most common protests that I get when I

[1503.48 - 1507.559] talk about the heuristic imperatives one

[1505.1 - 1509.4189999999999] is won't reduce suffering result in the

[1507.559 - 1511.78] extermination of all life the short

[1509.419 - 1514.64] answer is yes if you only have that one

[1511.78 - 1517.34] which is why I spent two years working

[1514.64 - 1519.74] on the other two heuristic imperatives

[1517.34 - 1522.1999999999998] to counterbalance them because I realize

[1519.74 - 1523.58] that any single objective function is

[1522.2 - 1526.94] always going to be intrinsically

[1523.58 - 1530.0] unstable you must have a system that

[1526.94 - 1532.52] balances multiple sometimes antagonistic

[1530.0 - 1535.4] functions against each other in order to

[1532.52 - 1537.799] stabilize and reach that equilibrium

[1535.4 - 1539.539] number two yeah but who gets to Define

[1537.799 - 1542.059] suffering prosperity and understanding

[1539.539 - 1543.74] the short answer is nobody that is the

[1542.059 - 1546.62] point of of implementing it as a

[1543.74 - 1550.039] heuristic the machine learns as it goes

[1546.62 - 1551.7199999999998] and anyways llms like gpt4 already have

[1550.039 - 1554.36] a far more nuanced understanding

[1551.72 - 1556.64] understanding of the concept of

[1554.36 - 1559.3999999999999] suffering prosperity and understanding

[1556.64 - 1561.8600000000001] um than any individual human does and

[1559.4 - 1563.72] also humans have never needed perfect

[1561.86 - 1565.9399999999998] definitions we learn as we go as well

[1563.72 - 1568.46] and we get by

[1565.94 - 1571.1000000000001] number three well what about uh cultural

[1568.46 - 1574.039] biases and individual differences as I

[1571.1 - 1575.7199999999998] just mentioned in the last slide gpd4

[1574.039 - 1578.12] already understands the importance of

[1575.72 - 1581.299] individual liberty and autonomy as well

[1578.12 - 1583.9399999999998] as how critical self-determination is to

[1581.299 - 1585.76] suffering or to reduce suffering and

[1583.94 - 1588.98] increase prosperity

[1585.76 - 1590.9] so because of that and also because it

[1588.98 - 1592.24] is aware of context the importance of

[1590.9 - 1594.44] context

[1592.24 - 1596.96] issue number three is actually less of

[1594.44 - 1599.8400000000001] an issue than you might think and

[1596.96 - 1601.94] finally number four uh and most

[1599.84 - 1603.9189999999999] importantly why would the machine hold

[1601.94 - 1606.44] to these imperatives in the first place

[1603.919 - 1607.5200000000002] and we will get into this in a lot more

[1606.44 - 1610.7] detail

[1607.52 - 1612.679] but the tldr is that with Game Theory

[1610.7 - 1614.72] and thinking of it in terms of the

[1612.679 - 1617.0590000000002] Byzantine generals problems

[1614.72 - 1618.38] all of the agis equipped with the

[1617.059 - 1620.539] heuristic imperatives would be

[1618.38 - 1622.0390000000002] incentivized to cooperate Not only would

[1620.539 - 1623.779] they be incentivized to cooperate with

[1622.039 - 1625.36] each other they'll be incentivized to

[1623.779 - 1629.559] cooperate with us

[1625.36 - 1632.8999999999999] and that results in a collective

[1629.559 - 1634.8799999999999] equilibrium in which the Hostile and

[1632.9 - 1638.179] malicious agis are going to be the

[1634.88 - 1640.4] pariahs so basically the benevolent

[1638.179 - 1644.14] machines are stronger together than the

[1640.4 - 1644.14] Hostile actors are individually

[1644.24 - 1648.02] okay great

[1645.799 - 1649.34] assuming that you're on board how do you

[1648.02 - 1651.2] implement this this sounds too

[1649.34 - 1653.0] complicated well fortunately it's

[1651.2 - 1656.0] actually not that complicated

[1653.0 - 1657.74] first is constitutional AI so I proposed

[1656.0 - 1660.08] a constitution in my book natural

[1657.74 - 1662.6] language cognitive architecture back in

[1660.08 - 1665.36] the summer of 2021 almost two years ago

[1662.6 - 1666.98] right after that anthropic AI came out

[1665.36 - 1668.62] and they did their own version of

[1666.98 - 1672.14] constitutional AI which was reduce

[1668.62 - 1674.2399999999998] harmfulness or achieve harmlessness

[1672.14 - 1676.039] I don't think anthropic's core objective

[1674.24 - 1678.679] function is good because the most

[1676.039 - 1681.919] harmless AGI is not going to be one that

[1678.679 - 1683.659] fights other malicious agis at least I

[1681.919 - 1685.94] don't think so

[1683.659 - 1688.159] um another way but still the premise of

[1685.94 - 1689.3600000000001] of implementing it in a Constitution

[1688.159 - 1691.64] which is just a natural language

[1689.36 - 1693.74] document saying how the AI should behave

[1691.64 - 1695.96] does seem to work

[1693.74 - 1697.76] reinforcement learning the heuristic

[1695.96 - 1700.7] imperatives can make a really good

[1697.76 - 1701.9] reinforcement learning signal similar to

[1700.7 - 1704.299] reinforcement learning with human

[1701.9 - 1706.48] feedback but instead use the heuristic

[1704.299 - 1708.62] imperatives as feedback so it'd be

[1706.48 - 1711.02] rlhi reinforcement learning with

[1708.62 - 1713.2399999999998] heuristic imperatives so it's just a

[1711.02 - 1714.799] different reward system this also tends

[1713.24 - 1718.279] to work pretty well I've tested it with

[1714.799 - 1720.62] fine tuning it works pretty well

[1718.279 - 1722.179] um number three planning cognitive

[1720.62 - 1723.9799999999998] control task management and

[1722.179 - 1725.72] prioritization these heuristic

[1723.98 - 1727.76] imperatives work really well with

[1725.72 - 1729.5] Frameworks such as atom which atom is a

[1727.76 - 1731.24] framework that I recently wrote about

[1729.5 - 1735.32] called autonomous task orchestration

[1731.24 - 1737.9] manager so basically as your AI system

[1735.32 - 1740.0] is coming up with and executing tasks

[1737.9 - 1742.22] you use the heuristic imperatives to

[1740.0 - 1744.559] plan the tasks to choose which tasks to

[1742.22 - 1746.779] do to prioritize them and also choose

[1744.559 - 1749.0] which tasks not to do

[1746.779 - 1751.039] and then finally for review assessment

[1749.0 - 1753.14] and self-evaluation online learning

[1751.039 - 1755.5] systems that use the heuristic

[1753.14 - 1759.0200000000002] imperatives are super easy to implement

[1755.5 - 1760.76] and and are very flexible and that can

[1759.02 - 1764.179] also allow you to label data for

[1760.76 - 1766.46] training and future decision making

[1764.179 - 1768.02] so if you're on board with all this and

[1766.46 - 1770.08] you want to read more

[1768.02 - 1772.1589999999999] um I've got it all for free on GitHub

[1770.08 - 1773.6] I've also got a few books that are on

[1772.159 - 1776.48] Barnes and Noble but most people just

[1773.6 - 1778.82] use the the free ones anyways so the

[1776.48 - 1781.82] most recent work is on my GitHub under

[1778.82 - 1784.279] Dave shop heuristic imperatives this is

[1781.82 - 1786.1399999999999] a white paper that was almost entirely

[1784.279 - 1788.12] written by gpt4 so you can see how

[1786.14 - 1789.8600000000001] nuanced gpt4's understanding of the

[1788.12 - 1791.6] problem is

[1789.86 - 1793.52] um about a year ago I published a book

[1791.6 - 1795.4399999999998] called benevolent by Design which is the

[1793.52 - 1798.26] first book that fully promotes uh

[1795.44 - 1800.299] proposes this framework and explores

[1798.26 - 1802.7] different ways to implement it and then

[1800.299 - 1804.5] finally also very recently I proposed

[1802.7 - 1806.48] the atom framework which includes the

[1804.5 - 1807.799] heuristic imperatives for task

[1806.48 - 1810.5] orchestration

[1807.799 - 1812.059] but also moreover I encourage you to

[1810.5 - 1814.34] just have a conversation with chatgpt

[1812.059 - 1816.02] about these uh plenty of people on

[1814.34 - 1818.12] Reddit and other and Discord and other

[1816.02 - 1819.58] places have tested the heuristic

[1818.12 - 1823.1] imperatives they've tried to break them

[1819.58 - 1825.74] and they and you know they use the one

[1823.1 - 1828.1999999999998] one interesting conversation was someone

[1825.74 - 1830.659] used chat GPT to try and come up with

[1828.2 - 1832.82] the the pitfalls of the heuristic

[1830.659 - 1834.5] imperatives and I said yeah like that

[1832.82 - 1836.48] just goes to show that it has a more

[1834.5 - 1838.22] nuanced understanding of the risks and

[1836.48 - 1839.779] the implementation than you do and

[1838.22 - 1840.98] they're like okay yeah I guess I see

[1839.779 - 1845.179] what you mean

[1840.98 - 1847.039] okay so part six conclusion

[1845.179 - 1849.98] as far as I can tell the problem is

[1847.039 - 1852.2] solved but there's still a lot of work

[1849.98 - 1854.72] to do

[1852.2 - 1857.299] so the problem comes down to twofold one

[1854.72 - 1858.8600000000001] is dissemination and experimentation the

[1857.299 - 1860.84] perfect solution doesn't matter if no

[1858.86 - 1862.1] one knows about it so we need to spread

[1860.84 - 1864.26] the word

[1862.1 - 1865.76] um this is why I created my YouTube

[1864.26 - 1868.52] channel

[1865.76 - 1870.14] um and even if my heuristic comparatives

[1868.52 - 1871.82] are not perfect it's the best we've got

[1870.14 - 1872.419] so far

[1871.82 - 1874.7] um

[1872.419 - 1876.26] yeah so I've been working pretty much a

[1874.7 - 1877.88] year straight to get my YouTube channel

[1876.26 - 1882.62] as big as possible

[1877.88 - 1884.24] to achieve to arrive at this moment

[1882.62 - 1885.9189999999999] another problem is that there's only so

[1884.24 - 1887.8990000000001] much experimentation I can do on my own

[1885.919 - 1889.7] now that being said lots of other people

[1887.899 - 1892.76] have started experimenting I'm working

[1889.7 - 1894.26] with various cognitive architects who

[1892.76 - 1897.2] have put the heuristic imperatives into

[1894.26 - 1900.44] their machines and again they have

[1897.2 - 1901.94] discovered that yes it is one very easy

[1900.44 - 1904.64] to implement the heuristic imperatives

[1901.94 - 1907.22] and two it does seem to drive curiosity

[1904.64 - 1908.8990000000001] and a few other uh beneficial behaviors

[1907.22 - 1910.76] for the machine it makes them very

[1908.899 - 1911.9599999999998] thoughtful

[1910.76 - 1913.46] um there's a few places that you can

[1911.96 - 1914.6000000000001] join the conversation

[1913.46 - 1916.88] um all the links are in the description

[1914.6 - 1918.62] so I just created a new subreddit called

[1916.88 - 1920.5390000000002] heuristic imperatives so that we can

[1918.62 - 1922.76] talk about these and share our work

[1920.539 - 1924.799] there's also a Discord Community

[1922.76 - 1927.02] um also Link in the description but I've

[1924.799 - 1929.12] been working on this since 2019 when

[1927.02 - 1931.58] gpt2 came out

[1929.12 - 1933.559] um and you know I will be the first to

[1931.58 - 1936.1399999999999] admit there's a lot of ways to skin this

[1933.559 - 1938.48] cat maybe my heuristic imperatives

[1936.14 - 1940.159] aren't even the best but at least now

[1938.48 - 1942.98] you're aware of the concept and you know

[1940.159 - 1944.3600000000001] how easy it is to implement so maybe the

[1942.98 - 1946.64] rest of us can collectively work

[1944.36 - 1949.8799999999999] together and implement this situation

[1946.64 - 1951.5] where even in an uncertain environment

[1949.88 - 1953.779] with potentially hostile actors the

[1951.5 - 1955.76] Byzantine generals environment we can

[1953.779 - 1958.039] have agis that will cooperate and

[1955.76 - 1961.76] collaborate and that will ultimately end

[1958.039 - 1964.039] up in a very safe and stable environment

[1961.76 - 1966.14] so all that being said thank you for

[1964.039 - 1968.48] watching please jump in the comments the

[1966.14 - 1970.8200000000002] conversation Discord and Reddit and do

[1968.48 - 1972.02] the experiments yourself I promise it's

[1970.82 - 1975.82] pretty easy

[1972.02 - 1975.82] all right that's it