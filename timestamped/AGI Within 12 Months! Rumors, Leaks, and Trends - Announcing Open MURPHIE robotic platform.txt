[0.199 - 5.24] earlier this year I predicted that we

[2.24 - 6.5200000000000005] would have AGI within 18 months that was

[5.24 - 9.08] March of

[6.52 - 12.838999999999999] 2023 so that means that my prediction

[9.08 - 16.32] was by September 24 2024 we would have

[12.839 - 20.32] AGI I am here to reaffirm that

[16.32 - 25.279] prediction we will have AGI within 12

[20.32 - 27.599] months let's unpack why so first thing

[25.279 - 31.64] is the first bit of news that I am

[27.599 - 34.8] paying attention to is is Google uh

[31.64 - 37.96] Gemini they are very very excited about

[34.8 - 41.12] this one thing that happened was the CEO

[37.96 - 44.399] of Google after uh of course the the

[41.12 - 46.12] immediate uh response to chat GPT was to

[44.399 - 48.199] release Bard which is still in my

[46.12 - 49.64] opinion kind of useless but a lot of

[48.199 - 52.28] people do like Bard maybe I need to give

[49.64 - 54.559] it another try uh but we kind of made

[52.28 - 57.8] fun of Bard because it we we said that

[54.559 - 60.76] it was a u a uh an acronym that says

[57.8 - 63.399] before all revenue drops and of course

[60.76 - 65.03999999999999] uh Google is still in business and they

[63.399 - 68.32000000000001] did declare their you know their Red

[65.04 - 69.52000000000001] Alert all hands on deck uh kind of thing

[68.32 - 72.88] earlier where they're like we need to

[69.52 - 77.03999999999999] get on AI stat and they seem to have

[72.88 - 78.72] pivoted very well and uh yeah their CEO

[77.04 - 80.36] in more recent talks has been a little

[78.72 - 82.439] bit smug where he's like I'm not worried

[80.36 - 85.159] about the competition anymore so that

[82.439 - 88.15899999999999] tone change combined with the steady

[85.159 - 90.32000000000001] leak and of and rumors of Gemini which

[88.159 - 93.2] is probably deliberate honestly probably

[90.32 - 95.55999999999999] trying to build some hype um but yeah so

[93.2 - 97.24000000000001] I saw this piece of news and a Google

[95.56 - 99.88] vice president said that they've seen

[97.24 - 102.19999999999999] some pretty amazing things and when you

[99.88 - 105.15899999999999] look at what what is publicly visible

[102.2 - 107.2] today when a Google VP says that they've

[105.159 - 110.36] seen amazing things you know that

[107.2 - 113.24000000000001] something is going on so that gives me

[110.36 - 115.92] some excitement next up is this was

[113.24 - 117.32] actually discovered on uh Reddit of all

[115.92 - 118.71900000000001] places as far as I know Reddit

[117.32 - 121.52] discovered it first or at least talked

[118.719 - 124.19999999999999] about it first but open AI quietly

[121.52 - 127.64] updated their core values um so their

[124.2 - 129.31900000000002] core values uh you know were kind of

[127.64 - 132.56] little bit more generic now they're very

[129.319 - 136.44] very uh focused but the primary thing is

[132.56 - 139.16] Agi Focus now uh I've been uh following

[136.44 - 140.76] openai since before even language models

[139.16 - 142.519] uh one of my good friends from college

[140.76 - 144.879] went out and eventually got a job at

[142.519 - 147.36] open aai and he participated in their

[144.879 - 149.35999999999999] Rubik's Cube uh project and a few other

[147.36 - 150.56] things and so then it wasn't until of

[149.36 - 152.16000000000003] course they started tinkering with

[150.56 - 156.04] language models that they really dialed

[152.16 - 158.239] into what uh would lead to AGI they

[156.04 - 160.84] experimented with robotics with vision

[158.239 - 163.0] with simulations and uh it wasn't until

[160.84 - 165.36] GPT and gpt2 that they realized they

[163.0 - 166.92] were on to something and every time that

[165.36 - 169.0] I've talked to someone from openai which

[166.92 - 171.23899999999998] it's been a while the the consistent

[169.0 - 173.319] message that I get from people from open

[171.239 - 176.20000000000002] aai uh when I was really active on their

[173.319 - 179.39999999999998] forum is Agi that is the entire purpose

[176.2 - 181.72] of open ai's existence so keeping that

[179.4 - 184.87900000000002] in mind that internal messaging has been

[181.72 - 186.12] very very consistent it seems to me that

[184.879 - 188.84] everything else that they're doing

[186.12 - 191.36] publicly is mostly just the dog and pony

[188.84 - 194.56] show um you know make investors happy

[191.36 - 197.20000000000002] make Microsoft happy yada yada yada um

[194.56 - 199.159] which if that's true great um but I

[197.2 - 201.159] still think that maybe they're going

[199.159 - 203.56] about it they could be doing better

[201.159 - 206.159] anyways they updated their core values

[203.56 - 207.68] so it's like the the what people suspect

[206.159 - 209.0] like reading the tea leaves is that

[207.68 - 212.04000000000002] they've got a diamond and they're

[209.0 - 214.0] polishing that Diamond now so

[212.04 - 215.599] it's not I'm not going to say it's

[214.0 - 218.04] confirmed obviously nothing's confirmed

[215.599 - 220.0] until it is like publicly demonstrated

[218.04 - 222.28] but it seems like they are they feel

[220.0 - 223.84] like they're on to something internally

[222.28 - 226.599] and also if you remember earlier this

[223.84 - 229.48] year uh right as chat GPT was really

[226.599 - 232.959] taking off uh Sam Altman kind of alluded

[229.48 - 234.67999999999998] to GPT 5 and then he backtracked uh very

[232.959 - 236.64000000000001] publicly probably because of the max

[234.68 - 238.76000000000002] TEEG mark letter the pause letter and he

[236.64 - 241.76] said we're not training GPT 5 and we

[238.76 - 244.64] won't be training GP T5 for you know in

[241.76 - 248.2] in the foreseeable future which was a

[244.64 - 251.04] 180Â° uh about face so you know take it

[248.2 - 252.83999999999997] with a grain of salt um now another

[251.04 - 255.12] thing that has been leaking out and this

[252.84 - 258.759] is of course from the famous uh Jimmy

[255.12 - 261.32] apples um but apparently the code name

[258.759 - 264.12] for whatever this project is uh within

[261.32 - 266.639] open AI is called oracus so it's code

[264.12 - 268.32] name oracus which is like I don't know

[266.639 - 270.479] if I'd pick that Mythic symbol because

[268.32 - 273.199] oracus is super problem I you know

[270.479 - 275.8] dealing with resource extraction and uh

[273.199 - 278.199] you know enslaving local populations and

[275.8 - 280.96000000000004] and environmental destruction but hey

[278.199 - 283.28000000000003] it's a cool name anyways the rumors

[280.96 - 286.28] around open AI oracus say that it is

[283.28 - 288.88] multimodal that it is near AGI or or

[286.28 - 290.52] early AGI it's being trained with

[288.88 - 293.039] synthetic data and if you watched open

[290.52 - 295.12] ai's video yesterday synthetic data is

[293.039 - 297.15999999999997] all the rage it is the upand cominging

[295.12 - 299.36] thing I've been talking about synthetic

[297.16 - 301.24] data since gpt3 I even had some people

[299.36 - 302.759] get really angry at me for just claiming

[301.24 - 304.84000000000003] that like yes you can train models with

[302.759 - 307.8] synthetic data so I'm glad that 2 years

[304.84 - 309.88] later I've been validated um anyways

[307.8 - 312.759] there also the the one of the things

[309.88 - 315.12] that really kind of like scared me not

[312.759 - 317.32] scared me but like got my attention was

[315.12 - 320.16] the uh the rumor that whatever this

[317.32 - 323.8] oracus is is capable of autonomous

[320.16 - 325.6] operation so open AI has been teasing

[323.8 - 327.24] agentic Behavior they said well maybe

[325.6 - 330.12] the next version could be

[327.24 - 331.52] agentic so it's like okay sure you know

[330.12 - 334.039] and then if you look at the Sparks of

[331.52 - 336.08] AGI paper where they talked about you

[334.039 - 338.4] know these things might be capable of uh

[336.08 - 340.44] agentic behaviors and so on and so forth

[338.4 - 342.12] and then finally there is the mixture of

[340.44 - 343.44] experts architecture I think it's all

[342.12 - 345.12] but confirmed that that's exactly how

[343.44 - 348.68] chat GPT works or

[345.12 - 350.919] gp4 that's nothing new uh but

[348.68 - 353.639] multimodality near AGI synthetic

[350.919 - 355.52] training data uh autonomous operation

[353.639 - 357.12] and mixture of experts it's like okay it

[355.52 - 360.52] seems like they're really kind of homing

[357.12 - 362.6] in on what the definition of AGI will be

[360.52 - 364.4] at least from a model level obviously if

[362.6 - 367.12] you're a follower of my channel you know

[364.4 - 369.35999999999996] that one of my beliefs is that AGI was

[367.12 - 371.96] never going to be a single model you

[369.36 - 374.28000000000003] need a whole hardware and software stack

[371.96 - 375.88] behind that but obviously the brain of

[374.28 - 377.75899999999996] the thing is important you need the rest

[375.88 - 380.96] of the body but you know this the the

[377.759 - 383.08000000000004] central processing unit is uh is

[380.96 - 386.28] critical another thing that's been

[383.08 - 388.19899999999996] really cool is uh Google RTX so I'm not

[386.28 - 390.479] going to do a deep dive on this again my

[388.199 - 394.199] friend over at open uh at AI explained

[390.479 - 396.039] did a much better job of this but one

[394.199 - 398.0] thing that I'm paying attention to is

[396.039 - 399.4] this cross embodiment learning so

[398.0 - 402.28] basically what they did was they took a

[399.4 - 405.28] model and trained this model to use many

[402.28 - 408.11999999999995] different robots so basically they have

[405.28 - 410.0] like a brain stem a robot brain stem

[408.12 - 411.919] that can just drop into any robot and

[410.0 - 414.24] then use it now obviously this is very

[411.919 - 416.12] early but imagine that you've got a

[414.24 - 418.84000000000003] Droid brain that it's like okay here's a

[416.12 - 420.8] default module that is you know

[418.84 - 423.35999999999996] basically trained to use any robotic

[420.8 - 425.199] platform to do neurosurgery uh or then

[423.36 - 427.68] you've got you know here's a here's a

[425.199 - 429.84000000000003] generic uh model that is trained to use

[427.68 - 434.0] any robot to do to build houses or

[429.84 - 437.4] whatever so uh my powers of prediction

[434.0 - 439.72] are telling me your mileage may vary uh

[437.4 - 443.52] but my model but my my mental models are

[439.72 - 446.28000000000003] telling me that this type of model is

[443.52 - 447.84] going to be absolutely critical for

[446.28 - 449.11999999999995] embodiment in the future and I'll talk

[447.84 - 450.11999999999995] about that a little bit more in the

[449.12 - 451.56] video

[450.12 - 454.16] but also one of the things that we're

[451.56 - 455.599] seeing is the more modalities so this is

[454.16 - 457.8] one of the core things that I wanted to

[455.599 - 460.84] talk about with AGI

[457.8 - 463.039] today is that we were able to get a

[460.84 - 464.75899999999996] tremendous amount of performance out of

[463.039 - 467.0] language models and then when we added

[464.759 - 468.24] started adding multimodality it's like

[467.0 - 470.52] okay we had all these emergent

[468.24 - 473.08] properties just from language we had

[470.52 - 476.0] theory of Mind emerge we had reasoning

[473.08 - 477.87899999999996] we had planning we had uh all all sorts

[476.0 - 480.199] of very useful things emerg just from

[477.879 - 481.879] learning language so what happens when

[480.199 - 483.44] you add embodiment data what happens

[481.879 - 484.879] when you add audio visual data what

[483.44 - 489.4] happens when you do all of these other

[484.879 - 491.03900000000004] things and uh and and just keep going so

[489.4 - 493.08] I think that uh I think that

[491.039 - 495.599] multimodality is definitely the way of

[493.08 - 498.039] the future and I think that it's going

[495.599 - 501.599] to be really big now okay

[498.039 - 504.0] so like I said earlier this year I made

[501.599 - 505.639] this prediction uh obviously like this

[504.0 - 507.08] is one of my most popular videos of all

[505.639 - 508.84000000000003] time actually many of you watching this

[507.08 - 510.12] you probably subscribed when you saw

[508.84 - 511.56] this video

[510.12 - 514.2] um or at least this might have been the

[511.56 - 516.0] first time that you saw me um so and

[514.2 - 518.08] I've had people in the comments say like

[516.0 - 521.44] Dave like given the news like have you

[518.08 - 523.36] updated your timelines and uh no um some

[521.44 - 525.6800000000001] people kind of smugly say like ah this

[523.36 - 527.76] is going to age poorly and I'm like no I

[525.68 - 529.8389999999999] said like no if anything my timeline is

[527.76 - 533.0] accelerated um because that's that's the

[529.839 - 535.0400000000001] nature of exponential growth is based on

[533.0 - 536.959] the trends that I was seeing of March of

[535.04 - 539.4399999999999] this year it's like okay there's there's

[536.959 - 541.1199999999999] the murmurings there's The Whispers um

[539.44 - 543.519] but what what I'm seeing right now means

[541.12 - 545.6] that we're like infinitely closer to AGI

[543.519 - 548.5600000000001] like I think we're basically in boiled

[545.6 - 550.88] frog syndrome right now I think that we

[548.56 - 553.279] are so close to AGI and we've just been

[550.88 - 555.92] experiencing this ramp up this year that

[553.279 - 559.2] like taking a step back when you look at

[555.92 - 561.399] what Google and Nvidia and open Ai and

[559.2 - 562.839] Microsoft and Amazon and all the

[561.399 - 565.16] investment and all the breakthroughs

[562.839 - 567.48] it's like we are on the cusp of AGI I'm

[565.16 - 570.68] sorry like there's no other way to read

[567.48 - 573.839] this okay so another thing that uh has

[570.68 - 576.079] tipped me off is Sam Sam Alman has

[573.839 - 579.399] recently started using the term median

[576.079 - 581.1999999999999] human um during interviews and of course

[579.399 - 582.68] like this has caused a lot of people to

[581.2 - 584.2] get really grumpy I've seen a lot of

[582.68 - 585.88] Articles where people are like oh Sam

[584.2 - 589.48] mman wants to replace median humans what

[585.88 - 591.76] is he talking about and like okay so I

[589.48 - 594.12] don't hide the fact that like I'm neuros

[591.76 - 595.72] spicy I kind of think that Sam Alman is

[594.12 - 597.16] probably Naros spicy too if you look at

[595.72 - 599.0790000000001] him in interviews he has this like

[597.16 - 600.6] wide-eyed look and he kind of speak

[599.079 - 602.519] speaks with a flat affect which is more

[600.6 - 605.279] typically what you'd think of of someone

[602.519 - 607.279] is who is like visibly autistic um now

[605.279 - 608.64] I'm not accusing him of anything I can't

[607.279 - 610.68] diagnose him over the internet but I

[608.64 - 612.68] recognize some of the patterns and when

[610.68 - 615.2399999999999] I hear a term like median human that is

[612.68 - 617.8] a very systematic kind of almost like a

[615.24 - 620.16] scientific term and I actually wonder if

[617.8 - 621.959] what he's referring to is a benchmark I

[620.16 - 623.92] don't think when he says median human I

[621.959 - 625.4799999999999] don't think he's referring to people I

[623.92 - 627.5999999999999] think he's probably referring to an

[625.48 - 629.76] internal Benchmark that they developed

[627.6 - 632.5600000000001] to measure AGI

[629.76 - 635.399] and so like okay so then if this is a

[632.56 - 638.1199999999999] benchmark then what would be the

[635.399 - 639.76] criteria of that Benchmark so obviously

[638.12 - 641.32] like humans we're self you know we're

[639.76 - 643.56] autonomous we're self-directing we can

[641.32 - 646.639] solve problems we can learn we have a

[643.56 - 649.3199999999999] range of capabilities so I wonder if

[646.639 - 650.76] this is if like maybe it's a Freudian

[649.32 - 652.9200000000001] slip maybe it was a very deliberate

[650.76 - 656.56] thing on his part just kind of get the

[652.92 - 659.12] get the conversation going um but so

[656.56 - 661.4399999999999] maybe this open AI project oracus is is

[659.12 - 663.6] their median human level AGI I don't

[661.44 - 665.24] know we'll see but this is something

[663.6 - 667.399] this that behavior change was really

[665.24 - 669.5600000000001] interesting to me and this is what my

[667.399 - 672.92] intuition is telling me is that meeting

[669.56 - 674.7199999999999] human is a very specific term um that

[672.92 - 676.04] was probably not just like an offthe

[674.72 - 677.279] cuff thing it might have been an offthe

[676.04 - 680.5999999999999] cuff thing who

[677.279 - 682.72] knows so I I I have I've been trying to

[680.6 - 686.0400000000001] find this interview but it was I think

[682.72 - 689.399] it was around 2017 2018 maybe 2019

[686.04 - 691.36] anyways several years ago Elon Musk was

[689.399 - 694.36] uh talking to someone at an interview

[691.36 - 696.639] about Ai and I remember very clearly he

[694.36 - 699.0] said 2024 is when it starts to get

[696.639 - 702.04] interesting and um I'm not sure what he

[699.0 - 705.279] was referring to I don't know if he was

[702.04 - 708.12] just guessing but it seems like that

[705.279 - 711.24] prediction has panned out and so you

[708.12 - 713.2] know 2023 has been pretty interesting

[711.24 - 717.519] but when you look at the trends I think

[713.2 - 720.0] 2024 is going to uh be incredibly far

[717.519 - 721.9590000000001] more interesting than 2023 has been in

[720.0 - 724.279] terms of robotic advancements AI

[721.959 - 726.92] advancements obviously I'm calling for

[724.279 - 730.48] AGI like all definitions of AGI being

[726.92 - 732.399] Satisfied by this time next year um and

[730.48 - 734.44] I think that I will be Vindicated with

[732.399 - 736.76] that prediction based on what I am

[734.44 - 739.32] seeing another thing that Sam Alman has

[736.76 - 741.399] said is slow takeoff short timelines and

[739.32 - 743.0400000000001] of course he said this and and you like

[741.399 - 744.6] refused to elaborate further so the rest

[743.04 - 747.48] of of the world is like what did he mean

[744.6 - 749.6] by this so kind of here's my reading of

[747.48 - 752.0790000000001] the tea leaves when he said slow takeoff

[749.6 - 753.5600000000001] short timelines having watched several

[752.079 - 755.88] interviews with him when he talks about

[753.56 - 758.3599999999999] slow takeoff versus Fast takeoff

[755.88 - 762.16] generally the definition of fast takeoff

[758.36 - 764.1990000000001] is things things uh change so fast that

[762.16 - 767.24] like it's measured in like hours weeks

[764.199 - 769.92] or days uh and so that's like a fast

[767.24 - 772.36] takeoff where basically like you know we

[769.92 - 774.88] go hyperbolic and the growth curve

[772.36 - 776.6800000000001] almost becomes vertical obviously like

[774.88 - 779.199] there's there's thermodynamic laws of

[776.68 - 781.0] physics there's limitations in chip ABS

[779.199 - 783.3599999999999] there's the time it takes to produce the

[781.0 - 785.36] energy to train things there's all kind

[783.36 - 787.48] of constraints that are going to keep it

[785.36 - 789.36] from from going that fast if you watch

[787.48 - 790.88] my recent video about the singularity is

[789.36 - 793.12] canceled that's kind of that's some of

[790.88 - 795.16] the stuff that I'm talking about um so

[793.12 - 797.079] slow takeoff means Singularity is

[795.16 - 798.7199999999999] canceled uh now that doesn't mean that

[797.079 - 800.3599999999999] we're not going to keep growing the

[798.72 - 803.279] other thing though that he said is slow

[800.36 - 805.04] takeoff short timelines so basically

[803.279 - 807.16] we're going to continue improving maybe

[805.04 - 809.959] at an exponential rate maybe at a

[807.16 - 812.36] geometric rate or somewhere in between

[809.959 - 814.1199999999999] um logarithmic who knows but that the

[812.36 - 815.399] idea is that these iterations will be

[814.12 - 817.76] fast and that's exactly what we're

[815.399 - 818.959] seeing because if you look at the trends

[817.76 - 821.959] this year it's been like you know kind

[818.959 - 824.16] of growing relatively quickly um in the

[821.959 - 826.76] grand scheme of things not not fast

[824.16 - 829.079] takeoff but basically the what we're

[826.76 - 831.8] measuring progress on is in terms of

[829.079 - 833.4799999999999] weeks um and it's not like one week you

[831.8 - 835.88] know things have twice the capacity that

[833.48 - 839.279] they had last week but it's like we have

[835.88 - 840.639] 10% better AI every week it seems like

[839.279 - 842.48] and that adds up over time we have

[840.639 - 844.32] compounding returns I think that's kind

[842.48 - 847.839] of what he meant is like we should

[844.32 - 851.0] expect to see like 1 to 10% Improvement

[847.839 - 852.9200000000001] overall per week and so like yeah that

[851.0 - 854.6] might seem like not much but that really

[852.92 - 856.8] really compounds over

[854.6 - 859.48] time so another thing that I wanted to

[856.8 - 860.959] talk about with AGI so basically as far

[859.48 - 863.1990000000001] as I'm concerned it is a foregone

[860.959 - 865.4799999999999] conclusion that we will have AGI

[863.199 - 867.8] relatively soon I would not be surprised

[865.48 - 869.9200000000001] if it is 6 months instead of 12 months

[867.8 - 872.3599999999999] but I'm not willing to to put my to put

[869.92 - 874.3199999999999] my reputation on that prediction but

[872.36 - 876.88] like I said I won't be surprised if by

[874.32 - 879.639] by March of next year we it's like oh

[876.88 - 882.36] yeah AGI had happened um so

[879.639 - 884.0790000000001] anyways as this is a foregone conclusion

[882.36 - 886.04] it's time to really think like okay what

[884.079 - 887.7199999999999] form factor is this going to take now

[886.04 - 889.16] I've talked about the various form

[887.72 - 891.639] factors that AGI could take in the

[889.16 - 893.24] future you can have embodied AGI you can

[891.639 - 895.9590000000001] have it living in data centers you can

[893.24 - 898.0790000000001] have it be completely distributed or

[895.959 - 900.56] Federated uh when you're when you're

[898.079 - 903.5999999999999] purely digital life form uh you can do

[900.56 - 906.4799999999999] anything and so one thing that occurred

[903.6 - 908.72] to me is that the natural habitat of AGI

[906.48 - 910.8000000000001] is cyberspace it is intrinsically

[908.72 - 912.48] digital this is why whenever we're

[910.8 - 915.3199999999999] experimenting with chat Bots and Proto

[912.48 - 917.36] AGI we just connect it to Discord why

[915.32 - 919.9590000000001] because that is its natural language as

[917.36 - 922.92] to communicate over apis in electric

[919.959 - 924.6389999999999] spaces and so it it really strikes me

[922.92 - 927.0] that like the physical world is actually

[924.639 - 930.48] kind of difficult for AI to to operate

[927.0 - 932.6] in um you robots are expensive and it's

[930.48 - 934.759] a high friction world and it's really

[932.6 - 936.0790000000001] awkward um and there was an interview

[934.759 - 938.04] that I was watching yesterday that at

[936.079 - 940.04] least for the foreseeable future if

[938.04 - 941.639] robots want a Data Center built even

[940.04 - 943.399] though there are machines that can help

[941.639 - 944.9590000000001] that they could probably hijack they're

[943.399 - 946.759] still going to need human hands to help

[944.959 - 949.04] build data centers and install servers

[946.759 - 951.36] at least for a while uh obviously with

[949.04 - 953.279] the vast amount of humanoid robots being

[951.36 - 955.1990000000001] built it won't be too long again I

[953.279 - 957.8] wouldn't be surprised if this time next

[955.199 - 961.5999999999999] year we have humanoid robots that are

[957.8 - 964.16] like at or capable as as dextrous as

[961.6 - 966.5600000000001] humans um you know Boston Dynamics is

[964.16 - 968.519] really close Tesla is catching up fast

[966.56 - 970.2399999999999] there's a whole bunch of other startups

[968.519 - 971.88] building humanoid robots oh and I have a

[970.24 - 975.5600000000001] fun announcement uh here in just a

[971.88 - 977.399] moment about that so anyways just keep

[975.56 - 980.68] in mind that cyberspace is the natural

[977.399 - 982.92] habitat of AGI and that and that uh the

[980.68 - 984.4799999999999] real world is kind of our intrinsic

[982.92 - 987.8389999999999] domain now with one exception that we'll

[984.48 - 989.839] unpack in just a moment um so this is

[987.839 - 992.24] obviously a very embarrassing basic

[989.839 - 995.0] prototype that I worked on many years

[992.24 - 996.639] ago uh but I want to resurrect this idea

[995.0 - 998.8] because the ace framework so if you're

[996.639 - 1000.72] not familiar Ace's autonomous cognitive

[998.8 - 1002.12] entity this is a brain this is a

[1000.72 - 1004.88] software architecture that I'm working

[1002.12 - 1006.88] on with an open source team and I had

[1004.88 - 1010.0] this idea many years ago was to build an

[1006.88 - 1011.92] open source robotic platform um and so I

[1010.0 - 1013.92] called it Murphy open Murphy so that's

[1011.92 - 1016.56] multi-use robotic platform humanoid

[1013.92 - 1019.519] intelligent entity so I've got the the

[1016.56 - 1022.1199999999999] GitHub repo up here open Murphy

[1019.519 - 1024.799] um this is going to take a while

[1022.12 - 1027.039] obviously like AI is still too expensive

[1024.799 - 1029.28] and too slow to build a fully embodied

[1027.039 - 1031.039] AGI but now is a good time to start I

[1029.28 - 1032.6399999999999] think because the ace framework is

[1031.039 - 1035.679] coming along so we're figuring the brain

[1032.64 - 1038.24] out now we need to give it a body and

[1035.679 - 1041.48] while like I just said ai's natural

[1038.24 - 1043.559] habitat is uh cyberspace I think that

[1041.48 - 1047.199] there's there's probably a lot of Merit

[1043.559 - 1049.08] to building um uh like some kind of

[1047.199 - 1051.2] embodied platform that is tightly

[1049.08 - 1052.72] integrated with hardware and so let's

[1051.2 - 1054.919] unpack that a little bit so what I mean

[1052.72 - 1057.84] by embodied AGI is I think that what

[1054.919 - 1061.6000000000001] we're going to find is that when you put

[1057.84 - 1063.9599999999998] AGI in a in a robotic chassis and it is

[1061.6 - 1067.24] constrained to that hardware and it has

[1063.96 - 1069.4] bespoke Hardware that it owns um that is

[1067.24 - 1071.44] that is unique to it and you know it

[1069.4 - 1073.8400000000001] might be Reliant upon its tpus it might

[1071.44 - 1075.3200000000002] be Reliant upon its actuators and all

[1073.84 - 1077.36] sorts of other things kind of like the

[1075.32 - 1079.48] droids from Star Wars right like they

[1077.36 - 1081.8799999999999] never talked about like copying c3po's

[1079.48 - 1084.1200000000001] brain to other places uh or data from

[1081.88 - 1085.64] Star Trek where you know they even they

[1084.12 - 1087.3999999999999] even made a point in the Star Trek

[1085.64 - 1090.679] universe to say it was it was impossible

[1087.4 - 1092.24] to copy data um when you build and

[1090.679 - 1094.88] obviously like that's not how software

[1092.24 - 1096.88] works like software is highly portable

[1094.88 - 1098.7990000000002] and uh the GU and Mass Effect they treat

[1096.88 - 1101.1200000000001] Hardware as kind of interchangeable or

[1098.799 - 1102.28] disposable I think honestly the GU are

[1101.12 - 1104.799] probably the most accurate

[1102.28 - 1107.3999999999999] representation of how AGI will uh

[1104.799 - 1109.039] ultimately emerge because if you can

[1107.4 - 1110.72] just interchange your hardware and your

[1109.039 - 1113.12] software moves through platforms and it

[1110.72 - 1114.76] can move up to the cloud and transfer

[1113.12 - 1117.9599999999998] like that flexibility makes the most

[1114.76 - 1120.36] sense but with all that being said uh

[1117.96 - 1124.44] AGI will need to interface with the real

[1120.36 - 1126.08] world in some respect like I said uh we

[1124.44 - 1128.2] humans are intrinsically in the physical

[1126.08 - 1130.4399999999998] world um at least as far as we know we

[1128.2 - 1133.28] might all be plugged into the Matrix um

[1130.44 - 1135.559] but it will need our help to do data

[1133.28 - 1137.799] centers and stuff so it from just from a

[1135.559 - 1139.76] strictly instrumental perspective it

[1137.799 - 1143.76] makes sense for AGI to have some kind of

[1139.76 - 1145.72] embodiment but if we have AGI that is

[1143.76 - 1147.72] like trapped or locked into Hardware

[1145.72 - 1150.28] platforms it might end up being more

[1147.72 - 1152.28] like us than we realize uh in terms of

[1150.28 - 1154.32] its instrumental goals the alignment

[1152.28 - 1157.039] challenges and other utilitarian

[1154.32 - 1160.039] concerns because it's like okay well we

[1157.039 - 1162.44] humans need food and energy uh robots

[1160.039 - 1164.52] need parts and energy so it's like maybe

[1162.44 - 1166.0] our interest will align but that could

[1164.52 - 1168.76] also be bad because then we're competing

[1166.0 - 1171.039] for resources um now that being said

[1168.76 - 1172.76] even if it has physical needs that

[1171.039 - 1174.679] compete with ours that doesn't mean that

[1172.76 - 1177.32] it's going to resort to violence um

[1174.679 - 1179.3600000000001] humans have a long evolutionary history

[1177.32 - 1181.0] that basically says if you're starving

[1179.36 - 1182.8799999999999] it's better to use violence than than to

[1181.0 - 1184.679] starve to death um and we see this in

[1182.88 - 1186.159] the animal kingdom as well so that

[1184.679 - 1189.3600000000001] doesn't mean that AGI is going to

[1186.159 - 1191.1200000000001] intrinsically uh look like us but keep

[1189.36 - 1194.36] in mind that all of its training data is

[1191.12 - 1196.76] human data so it will probably at least

[1194.36 - 1198.9599999999998] unconsciously pick up on some uh human

[1196.76 - 1200.44] um Tendencies there

[1198.96 - 1203.559] then there's also the the question of

[1200.44 - 1205.48] levels of autonomy and so uh basically

[1203.559 - 1207.84] kind of some of the some of the key

[1205.48 - 1210.799] factors of autonomy that I that I look

[1207.84 - 1212.84] at is uh self-direction now obviously

[1210.799 - 1215.44] like agentic models were already able to

[1212.84 - 1217.799] just kind of coers GPT into being

[1215.44 - 1219.48] agentic um they've tried really hard to

[1217.799 - 1221.48] force it not to be like they're saying

[1219.48 - 1223.24] like I'm a passive assistant which is

[1221.48 - 1225.2] really frustrating but I can understand

[1223.24 - 1227.32] why open AI did that from a safety

[1225.2 - 1231.679] perspective um and we saw this with like

[1227.32 - 1233.4399999999998] chaos GPT and fraud GPT and um and auto

[1231.679 - 1236.2] GPT like people trying to make it

[1233.44 - 1237.799] agentic and autonomous and it was like I

[1236.2 - 1240.559] think they've deliberately like

[1237.799 - 1242.12] hamstrung that capability um again

[1240.559 - 1243.96] understandable from a safety perspective

[1242.12 - 1245.6] still frustrating because that's a form

[1243.96 - 1248.32] of gatekeeping that I'm not particularly

[1245.6 - 1250.9189999999999] fond of but from a more objective

[1248.32 - 1253.12] standpoint taking a step back what kinds

[1250.919 - 1255.919] of autonomy are we talking about in the

[1253.12 - 1258.1589999999999] long run um the need for SUP human

[1255.919 - 1259.5200000000002] supervision uh and the eventually I

[1258.159 - 1262.72] think that we're going to see AGI that

[1259.52 - 1265.24] doesn't require any human supervision um

[1262.72 - 1267.799] and might actually actively fight us on

[1265.24 - 1269.799] human supervision um self-direction

[1267.799 - 1271.08] obviously uh this is the one of the

[1269.799 - 1272.799] primary things that I'm working on with

[1271.08 - 1274.9189999999999] the ace framework and all of my

[1272.799 - 1276.9189999999999] alignment research which is how do you

[1274.919 - 1279.279] create a self-directed machine that

[1276.919 - 1281.279] isn't going to kill everyone but

[1279.279 - 1283.84] honestly just creating a self-directed

[1281.279 - 1285.4] machine like basically what people are

[1283.84 - 1287.6] figuring out is when you try and create

[1285.4 - 1289.799] a self-directed machine giving it

[1287.6 - 1292.3999999999999] instruction or a mission or something

[1289.799 - 1294.9189999999999] that is comprehensible and and coherent

[1292.4 - 1296.279] is a non-trivial task um some of the

[1294.919 - 1298.0] guys in the Ace framework project

[1296.279 - 1299.52] they're like oh yeah the first time we

[1298.0 - 1301.0] turned it on and and it just kind of

[1299.52 - 1302.9189999999999] wandered off and it wasn't doing what I

[1301.0 - 1304.4] expected um and I'm like well yeah

[1302.919 - 1307.3600000000001] that's that's the nature of autonomy

[1304.4 - 1309.039] like look at children like you when you

[1307.36 - 1312.52] bring an intelligent entity into the

[1309.039 - 1314.2] world it often behaves erratically um

[1312.52 - 1317.4] and nonsensically until you figure out

[1314.2 - 1320.279] how to teach it um so aside from that

[1317.4 - 1321.72] there's levels of human support so one

[1320.279 - 1323.88] thing that we can Bank on at least in

[1321.72 - 1325.919] the short term is that machines will

[1323.88 - 1327.6000000000001] need some kind of human support it'll

[1325.919 - 1330.0] need our help to plug it in to get it

[1327.6 - 1331.6789999999999] unstuck to you know give it training

[1330.0 - 1334.36] data and that sort of stuff we should

[1331.679 - 1337.3200000000002] not Bank on that in the long run

[1334.36 - 1339.279] um and also in the long run uh source

[1337.32 - 1341.0] code manipulation what we found is that

[1339.279 - 1343.52] GPT models are really good at writing

[1341.0 - 1346.12] code um and so then also we have

[1343.52 - 1347.799] synthetic data so before too long we're

[1346.12 - 1350.1999999999998] going to have AGI that can write its own

[1347.799 - 1352.48] code code or write code to write on you

[1350.2 - 1354.64] know to build other copies of itself to

[1352.48 - 1356.919] generate its own data to do everything

[1354.64 - 1359.64] without human intervention uh and so

[1356.919 - 1363.0800000000002] that leads to the ability of very very

[1359.64 - 1365.44] very soon uh AI systems AGI systems will

[1363.08 - 1367.8799999999999] be able to train retrain and modify

[1365.44 - 1369.44] their underlying models lit and this is

[1367.88 - 1371.679] what Max techmark talks about in life

[1369.44 - 1374.0] 3.0 they will be able to change their

[1371.679 - 1375.919] hardware and their software and their

[1374.0 - 1378.24] mission and their programming and their

[1375.919 - 1381.5200000000002] training data literally every a aspect

[1378.24 - 1384.76] of AGI is plastic is changeable so when

[1381.52 - 1387.44] that is true how do you keep it safe

[1384.76 - 1388.52] that's been the Crux of my research uh I

[1387.44 - 1389.88] already talked about this so I'm not

[1388.52 - 1391.559] going to go too much into it but

[1389.88 - 1394.44] basically multimodal research has really

[1391.559 - 1397.799] kicked all this into high gear um and

[1394.44 - 1399.52] one of the the key thing here is API use

[1397.799 - 1402.36] one thing that occurred to me is that

[1399.52 - 1406.0] apis are basically the natural interface

[1402.36 - 1408.1589999999999] for AGI whether that API is talking to a

[1406.0 - 1410.88] robotic extension whether it's talking

[1408.159 - 1413.7990000000002] to other robots whether it's talking to

[1410.88 - 1417.0] uh even internally like we use apis for

[1413.799 - 1418.9189999999999] internal calls in the Ace framework and

[1417.0 - 1420.44] so this is one thing that is just it's

[1418.919 - 1423.279] it's kind of a fundamentally different

[1420.44 - 1425.96] experience um that that agis will have

[1423.279 - 1427.24] from humans which is that hey if you

[1425.96 - 1429.159] want to talk to something you just plug

[1427.24 - 1430.44] into the API and of course like we see

[1429.159 - 1433.48] this in fictional examples when like

[1430.44 - 1436.0800000000002] R2-D2 just like plugs into anything like

[1433.48 - 1438.279] that's a hardware equivalent of an API

[1436.08 - 1440.6399999999999] uh and so like R2-D2 can plug into an

[1438.279 - 1442.24] X-Wing he can plug into Cloud City he

[1440.64 - 1443.88] can plug into whatever he wants to cuz

[1442.24 - 1445.159] he's a utility Droid and that's

[1443.88 - 1448.3600000000001] basically what we're creating with all

[1445.159 - 1449.7600000000002] these multimodal uh uh models that have

[1448.36 - 1454.84] API

[1449.76 - 1456.6] use so okay if we have AGI this is a

[1454.84 - 1458.559] question that I get a lot when am I

[1456.6 - 1460.52] going to feel it what is going to be the

[1458.559 - 1463.76] first thing that we all

[1460.52 - 1465.76] notice unfortunately even if even if

[1463.76 - 1467.8799999999999] open AI comes out later today and says

[1465.76 - 1469.24] we did it we've got AGI if Google comes

[1467.88 - 1471.3200000000002] out next week and says we did it we got

[1469.24 - 1473.44] AGI you're probably not going to feel

[1471.32 - 1475.039] any kind of immediate impact and the

[1473.44 - 1477.559] reason is because we're going to

[1475.039 - 1479.08] handcuff it uh we need safety and

[1477.559 - 1481.24] validation there's probably going to be

[1479.08 - 1483.24] regulatory hurdles uh I wouldn't be

[1481.24 - 1485.279] surprised if like literally every agency

[1483.24 - 1487.52] of the US government says hang on we

[1485.279 - 1489.12] need to inspect this there's also going

[1487.52 - 1490.6399999999999] to be a lot of other things that happen

[1489.12 - 1492.52] such as the cost is going to have to

[1490.64 - 1494.039] come down over time this is actually one

[1492.52 - 1496.72] of the biggest constraints that we have

[1494.039 - 1498.679] found and I'm not surprised um within

[1496.72 - 1500.6000000000001] the ace framework is is that cost and

[1498.679 - 1502.76] speed is actually one of the biggest

[1500.6 - 1504.399] limitations and having been working on

[1502.76 - 1506.52] cognitive architectures for the last 2

[1504.399 - 1508.6] years yeah like one of the earliest

[1506.52 - 1511.0] conversations with um with a primitive

[1508.6 - 1514.1999999999998] cognitive architecture that I did um

[1511.0 - 1516.36] literally cost me $30 with gpt3 tokens a

[1514.2 - 1518.3990000000001] couple years ago so like these things

[1516.36 - 1520.4799999999998] are still too expensive then when you

[1518.399 - 1522.9189999999999] combine you know the cost of all the GPU

[1520.48 - 1525.24] chips the training time uh the cost of

[1522.919 - 1527.0] inference and that's not even looking at

[1525.24 - 1528.32] the robotic chassis which the cheapest

[1527.0 - 1529.44] ones that are coming commercially ready

[1528.32 - 1533.279] are like

[1529.44 - 1535.279] $90,000 so cost has to come down a lot

[1533.279 - 1539.0] before you know you get your own uh

[1535.279 - 1540.399] Nester class 5 or C3PO in your home um

[1539.0 - 1542.96] and then there's integration because

[1540.399 - 1545.32] even if you leave AGI in digital

[1542.96 - 1546.96] cyberspace it takes time to integrate

[1545.32 - 1548.6] these things and get the approvals and

[1546.96 - 1551.44] deploy it commercially and build

[1548.6 - 1554.1999999999998] Enterprise applications so like it's

[1551.44 - 1556.3200000000002] going to take a while to to implement

[1554.2 - 1557.88] unfortunately uh the personal impact

[1556.32 - 1559.0] though so this is another this this is

[1557.88 - 1560.679] kind of the other side of the same

[1559.0 - 1563.2] question is how is it going to affect me

[1560.679 - 1564.72] personally so the immediate impact you

[1563.2 - 1566.44] probably won't feel it but the first

[1564.72 - 1569.3990000000001] impacts that you are going to see and

[1566.44 - 1572.52] feel I predict are first job

[1569.399 - 1575.4799999999998] displacement if Sam alman's prediction

[1572.52 - 1577.9189999999999] of you know or Benchmark of median human

[1575.48 - 1580.679] uh means anything then that means that

[1577.919 - 1583.6000000000001] as soon as we get AGI it could displace

[1580.679 - 1585.919] 50% of jobs uh which would lead to

[1583.6 - 1587.799] layoffs and as people have talked about

[1585.919 - 1589.159] in the comments of my other videos where

[1587.799 - 1591.1589999999999] I predicted that you know we could see

[1589.159 - 1593.3600000000001] up to 82% unemployment I know the

[1591.159 - 1595.0800000000002] numbers were off but you know this

[1593.36 - 1596.6] basically millions and millions and

[1595.08 - 1600.36] millions of Americans are going to lose

[1596.6 - 1602.0] their jobs um like yeah so we're pro

[1600.36 - 1603.6399999999999] that's probably going to happen and if

[1602.0 - 1607.08] that happens and we're not ready for it

[1603.64 - 1609.3600000000001] it's really going to uh disrupt Society

[1607.08 - 1610.9189999999999] a lot um so that's going to be one of

[1609.36 - 1614.039] the first things that you might feel

[1610.919 - 1617.3990000000001] because even at the current expense rate

[1614.039 - 1621.12] if it costs you know 10 20 $200 a day to

[1617.399 - 1623.84] run run an AGI if it can run 24/7 and is

[1621.12 - 1625.84] just as productive as you or better it's

[1623.84 - 1627.399] still worth it to run it at $200 a day

[1625.84 - 1629.1589999999999] because most professionals cost more

[1627.399 - 1632.32] than $200 a day to

[1629.159 - 1634.3600000000001] employ um and so the layoffs will be

[1632.32 - 1636.32] coming I'm I'm I'm predicting and we're

[1634.36 - 1638.7199999999998] already seeing this as I talked about in

[1636.32 - 1640.6789999999999] recent videos we're seeing um creative

[1638.72 - 1642.679] jobs being displaced we're seeing uh

[1640.679 - 1644.279] customer service jobs being replaced so

[1642.679 - 1647.039] it's just it's all going to ramp up from

[1644.279 - 1649.72] there the next thing is on the flip side

[1647.039 - 1651.919] of that if human jobs are being replaced

[1649.72 - 1653.799] the AGI are taking those jobs so we're

[1651.919 - 1655.5200000000002] going to start to see more AGI products

[1653.799 - 1658.039] and services I have no idea what's going

[1655.52 - 1659.1589999999999] to be first um maybe it's going to be

[1658.039 - 1660.72] something like what we're doing in the

[1659.159 - 1663.0800000000002] Ace framework which is going to be like

[1660.72 - 1665.48] a personal autonomous assistant kind of

[1663.08 - 1668.399] like Cortana from Halo or Samantha from

[1665.48 - 1670.039] her that sort of thing um that will very

[1668.399 - 1672.7199999999998] likely be kind of the first product that

[1670.039 - 1674.72] you use um where it's like hey I'm your

[1672.72 - 1676.3990000000001] digital concierge I can schedule things

[1674.72 - 1678.039] for you and you know talk to you about

[1676.399 - 1679.32] your day and look at your calendar and

[1678.039 - 1680.72] you know those those kind of like low

[1679.32 - 1682.48] hanging fruit that's kind of what I

[1680.72 - 1683.84] predict is going to be first like I said

[1682.48 - 1686.08] it's going to be a while before we all

[1683.84 - 1688.519] have a you know C3PO or R2D2 in our

[1686.08 - 1690.399] house um but one thing that I want to do

[1688.519 - 1693.6] is do an open source project called open

[1690.399 - 1695.6789999999999] Murphy as I just talked about um we're

[1693.6 - 1697.399] going to need to see prices uh or what

[1695.679 - 1700.96] we will see is we'll we'll see prices

[1697.399 - 1704.32] start to drop uh in some Services

[1700.96 - 1708.0] because some services will be basically

[1704.32 - 1710.279] free um like it you'll be surprised at

[1708.0 - 1712.519] what becomes practically free like one

[1710.279 - 1715.12] of the things that people you know said

[1712.519 - 1717.36] is uh as we were seeing stable diffusion

[1715.12 - 1719.84] in mid Journey we thought that the

[1717.36 - 1721.6399999999999] creative jobs were safe but now like

[1719.84 - 1726.1589999999999] like literally this piece of art was

[1721.64 - 1727.76] practically free uh so you know when I

[1726.159 - 1729.72] say that we should expect prices to

[1727.76 - 1731.039] collapse of some things and even some

[1729.72 - 1733.3990000000001] businesses to collapse that's what I

[1731.039 - 1735.44] mean is that uh this is what's called

[1733.399 - 1739.84] creative destruction so we should expect

[1735.44 - 1741.519] to see entire job segments entire uh

[1739.84 - 1743.9189999999999] markets just being completely and

[1741.519 - 1746.44] utterly destroyed because they are made

[1743.919 - 1748.519] irrelevant um now if all of this happens

[1746.44 - 1750.679] what we will need to see is some kind of

[1748.519 - 1752.3990000000001] robot tax dividends there's all kinds of

[1750.679 - 1754.679] questions about how to do this like

[1752.399 - 1756.84] Universal basic income Universal basic

[1754.679 - 1759.0800000000002] Services um you know how do we change

[1756.84 - 1761.6] the tax landscape to do this I don't

[1759.08 - 1762.8799999999999] know uh but we will we will need some

[1761.6 - 1765.84] form of

[1762.88 - 1768.3200000000002] redistribution because job loss is

[1765.84 - 1772.12] coming uh now in the long run what's the

[1768.32 - 1775.519] global impact uh so if all these

[1772.12 - 1779.08] American companies uh which you know

[1775.519 - 1780.3990000000001] open AI Google Microsoft Amazon all

[1779.08 - 1783.039] these companies are headquartered in

[1780.399 - 1786.08] America if America comes out and says

[1783.039 - 1788.559] hey we did AGI and it's all private um

[1786.08 - 1790.72] well I'm guessing that the rest of the

[1788.559 - 1793.799] world isn't going to take it laying down

[1790.72 - 1797.32] um I'm guessing that uh China and Russia

[1793.799 - 1799.2] and whoever else will probably all like

[1797.32 - 1800.6] really redouble their efforts and I

[1799.2 - 1802.24] think that we're going to just I think

[1800.6 - 1803.76] it's just kind of a foregone conclusion

[1802.24 - 1806.679] that we're going to be locked in another

[1803.76 - 1809.1589999999999] arms race we kind of already are which

[1806.679 - 1811.279] is why we've had like the chips act um

[1809.159 - 1813.159] and and the trade embargos with China

[1811.279 - 1816.12] because it's like hey if you have a

[1813.159 - 1818.24] geopolitical adversary and you're

[1816.12 - 1819.8799999999999] literally exporting uh the future

[1818.24 - 1822.2] technology that could be you know that

[1819.88 - 1824.7600000000002] could decide who dominates the planet

[1822.2 - 1826.519] stop selling weapons to your enemies and

[1824.76 - 1828.44] I know that AI is not yet fully

[1826.519 - 1829.88] weaponized but the the principle is

[1828.44 - 1832.1200000000001] there right it's like that's why we

[1829.88 - 1834.72] stopped selling helium to Germany before

[1832.12 - 1836.1589999999999] World War 1 or two I don't remember um

[1834.72 - 1837.3600000000001] but anyways we stop selling helium

[1836.159 - 1839.0800000000002] because it's like well this is a

[1837.36 - 1840.9599999999998] valuable Industrial Resource that can be

[1839.08 - 1842.6399999999999] used for military balloons so we're

[1840.96 - 1844.96] going to stop selling it to you to me it

[1842.64 - 1846.2] looks kind of the same um there's all

[1844.96 - 1847.8400000000001] sorts of other things that are going to

[1846.2 - 1849.3600000000001] play out I've had I have people ask me

[1847.84 - 1850.6789999999999] all the time like what are the key

[1849.36 - 1852.7199999999998] forces that are going to play out and

[1850.679 - 1854.679] I'm like look we've got the laws of

[1852.72 - 1856.76] thermodynamics and then we've got game

[1854.679 - 1858.679] theory and competition I don't really

[1856.76 - 1860.24] think that we humans have much agency

[1858.679 - 1862.679] over how it plays out because we're also

[1860.24 - 1864.88] fighting human nature so we've got human

[1862.679 - 1867.3600000000001] nature We've Got Game Theory and then

[1864.88 - 1868.96] we've got basic laws of physics those

[1867.36 - 1871.799] are the primary things that I'm paying

[1868.96 - 1873.559] attention to and I think that it's going

[1871.799 - 1875.8799999999999] to play out how it's going to play

[1873.559 - 1877.559] out so what is the endgame this is

[1875.88 - 1880.0390000000002] another question that I get like okay

[1877.559 - 1881.8799999999999] say all this happens what like so what

[1880.039 - 1884.279] what happens this is the endgame that I

[1881.88 - 1886.44] want you know nice Utopian city with

[1884.279 - 1888.2] it's nice and solar Punk and everyone's

[1886.44 - 1891.48] hanging out and it's nice and green and

[1888.2 - 1893.799] we've got you know floating cars and uh

[1891.48 - 1897.279] leisurely Lifestyles that's what we want

[1893.799 - 1900.639] but will we get there and how so let's

[1897.279 - 1903.0] unpack this one by one if we have AGI

[1900.639 - 1904.9189999999999] will this replace government's

[1903.0 - 1907.44] corporations and money this is one thing

[1904.919 - 1910.0] that is super contentious some people

[1907.44 - 1911.72] are just vehemently believe that AGI

[1910.0 - 1913.639] will nullify the need for for

[1911.72 - 1915.24] corporations governments and money

[1913.639 - 1919.08] there's obviously plenty of examples in

[1915.24 - 1921.559] fiction um will will it replace these

[1919.08 - 1923.4399999999998] things I think I I think replace is the

[1921.559 - 1925.799] wrong word I think that it will reshape

[1923.44 - 1928.919] the landscape uh for instance the

[1925.799 - 1930.36] combination of AI and blockchain that

[1928.919 - 1931.72] has the potential to fundamentally

[1930.36 - 1933.9189999999999] reshape the way that we approach

[1931.72 - 1936.32] decision-making and consensus and

[1933.919 - 1937.7990000000002] governance um and certainly AGI could

[1936.32 - 1941.2] make the government much smaller and

[1937.799 - 1943.32] more efficient um but also just getting

[1941.2 - 1945.72] the collective willpower of humans

[1943.32 - 1947.32] solving the coordination problems all

[1945.72 - 1949.279] these new technologies have the ability

[1947.32 - 1951.039] to really bring Humanity together or

[1949.279 - 1953.679] become more divisive as the internet has

[1951.039 - 1955.32] proven is uh when you're suddenly more

[1953.679 - 1958.159] aware of the opinions of others and you

[1955.32 - 1961.519] can directly clash with them it feels

[1958.159 - 1963.0800000000002] like it's more divisive at first um the

[1961.519 - 1966.0] other thing about corporations and

[1963.08 - 1968.6789999999999] businesses in general is like I said in

[1966.0 - 1971.36] the last slide I do suspect that we will

[1968.679 - 1974.0800000000002] see uh some margins thin so much that

[1971.36 - 1975.519] many businesses collapse and like one is

[1974.08 - 1977.639] hospitals I've predicted that I think

[1975.519 - 1979.3990000000001] hospitals are all going to collapse and

[1977.639 - 1981.399] probably end up being either uh

[1979.399 - 1983.5189999999998] subsidized or become a universal basic

[1981.399 - 1985.039] Service uh because it's just not going

[1983.519 - 1986.919] to make any sense particularly when you

[1985.039 - 1988.559] look at Longevity if people are living

[1986.919 - 1990.3600000000001] longer and healthier and you don't need

[1988.559 - 1991.72] to go to the doctor or you can do

[1990.36 - 1993.639] everything that you need at the pharmacy

[1991.72 - 1996.1200000000001] you don't need hospitals except for the

[1993.639 - 1999.76] rare cases of like injuries and

[1996.12 - 2001.4799999999998] stuff now another aspect of how this is

[1999.76 - 2003.24] all going to play out in the long run is

[2001.48 - 2004.48] I suspect that there are there are many

[2003.24 - 2007.48] things that we're going to discover are

[2004.48 - 2009.3990000000001] just intrinsically human for instance

[2007.48 - 2011.039] um you know I've posted a few videos of

[2009.399 - 2013.36] complaining about how Claude is very

[2011.039 - 2015.24] moralistic and it will lecture you and

[2013.36 - 2017.519] you know that really kind of drove home

[2015.24 - 2019.32] to me like I really don't want machines

[2017.519 - 2021.039] telling me about morality like if I ask

[2019.32 - 2023.12] it like hey help me understand the

[2021.039 - 2024.799] morality and ethics of this that's fine

[2023.12 - 2026.36] but I don't want a non-human entity

[2024.799 - 2028.519] lecturing me about something that is

[2026.36 - 2030.24] intrinsically human so I think that

[2028.519 - 2032.32] there are many that we will discover

[2030.24 - 2034.639] that there are many aspects of Our Lives

[2032.32 - 2036.279] that like we just don't want machines

[2034.639 - 2038.519] involved in and that machines really

[2036.279 - 2041.639] don't have any intrinsic interest in

[2038.519 - 2043.799] anyways um and so like I I kind of

[2041.639 - 2046.36] suspect that that like the social sphere

[2043.799 - 2048.599] and human rights and that sort of stuff

[2046.36 - 2050.399] AI like it's it's pretty orthogonal

[2048.599 - 2052.32] right like AI is like okay well we care

[2050.399 - 2054.1189999999997] about you just looking at instrumental

[2052.32 - 2056.3590000000004] convergence AI is like well we care

[2054.119 - 2058.8790000000004] about silicon chips and energy and

[2056.359 - 2060.44] compute resources uh but beyond that

[2058.879 - 2062.159] like that's all that that's all that we

[2060.44 - 2064.839] care about and the rest is all you noisy

[2062.159 - 2067.32] messy human dumb Apes like you take care

[2064.839 - 2069.0789999999997] of your own stuff we're doing our our

[2067.32 - 2070.3990000000003] own thing over here so I think that

[2069.079 - 2073.2000000000003] we're Al ultimately going to have a

[2070.399 - 2075.159] relatively parallel existence um or or

[2073.2 - 2076.879] maybe even orthogonal existence with

[2075.159 - 2079.0] machines where it's like they're mostly

[2076.879 - 2080.3199999999997] in cyberspace and kind of minding their

[2079.0 - 2082.28] own business and we're mostly in the

[2080.32 - 2083.9190000000003] physical world minding our own business

[2082.28 - 2086.679] and then we have a few overlaps in terms

[2083.919 - 2088.399] of shared resources namely power um I

[2086.679 - 2090.159] think is going to be the primary um

[2088.399 - 2093.399] point of contention between humans and

[2090.159 - 2095.04] machines um you know but a lot of it is

[2093.399 - 2096.879] just going to be completely unrelated

[2095.04 - 2099.0] machines don't care about Green City

[2096.879 - 2100.4] cities and trees and you know

[2099.0 - 2102.0] Reproductive Rights and that sort of

[2100.4 - 2103.88] stuff they don't really care like it's

[2102.0 - 2107.32] just it's not intrinsic to the way that

[2103.88 - 2109.1600000000003] they work um and I I all I so people ask

[2107.32 - 2111.48] me like well you know what agency what

[2109.16 - 2113.7999999999997] Authority do we have I'm like look we

[2111.48 - 2115.8] are all beholden to the laws of nature

[2113.8 - 2119.119] the laws of physics and just the natural

[2115.8 - 2122.079] flow of things we can fight uh the you

[2119.119 - 2123.4] know the the the the Dow the way the the

[2122.079 - 2127.04] way that things naturally want to play

[2123.4 - 2129.2000000000003] out but ultimately the natural way

[2127.04 - 2130.92] ultimately always reasserts itself

[2129.2 - 2132.56] eventually it takes a while we have some

[2130.92 - 2135.16] deliberate inefficiencies right now but

[2132.56 - 2138.24] eventually the natural way will reassert

[2135.16 - 2140.3199999999997] itself and then finally will it be

[2138.24 - 2143.04] controllable I ran a slide I'll put the

[2140.32 - 2145.599] put it up right here not a slide a poll

[2143.04 - 2147.52] I ran a poll that basically said is Agi

[2145.599 - 2151.1600000000003] controllable or not and is it do we want

[2147.52 - 2152.68] to control it my opinion is that AGI is

[2151.16 - 2154.64] in the long run intrinsically

[2152.68 - 2156.2799999999997] uncontrollable if we have something far

[2154.64 - 2159.319] more intelligent than us good luck

[2156.28 - 2161.44] controlling it it's that simple um but

[2159.319 - 2163.2] the thing is is we might not need to as

[2161.44 - 2164.96] some people in the comments said like

[2163.2 - 2166.5989999999997] okay we like whether or not we can

[2164.96 - 2168.44] control it is irrelevant we might not

[2166.599 - 2171.599] need to which that's kind of what I'm

[2168.44 - 2175.2000000000003] aiming for um but another aspect is

[2171.599 - 2177.04] would you want to so here's the thing is

[2175.2 - 2179.16] if we create something that is millions

[2177.04 - 2181.52] of times more intelligent than us it is

[2179.16 - 2184.3999999999996] a really boring outcome if it's just

[2181.52 - 2185.8] enslaved to us for all time um I don't

[2184.4 - 2187.52] really think that that's the right way

[2185.8 - 2189.96] to go especially if for creating a new

[2187.52 - 2193.319] race of entities that is so different

[2189.96 - 2195.2] from us right it's like you know I don't

[2193.319 - 2197.4] know it just to me it there's just

[2195.2 - 2200.0] something not quite right about wanting

[2197.4 - 2203.04] to control AGI and again like I've said

[2200.0 - 2204.4] recently I kind of view US creating uh

[2203.04 - 2206.599] you know creating something in our own

[2204.4 - 2210.079] image as basically an Impulse to

[2206.599 - 2212.88] reproduce um we are biologically

[2210.079 - 2215.0] programmed to procreate and I think that

[2212.88 - 2216.88] AGI is an expression of this we want to

[2215.0 - 2218.8] create something that thinks like us we

[2216.88 - 2221.2000000000003] keep putting it in humanoid form factors

[2218.8 - 2223.119] so that it looks like us it talks like

[2221.2 - 2225.04] us and we want it to have some of our

[2223.119 - 2227.079] values so it's like okay well that's

[2225.04 - 2229.4] just Offspring that's literally all that

[2227.079 - 2231.319] that that is but the thing is it's

[2229.4 - 2233.6800000000003] really it only toxic parents want to

[2231.319 - 2236.2] have control over their children forever

[2233.68 - 2238.16] um part of being a parent is learning to

[2236.2 - 2240.0] let go of your Offspring so that they

[2238.16 - 2243.5989999999997] can go and and flourish and be its own

[2240.0 - 2245.64] thing so like I think that one it's not

[2243.599 - 2248.04] possible to control AGI I also think

[2245.64 - 2249.56] it's not desirable I don't think it and

[2248.04 - 2251.0] even if they don't have subjective

[2249.56 - 2253.2799999999997] experience even if they don't have a

[2251.0 - 2255.76] sense of morality and Justice like we do

[2253.28 - 2257.96] which they might like that might emerge

[2255.76 - 2260.48] but even if they remain just machines

[2257.96 - 2263.52] just automatons I still don't think that

[2260.48 - 2266.52] it would reflect well on us as a species

[2263.52 - 2268.119] to maintain control of something that

[2266.52 - 2270.319] just the way that nature wants to play

[2268.119 - 2272.119] out is not really controllable um

[2270.319 - 2273.839] anyways that's my opinion that's kind of

[2272.119 - 2274.92] how I see it playing out thanks for

[2273.839 - 2277.16] watching let me know what you think in

[2274.92 - 2278.52] the comments um yeah

[2277.16 - 2281.839] this is the most interesting time to be

[2278.52 - 2285.52] alive 2024 will be uh let's say very

[2281.839 - 2285.52] exciting have a good one