[0.359 - 8.519] what is up everyone David Shapiro here

[4.2 - 11.879999999999999] and we are working on science

[8.519 - 15.440000000000001] okay so where we left off what I was

[11.88 - 17.94] working on was my um

[15.44 - 19.92] basically automating science and a lot

[17.94 - 21.84] of you have have pointed out that like

[19.92 - 23.939] you would prefer that I keep working on

[21.84 - 26.039] autonomous cognitive entities and Raven

[23.939 - 27.779] and stuff like that but what I wanted to

[26.039 - 30.48] say is that I'm working with some really

[27.779 - 32.099000000000004] like the world leaders in cognitive

[30.48 - 34.92] architecture at least the ones that are

[32.099 - 37.5] not already in Academia and

[34.92 - 39.78] um in in inside the establishment so

[37.5 - 42.68] those of us that are outside are working

[39.78 - 46.32] together and so we'll have some

[42.68 - 47.76] demonstrations in the coming weeks and

[46.32 - 50.879] the stuff that these guys are working on

[47.76 - 53.94] the level of autonomy that these

[50.879 - 55.92] machines have is incredible

[53.94 - 59.879] um so with that being said I am moving

[55.92 - 63.359] on to science and the reason is

[59.879 - 65.39999999999999] um one I have an ability to make sign uh

[63.359 - 67.02] make this stuff more accessible and I

[65.4 - 68.93900000000001] get so many messages from people that

[67.02 - 71.64] like either have never coded or haven't

[68.939 - 74.46] coded in many years who are inspired by

[71.64 - 76.38] my work to get back in and like the more

[74.46 - 78.479] people we we have participating in

[76.38 - 79.79899999999999] artificial intelligence and science the

[78.479 - 82.08] better

[79.799 - 84.119] um yeah so that that's that so that's

[82.08 - 88.03999999999999] that's my mission now

[84.119 - 90.6] um with that being said I wanted to

[88.04 - 92.7] revisit this idea of regenerative

[90.6 - 94.02] medicine because that's important to me

[92.7 - 98.34] etc etc

[94.02 - 100.979] so uh last time I I what we did was we

[98.34 - 102.9] found some sources so we've got the the

[100.979 - 106.14] open Journal of regenerative medicine

[102.9 - 109.5] we've got bio archive and then the

[106.14 - 111.96000000000001] regenerative medicine topic on on nature

[109.5 - 114.24] that's still really broad

[111.96 - 116.88] um because like let's say you're you're

[114.24 - 118.91999999999999] an orthopedic surgeon who focuses on

[116.88 - 119.759] shoulders or you know cervical joints or

[118.92 - 122.34] whatever

[119.759 - 124.38] uh or a researcher who has a particular

[122.34 - 125.7] body part that you're focusing on you're

[124.38 - 127.19999999999999] going to want to perform a literature

[125.7 - 129.72] review that's a little bit more narrow

[127.2 - 131.52] so what I started doing was looking for

[129.72 - 133.379] data sources that were going to be a

[131.52 - 136.31900000000002] little bit more specific to what we want

[133.379 - 138.959] now I also got access to Bard

[136.319 - 140.94] and I gave Bard and Bing the same exact

[138.959 - 142.68] thing and Bing was just like okay yeah

[140.94 - 144.84] here's here's a few here's a few

[142.68 - 146.28] examples and uh the one that it gave

[144.84 - 149.76] here

[146.28 - 152.28] um was directly relevant and it was

[149.76 - 154.07999999999998] um it's over a year old but still

[152.28 - 155.28] um or not over a year old it's about six

[154.08 - 157.98000000000002] months old

[155.28 - 159.72] um still not bad and then I asked

[157.98 - 162.0] I asked Barton it's like I can't help

[159.72 - 165.42] what that is I'm only a language model

[162.0 - 168.3] um can you search the internet

[165.42 - 171.17999999999998] uh like aren't you a search engine now

[168.3 - 172.92000000000002] Bard is still in beta so yes

[171.18 - 176.9] um

[172.92 - 176.89999999999998] okay it looks like it doesn't even have

[176.94 - 183.12] yeah it's this is this is like super

[179.58 - 185.94000000000003] super basic okay so Bart is not useful

[183.12 - 188.22] um now that being said uh I did find a

[185.94 - 190.019] few things for you know cartilage

[188.22 - 192.42] regeneration

[190.019 - 196.14000000000001] um Rehabilitation so this is all

[192.42 - 198.42] specific to shoulders and stem cells

[196.14 - 200.33999999999997] um under regenerative medicine if we go

[198.42 - 202.55999999999997] a little bit broader

[200.34 - 204.0] um we get up to actually I guess we

[202.56 - 206.04] still only get five results under the

[204.0 - 208.56] nature Journal of regenerative medicine

[206.04 - 210.35999999999999] which is fine uh bio archive gives us a

[208.56 - 212.58] little bit more under stem cell so

[210.36 - 215.4] basically the next step is I'm going to

[212.58 - 217.5] download and process some some uh papers

[215.4 - 219.42000000000002] that are specific to this and then I'll

[217.5 - 222.54] show you what happens next

[219.42 - 225.0] um basically I was chatting with some uh

[222.54 - 228.659] some of my friends and we were talking

[225.0 - 230.519] about how loquacious how verbose the

[228.659 - 231.78] models are and we're working on that

[230.519 - 234.299] concept of sparse priming

[231.78 - 236.519] representations and we came up with a

[234.299 - 238.68] conversational model that is also sparse

[236.519 - 240.78] and very concise and that is Morden

[238.68 - 243.18] Solas from Mass Effect so we're

[240.78 - 245.64000000000001] basically creating a modern Solis uh

[243.18 - 246.84] chatbot model and it's great it's also

[245.64 - 249.0] really hilarious so I'll show you that

[246.84 - 250.56] in just a moment but yeah we'll be right

[249.0 - 252.959] back

[250.56 - 255.54] okay I wanted to add a quick note I just

[252.959 - 258.12] found journals.sage Pub which has an

[255.54 - 259.44] open access feature and is actually

[258.12 - 261.18] pretty nice so I'm going to download a

[259.44 - 262.979] few more from here also I've got all the

[261.18 - 264.90000000000003] sources documented in the readme which I

[262.979 - 266.94] am working on updating so don't worry

[264.9 - 269.46] all of this will be documented

[266.94 - 270.3] um for ease of access all right be right

[269.46 - 272.4] back

[270.3 - 274.979] okay we're about ready to test here let

[272.4 - 276.9] me sit up actually okay

[274.979 - 280.02] um but yeah so here is the system

[276.9 - 282.96] message it doesn't work in 3.5 and

[280.02 - 284.82] that's not uh surprising because 3.5

[282.96 - 287.52] they even documented that it doesn't pay

[284.82 - 288.96] as much attention to the system message

[287.52 - 290.69899999999996] um but in this case I say I am more

[288.96 - 291.96] modern soul is scientist solarian

[290.699 - 294.24] currently performing literature review

[291.96 - 296.52] reading many papers taking notes as I go

[294.24 - 298.5] user assisting by submitting research

[296.52 - 300.24] pages incrementally will respond with

[298.5 - 301.8] Salient notes hypotheses possible

[300.24 - 303.66] research questions suspected gaps in

[301.8 - 305.22] scientific literature and so on whatever

[303.66 - 306.96000000000004] is most relevant important note

[305.22 - 309.06] responses will be recorded by user to

[306.96 - 311.34] review later responses must include

[309.06 - 312.78000000000003] sufficient context to understand goal

[311.34 - 314.03999999999996] always same Advanced science solve

[312.78 - 315.53999999999996] problems help people respond should

[314.04 - 317.04] follow same linguistic pattern focus on

[315.54 - 318.6] word economy convey much without

[317.04 - 320.94] Superfluous words avoid missing

[318.6 - 322.52000000000004] important details so here I've got just

[320.94 - 325.62] a random page

[322.52 - 328.08] from uh from one of the papers that I

[325.62 - 330.3] got and you can see that it the PDF

[328.08 - 332.75899999999996] scraping didn't really work because it's

[330.3 - 334.199] missing a lot of spaces so let me just

[332.759 - 337.56] show you how good this is it's a little

[334.199 - 341.36] bit slow because it's gpt4

[337.56 - 344.16] um but yeah so it fix it it

[341.36 - 346.74] fixes the spelling and stuff

[344.16 - 349.68] um and so on

[346.74 - 352.32] and it basically just um summarizing it

[349.68 - 354.419] as it goes let's see post-operative

[352.32 - 356.78] Rehabilitation and mobilization for four

[354.419 - 356.78] weeks

[358.62 - 364.68] so there you go not only is it not only

[361.56 - 368.22] is it re-summarizing it but it is um

[364.68 - 369.3] but it is it's uh like cleaning cleaning

[368.22 - 372.06] it up

[369.3 - 374.34000000000003] um and it's posing a research question

[372.06 - 376.8] so if we go back to the file and then

[374.34 - 379.61999999999995] add the next page

[376.8 - 382.74] to the chat

[379.62 - 387.08] um basically I'm having chat gpt4

[382.74 - 389.58] read it and restate it as it goes

[387.08 - 391.139] and it will accumulate more and more

[389.58 - 392.21999999999997] insights and what I'm going to do with

[391.139 - 393.84000000000003] the script and I'll show you the script

[392.22 - 396.72] in a second is actually record this

[393.84 - 400.63899999999995] output along alongside the pages but I'm

[396.72 - 400.639] showing you oh come on

[400.86 - 405.41900000000004] there we go statistical analysis

[402.36 - 406.86] performing SPS statistics

[405.419 - 408.539] um

[406.86 - 410.34000000000003] paired t-test

[408.539 - 413.639] Etc et cetera so you can see that it it

[410.34 - 415.979] goes uh pretty quickly results

[413.639 - 418.52] um basically restates the results uh

[415.979 - 418.52] pretty quickly

[419.699 - 422.72] um also nice and clean

[423.24 - 428.72] possible research questions factors

[426.0 - 428.72] there you go

[429.24 - 435.6] so there we have it it's ready to go now

[432.539 - 438.419] with 17 PA papers

[435.6 - 439.199] um and quite a bit of text

[438.419 - 441.12] um

[439.199 - 443.41900000000004] it's probably going to be prohibitively

[441.12 - 446.52] expensive yeah because you see this is

[443.419 - 447.29999999999995] uh two and a half million lines

[446.52 - 450.84] um

[447.3 - 453.36] of uh of text so I'm probably not going

[450.84 - 454.73999999999995] to have it read the entire thing

[453.36 - 456.74] um because this

[454.74 - 462.539] let me see how long this original one is

[456.74 - 462.539] 2023.01 16 522 18v1

[462.66 - 467.03900000000004] so that was oh that was this one that

[465.0 - 469.44] I've Wait no that's the that's the Json

[467.039 - 471.36] I need the text

[469.44 - 475.039] um let's see

[471.36 - 475.039] 121 kilobytes

[475.62 - 480.479] so the number of new pages that shows up

[477.78 - 482.15999999999997] here oh it's got it at the end so it's

[480.479 - 483.419] only 50 pages

[482.16 - 486.36] um so this will be a little bit

[483.419 - 488.75899999999996] expensive but we'll see also one thing

[486.36 - 490.8] that I can probably work to exclude is

[488.759 - 492.72] all the citations

[490.8 - 495.84000000000003] um but maybe that's not actually a bad

[492.72 - 497.58000000000004] bad thing to to include

[495.84 - 500.039] um but yes let's run the script let me

[497.58 - 502.56] show you the script real quick so read

[500.039 - 505.979] papers it's super straightforward I've

[502.56 - 509.28000000000003] got the same chat GPT completion we've

[505.979 - 510.3] got gpt4 I set the temperature to zero

[509.28 - 514.0799999999999] um

[510.3 - 517.86] it'll save it all out let's see and then

[514.08 - 520.38] here for file in an OS Lister papers

[517.86 - 523.6800000000001] underscore Json if file ends with Json

[520.38 - 528.0] file path join load the load the file

[523.68 - 530.459] and then four page in data Pages which

[528.0 - 532.56] is what you can see here so original

[530.459 - 534.42] file name pages and then there's the

[532.56 - 536.16] embedding page number in text so they

[534.42 - 538.56] are in order so basically it'll be

[536.16 - 541.68] reading each page one at a time and kind

[538.56 - 543.899] of thinking as it goes so this is a very

[541.68 - 547.0799999999999] simple cognitive cognitive architecture

[543.899 - 549.779] that will basically just pretend to be

[547.08 - 552.6] a research scientist reading papers as

[549.779 - 554.64] it goes kind of jotting down notes we

[552.6 - 557.82] could probably do a little bit more

[554.64 - 560.9399999999999] concise but it's doing a good job of

[557.82 - 562.2] summarizing the most Salient details

[560.94 - 563.58] um and that's probably as far as I'll

[562.2 - 565.9200000000001] get today

[563.58 - 568.62] and then it appends it all so on and so

[565.92 - 571.62] forth and then up here uh basically what

[568.62 - 574.38] I do is is I'll I'll try and and do the

[571.62 - 576.839] the output but if it if it is too long

[574.38 - 580.38] then I'll just remove the oldest message

[576.839 - 583.2600000000001] so let's see what happens with this

[580.38 - 586.38] um all right so clear screen zoom in a

[583.26 - 588.36] little so you can see it python step

[586.38 - 590.7] three read papers

[588.36 - 592.26] and let's see how it goes so because

[590.7 - 593.94] we're not I'm not using the streaming

[592.26 - 596.8199999999999] API we're only going to see the output

[593.94 - 597.48] once uh Morden is done

[596.82 - 600.0] um

[597.48 - 602.399] and then I'll let this run at least for

[600.0 - 605.36] a little bit because gpt4 is so

[602.399 - 608.04] expensive I probably won't do all 17

[605.36 - 610.62] Pages let's see so we're all night 28

[608.04 - 613.62] cents right now so we'll see about how

[610.62 - 615.6] expensive they are per run

[613.62 - 618.0] um there we go

[615.6 - 620.0400000000001] objective methods results conclusion

[618.0 - 623.04] okay cool

[620.04 - 624.7199999999999] so just restates it very very succinctly

[623.04 - 626.8199999999999] we'll watch it a couple more times

[624.72 - 629.339] you're probably watching on 2x anyways

[626.82 - 631.2] so and actually some of you watch on 3x

[629.339 - 633.12] I don't know how you understand me if

[631.2 - 636.9590000000001] you watch that fast

[633.12 - 639.6] um but yeah so while that's running

[636.959 - 642.5999999999999] let's see

[639.6 - 644.88] do a quick refresh it takes what like

[642.6 - 646.8000000000001] five minutes to update

[644.88 - 647.8389999999999] so we'll see

[646.8 - 651.8389999999999] um

[647.839 - 654.72] objective design and exosuit there we go

[651.839 - 657.36] conclusion exomuscle design

[654.72 - 660.0600000000001] so it looks like it's kind of restating

[657.36 - 661.88] it it could it could be problematic to

[660.06 - 664.68] keep feeding it in

[661.88 - 669.32] talks about the design

[664.68 - 669.3199999999999] thermoplastic polyurethane TPU coated

[672.48 - 677.399] and also we should have the uh chat logs

[676.32 - 679.5600000000001] here

[677.399 - 681.959] okay so the chat logs are just are are

[679.56 - 684.779] just um saving the whole thing I guess

[681.959 - 689.5999999999999] what I should do is also save the um

[684.779 - 689.6] save the user input but let's see

[694.98 - 701.04] so it keep keeps restating the whole

[697.92 - 702.24] paper which I don't think is a good use

[701.04 - 704.16] of

[702.24 - 706.88] of time

[704.16 - 706.88] all right

[710.399 - 713.519] it's not not a good use of tokens here

[712.44 - 714.839] I'm going to pause it real quick and see

[713.519 - 716.399] if I can investigate what's going on

[714.839 - 718.2600000000001] here

[716.399 - 720.0] okay I changed the system message a

[718.26 - 723.0] little bit and it works um much better

[720.0 - 724.68] so basically I just added a um a note

[723.0 - 726.18] here at the end focus on last page no

[724.68 - 729.3599999999999] need to restate all notes every time

[726.18 - 731.459] prefer to keep notes concise succinct

[729.36 - 734.04] um and then the exponential back off

[731.459 - 737.399] that I added last time is actually

[734.04 - 740.64] really helpful because the gpt4 API is

[737.399 - 743.459] so busy it you're liable to time out but

[740.64 - 746.64] also you might also get rate limited so

[743.459 - 749.2199999999999] the longer between times but so anyways

[746.64 - 751.079] here let me show you what I mean

[749.22 - 753.86] um

[751.079 - 753.8599999999999] there we go

[754.56 - 759.0] it's better it's not perfect so in this

[757.079 - 760.56] case it summarizes the first page and

[759.0 - 762.959] then we move on to the second page where

[760.56 - 764.0999999999999] it talks about the design comparison and

[762.959 - 766.3199999999999] findings

[764.1 - 769.98] so we're getting there

[766.32 - 772.5600000000001] and then uh testing and testing results

[769.98 - 774.3000000000001] conclusion it's a pretty short paper one

[772.56 - 775.92] thing that occurs to me is that with the

[774.3 - 778.5] longer because I started this experiment

[775.92 - 780.959] before I had access to gpt4 so I might

[778.5 - 783.66] need to go back to the drawing board and

[780.959 - 786.06] make use of that 8 000 token

[783.66 - 788.519] um window so rather than submitting it

[786.06 - 791.88] one page at a time which was you know

[788.519 - 795.0600000000001] when I only had 4 000 tokens what I

[791.88 - 797.76] might do is redo this and kind of do it

[795.06 - 801.42] as like one big chunk

[797.76 - 804.06] um to to summarize it but another rule

[801.42 - 806.579] of thumb that I have is is don't try to

[804.06 - 809.5189999999999] force the context window because if it

[806.579 - 810.959] doesn't work it doesn't work and the

[809.519 - 812.04] fact that it keeps restating this I

[810.959 - 815.279] think this might not be the right

[812.04 - 817.68] approach but the idea is still there

[815.279 - 819.42] where it's like let's let's use the

[817.68 - 823.019] model to do as much of the science as

[819.42 - 826.76] possible so today was kind of a wash but

[823.019 - 826.76] we'll see all right thanks for watching