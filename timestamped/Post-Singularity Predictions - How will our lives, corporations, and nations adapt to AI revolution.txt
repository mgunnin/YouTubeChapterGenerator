[0.9 - 7.44] morning everybody David Shapiro here

[3.12 - 9.0] with a video so my video about AGI was

[7.44 - 10.8] super popular

[9.0 - 13.32] um I suppose I should uh should have

[10.8 - 16.26] anticipated that you know making a bold

[13.32 - 18.060000000000002] declaration like AGI within 18 months so

[16.26 - 23.220000000000002] with that being said

[18.06 - 25.5] uh the ramp up to AGI ASI or super

[23.22 - 26.939] intelligence and then Singularity seems

[25.5 - 29.88] like it's accelerating especially if you

[26.939 - 31.259999999999998] go by Reddit comments on R Singularity

[29.88 - 32.699999999999996] and a few other places people are like

[31.26 - 35.28] is this happening are we actually

[32.7 - 38.1] approaching the wall of exponential

[35.28 - 39.6] takeoff so assuming that that is the

[38.1 - 41.64] case and that we are ramping up to the

[39.6 - 43.739000000000004] singularity within the coming uh you

[41.64 - 46.14] know months or years or whatever let's

[43.739 - 47.699999999999996] explore how the singularity will

[46.14 - 49.26] actually unfold

[47.7 - 51.84] first

[49.26 - 54.12] we need to Define singularity so what do

[51.84 - 56.879000000000005] we mean when we say Singularity there's

[54.12 - 58.62] obviously a lot of ways to Define this

[56.879 - 60.539] thing and the simplest way that I could

[58.62 - 62.94] come up with to Define it is just we say

[60.539 - 65.04] that the singularity is when AI becomes

[62.94 - 67.92] orders of magnitude more intelligent

[65.04 - 70.08000000000001] than all humans combined so basically if

[67.92 - 74.1] current trends continue in terms of AI

[70.08 - 77.22] research and the power of AI gpt4 is

[74.1 - 80.69999999999999] just as intelligent as many people more

[77.22 - 83.15899999999999] intelligent than some and gpt5 is being

[80.7 - 85.92] trained and there's open source versions

[83.159 - 87.96000000000001] etc etc so basically if you haven't been

[85.92 - 91.38] living under a rock you are probably

[87.96 - 92.82] aware of the rapid ramp up of AI and

[91.38 - 94.5] it's not showing any signs of slowing

[92.82 - 96.83999999999999] down if anything it's accelerating

[94.5 - 98.579] because now we're not looking at AI

[96.84 - 101.10000000000001] advancements on a monthly basis we're

[98.579 - 103.079] looking at it on a weekly basis I was

[101.1 - 104.579] actually at a Meetup recently and

[103.079 - 105.89999999999999] someone pointed that out they're like I

[104.579 - 107.39999999999999] think we're actually already in the

[105.9 - 110.52000000000001] singularity because we're measuring

[107.4 - 112.5] advancements on a week to week basis and

[110.52 - 114.17999999999999] soon we might be measuring it on a

[112.5 - 117.72] day-to-day basis

[114.18 - 120.06] okay so let's break this down what are

[117.72 - 122.03999999999999] some of the macro economic changes

[120.06 - 123.36] um that we can expect to see with the

[122.04 - 125.759] singularity

[123.36 - 128.099] first we have to talk about what remains

[125.759 - 130.25900000000001] scarce because it's very easy to get

[128.099 - 131.7] caught up in this you know magical

[130.259 - 133.92] thinking of AI is going to change

[131.7 - 136.2] everything but there's going to be some

[133.92 - 139.14] things that don't change with Ai No

[136.2 - 141.11999999999998] matter how smart it gets so if the

[139.14 - 142.67999999999998] singularity happens there's a few things

[141.12 - 145.56] that are not really going to change that

[142.68 - 148.20000000000002] much so first is desirable and arable

[145.56 - 151.14000000000001] land some places will remain deserts

[148.2 - 154.2] hence this AI generated image of a woman

[151.14 - 156.35999999999999] in a desert we'll get to that later when

[154.2 - 159.29999999999998] we talk about Fusion

[156.36 - 161.09900000000002] but uh fresh potable water most of the

[159.3 - 163.56] water on the planet is also salt water

[161.099 - 166.56] again we might be able to change that if

[163.56 - 169.44] we solve uh if we solve uh nuclear

[166.56 - 170.58] fusion and other energy sources

[169.44 - 173.16] um but then there's other physical

[170.58 - 176.28] resources such as minerals

[173.16 - 179.34] um and and and mind natural resources

[176.28 - 182.04] that will also probably remain scarce no

[179.34 - 184.31900000000002] matter how intelligent AI becomes so

[182.04 - 186.9] this will be a really con uh critical

[184.319 - 189.599] constraint which could really drive up

[186.9 - 191.81900000000002] the value of some of these resources

[189.599 - 193.79999999999998] um but and as I mentioned if we solve

[191.819 - 196.73899999999998] nuclear fusion we could desalinate water

[193.8 - 198.0] we could air irrigate deserts and so on

[196.739 - 200.22] and so forth but that could have

[198.0 - 202.62] unintended consequences because if

[200.22 - 204.9] suddenly you irrigate every desert on

[202.62 - 207.06] the planet maybe those deserts actually

[204.9 - 210.239] form a really critical component of our

[207.06 - 211.98] ecosystem lastly if we solve space

[210.239 - 214.68] flight we could probably start

[211.98 - 217.2] harvesting asteroids and other even

[214.68 - 218.76000000000002] other planets for Rare Minerals because

[217.2 - 220.85999999999999] there are

[218.76 - 223.379] trillions and trillions and trillions of

[220.86 - 225.36] dollars worth of Rare Minerals out there

[223.379 - 226.98] on the solar system so it's entirely

[225.36 - 229.31900000000002] possible that all of these will actually

[226.98 - 230.819] be solved at some point in the future

[229.319 - 232.67999999999998] with this with the singularity but at

[230.819 - 234.78] least in the short term these will

[232.68 - 237.36] remain scarce resources

[234.78 - 239.459] now on the flip side from a

[237.36 - 241.5] macroeconomic perspective what becomes

[239.459 - 243.0] abundant with the singularity the

[241.5 - 244.98] primary thing that becomes abundant with

[243.0 - 247.56] the singularity is knowledge information

[244.98 - 249.72] and cognitive labor so what I mean by

[247.56 - 253.2] cognitive labor is thinking knowledge

[249.72 - 255.84] work service industry jobs so basically

[253.2 - 257.94] what happens if AI becomes orders of

[255.84 - 259.38] magnitude smarter than humans and we

[257.94 - 262.44] remain in control of it this is all

[259.38 - 264.419] assuming the the good ending right that

[262.44 - 266.52] we don't get wiped out

[264.419 - 269.75899999999996] basically what that means is that human

[266.52 - 273.12] cognitive effort becomes irrelevant

[269.759 - 274.259] um now that sounds really awful but one

[273.12 - 276.72] thing that I realized while I was

[274.259 - 277.86] working on this is that on an individual

[276.72 - 281.04] basis

[277.86 - 282.96000000000004] most people's cognitive effort is

[281.04 - 285.12] already irrelevant right we're on a

[282.96 - 286.68] planet of 8 billion people chances are

[285.12 - 288.78000000000003] someone has already solved the problem

[286.68 - 290.88] that you're working on whether or not

[288.78 - 292.5] you realize it is a different you know

[290.88 - 294.65999999999997] story but

[292.5 - 296.699] um you know doing science and and

[294.66 - 298.62] solving problems it can be really

[296.699 - 302.1] difficult and it's it's mostly a matter

[298.62 - 304.32] of right place right time but you know

[302.1 - 306.3] there are there well with that being

[304.32 - 307.32] said there are still unsolved problems

[306.3 - 311.1] out there

[307.32 - 313.919] and the singularity with you know hyper

[311.1 - 315.24] Advanced AI will probably just result in

[313.919 - 316.85999999999996] kind of the same behavior that we're

[315.24 - 318.6] already seeing because the collective

[316.86 - 321.36] wisdom of humanity solves problems

[318.6 - 323.46000000000004] pretty quickly right but the velocity of

[321.36 - 326.039] that problem solving will probably go up

[323.46 - 328.979] uh you know maybe a few degrees maybe an

[326.039 - 331.8] order of magnitude not sure yet but

[328.979 - 334.5] a hyperabundance of cognitive labor is

[331.8 - 336.72] is actually probably not going to be as

[334.5 - 339.0] immediately and dramatically impactful

[336.72 - 340.68] as you might think because like look at

[339.0 - 343.259] Reddit and Twitter and other social

[340.68 - 346.44] media platforms that allow you to solve

[343.259 - 349.199] problems get answers uh and and move on

[346.44 - 350.88] very quickly so basically instead of

[349.199 - 353.1] having you know lazy Twitter or Reddit

[350.88 - 355.08] where you ask a problem ask the machines

[353.1 - 356.759] for a problem where you know people are

[355.08 - 358.25899999999996] collectively now the machines are going

[356.759 - 359.82] to be doing it and actually some of the

[358.259 - 362.28000000000003] people that I'm working with on these

[359.82 - 363.71999999999997] autonomous AI projects one of the key

[362.28 - 366.23999999999995] things that we're working on is figuring

[363.72 - 369.90000000000003] out how to get AI to talk to each other

[366.24 - 371.46000000000004] in an autonomous manner that is safe and

[369.9 - 373.979] transparent and this is where natural

[371.46 - 376.08] language comes in because you don't want

[373.979 - 378.84] AI using their own coded language to

[376.08 - 381.0] talk to each other you want AI using uh

[378.84 - 383.52] human readable natural language to talk

[381.0 - 386.28] to each other anyways that's a topic for

[383.52 - 388.5] another video

[386.28 - 391.02] um so let's move forward from those

[388.5 - 393.78] macro economic changes to technological

[391.02 - 396.539] breakthroughs if suddenly we have a

[393.78 - 398.75899999999996] hyperabundance of cognitive effort or

[396.539 - 402.0] cognitive labor what kind of Technology

[398.759 - 404.22] Solutions can we imagine being solved so

[402.0 - 406.62] first is high energy physics

[404.22 - 409.259] high energy physics the stuff that

[406.62 - 411.84000000000003] they're working on at the CERN at LHC

[409.259 - 414.539] this includes nuclear fusion it could

[411.84 - 416.52] include even anti-matter research who

[414.539 - 418.8] knows maybe time travel may be faster

[416.52 - 420.419] than light travel not really sure but at

[418.8 - 422.52000000000004] least the first problem that will likely

[420.419 - 425.46] be solved in high energy physics is

[422.52 - 427.74] probably going to be nuclear fusion it's

[425.46 - 429.68] really difficult to anticipate what

[427.74 - 432.539] solving nuclear fusion will do because

[429.68 - 434.819] nuclear fusion is a thousand times more

[432.539 - 437.46] powerful and efficient than any form of

[434.819 - 439.74] energy we have today so when you have a

[437.46 - 441.65999999999997] hyperabundance of energy suddenly a lot

[439.74 - 443.58] of other things become possible for

[441.66 - 446.40000000000003] instance you can then afford to

[443.58 - 449.099] desalinate as much water as you need you

[446.4 - 451.5] can then afford to run underground Farms

[449.099 - 453.479] you know that are completely unbounded

[451.5 - 455.4] from arable land there's all kinds of

[453.479 - 459.18] stuff that you can do once you unlock

[455.4 - 461.63899999999995] nuclear fusion the knock-on effects of

[459.18 - 464.039] solving nuclear fusion are impossible to

[461.639 - 465.96000000000004] say not only just like in the short term

[464.039 - 467.699] we can come up with a couple ideas but

[465.96 - 469.85999999999996] certainly in the long term solving

[467.699 - 471.06] nuclear fusion solves so many other

[469.86 - 473.22] problems

[471.06 - 474.78000000000003] um it solves recycling because then

[473.22 - 476.28000000000003] suddenly you can afford to just melt

[474.78 - 478.61999999999995] down any material no matter how

[476.28 - 480.65999999999997] expensive it is so you can reclaim all

[478.62 - 483.66] the lithium all the Cobalt all the

[480.66 - 485.88000000000005] nickel from everything platinum gold

[483.66 - 488.699] pretty much every mineral becomes

[485.88 - 491.4] accessible no matter how difficult it is

[488.699 - 494.34000000000003] to isolate because suddenly if you have

[491.4 - 497.58] many many gigajoules of energy available

[494.34 - 499.02] at all times for practically free it

[497.58 - 500.699] doesn't matter how much energy it costs

[499.02 - 502.85999999999996] to recycle a material that's just

[500.699 - 505.68] another example

[502.86 - 507.72] um another set of solved problems that

[505.68 - 510.24] you can expect with a hyperabundance of

[507.72 - 513.599] of cognitive labor is basically disease

[510.24 - 515.94] genetics and aging uh that you know the

[513.599 - 518.099] the human body our genetics our

[515.94 - 521.82] metabolism one of the most complex

[518.099 - 523.26] systems uh in existence uh there's more

[521.82 - 525.0600000000001] than a hundred thousand metabolic

[523.26 - 528.06] pathways that we know of in the human

[525.06 - 529.56] body alone and they all interact uh not

[528.06 - 533.04] only that they interact with your genes

[529.56 - 535.8] your epigenetics uh your microflora all

[533.04 - 538.4399999999999] kinds of stuff super complex system but

[535.8 - 541.5] if you have a hyper abundance of of

[538.44 - 543.36] intellect then you can create new tools

[541.5 - 546.18] you can create new processes you can

[543.36 - 548.22] manage vast amounts of information and

[546.18 - 551.16] so then we might end up we might end up

[548.22 - 553.019] curing all disease all aging and

[551.16 - 555.0] untangling all genetics

[553.019 - 558.0600000000001] within a relatively short period of time

[555.0 - 560.279] after achieving you know the singularity

[558.06 - 562.26] or AGI or however you want to call it

[560.279 - 565.019] excuse me and then finally Material

[562.26 - 566.1] Science so Material Science we're

[565.019 - 567.48] already seeing the beginning of this

[566.1 - 570.0] with Alpha fold

[567.48 - 571.5600000000001] and so basically imagine that you have

[570.0 - 575.1] Alpha fold which if you're not familiar

[571.56 - 576.899] with Alpha fold that is a way of using

[575.1 - 579.48] deep neural networks using Transformers

[576.899 - 581.94] to model protein folding

[579.48 - 583.44] which was an unsolved problem but now

[581.94 - 585.9590000000001] that it's a solved problem we can model

[583.44 - 587.4590000000001] any protein folding now take that to the

[585.959 - 589.6199999999999] next level what if not only you can

[587.459 - 592.0799999999999] model all protein folding you can

[589.62 - 594.0600000000001] measure or you can model all protein

[592.08 - 596.1600000000001] interactions all genetic interactions

[594.06 - 598.0189999999999] then take that one step further you can

[596.16 - 601.3199999999999] measure or model

[598.019 - 604.8] um uh nanoparticles carbon You can

[601.32 - 607.9200000000001] predict how to build very very Advanced

[604.8 - 609.12] Materials which could revolutionize for

[607.92 - 612.5999999999999] instance batteries and computer

[609.12 - 613.92] technology I predict that the Materials

[612.6 - 617.58] Science breakthroughs that will result

[613.92 - 620.2199999999999] from um from AI means that like

[617.58 - 622.44] basically in a in five to ten years your

[620.22 - 624.9590000000001] phone could be more powerful than all

[622.44 - 626.8800000000001] computers on Earth today

[624.959 - 628.1999999999999] um and I'm not really exaggerating when

[626.88 - 631.26] I say that because the amount of

[628.2 - 632.94] computational power just in the atoms of

[631.26 - 634.519] a phone like if you have a membrane or

[632.94 - 637.44] whatever or a three-dimensional wafer

[634.519 - 640.38] the amount of potential computational

[637.44 - 644.1600000000001] power in matter is

[640.38 - 646.68] inconceivable basically inconceivable

[644.16 - 648.6] um so anyways you know it would it would

[646.68 - 650.6999999999999] not surprise me if we move up a

[648.6 - 652.5] kardashev scale or two

[650.7 - 655.62] um post singularity

[652.5 - 658.44] now that being said there are still some

[655.62 - 661.5600000000001] unsolved problems uh that that pretty

[658.44 - 665.1600000000001] much no amount of intellectual labor

[661.56 - 666.8389999999999] um on Earth could solve so for instance

[665.16 - 668.399] uh some people in the comments have

[666.839 - 670.86] asked about you know the hard problem of

[668.399 - 673.14] Consciousness uh that may or may not be

[670.86 - 674.7] solvable by machines period that might

[673.14 - 676.38] be something that we humans have to

[674.7 - 678.779] figure out for ourselves

[676.38 - 681.779] uh which extends to fundamental

[678.779 - 683.82] questions of existence of cosmology some

[681.779 - 686.16] of these things are not necessarily a

[683.82 - 687.839] matter of you know mathematically

[686.16 - 689.76] proving it and measuring it in the lab

[687.839 - 692.0400000000001] some of these things are a matter of

[689.76 - 693.959] interpretation some of these things are

[692.04 - 696.0] a matter of subjective values such as

[693.959 - 697.26] the meaning of life so on and so forth

[696.0 - 699.54] now

[697.26 - 701.22] one thing that people imagine when we

[699.54 - 703.68] talk about transhumanism or

[701.22 - 706.5600000000001] post-humanism is that we will have some

[703.68 - 708.3] sort of transcendence event I personally

[706.56 - 710.579] don't think the singularity will result

[708.3 - 714.06] in some kind of transcendent event where

[710.579 - 715.92] we all become like Q from Star Trek or

[714.06 - 719.04] you know some final solution where we

[715.92 - 720.5999999999999] become beings of energy I also don't

[719.04 - 721.8] think that mind uploading is a good idea

[720.6 - 724.2] I know a lot of people think that that's

[721.8 - 726.24] great but like we don't understand why

[724.2 - 729.4200000000001] we are conscious you know and and

[726.24 - 730.86] basically I I predict that you know if

[729.42 - 732.7199999999999] you try and upload your mind you're just

[730.86 - 734.16] going to upload a copy of yourself and

[732.72 - 736.32] then your body will be dead and so

[734.16 - 737.9399999999999] subjectively you will have died but a

[736.32 - 739.38] copy of you will continue on forever so

[737.94 - 742.1400000000001] I don't think that mind uploading is a

[739.38 - 744.36] good idea which if that's the case then

[742.14 - 745.74] like we will forever be locked in our

[744.36 - 747.92] organic bodies

[745.74 - 750.839] even if there are digital copies of us

[747.92 - 752.279] frolicking out in cyberspace they're not

[750.839 - 753.899] going to be us and they're going to have

[752.279 - 756.06] an entirely different set of constraints

[753.899 - 757.62] because then if they're if if you become

[756.06 - 759.3599999999999] or a copy of you becomes a digital

[757.62 - 761.4590000000001] entity you suddenly don't have the same

[759.36 - 764.639] biological constraints and so we have

[761.459 - 767.3389999999999] this like Grand Divergence of digital

[764.639 - 768.48] post-humans and then us organic meat

[767.339 - 770.399] bags

[768.48 - 772.139] um that's that to me sounds like an

[770.399 - 774.12] unsolved problem that I don't think AI

[772.139 - 777.839] is going to fix for us

[774.12 - 779.76] all right uh moving on to social changes

[777.839 - 781.5] jobs and occupations

[779.76 - 784.2] so

[781.5 - 787.139] as machines get more intelligent the the

[784.2 - 789.3000000000001] tldr is that most jobs are going to

[787.139 - 790.86] become irrelevant

[789.3 - 792.5999999999999] um you know I've talked with people

[790.86 - 794.279] about this there's a lot of there's a

[792.6 - 795.899] lot of BS jobs out there that nobody

[794.279 - 798.779] really wants to do but you do it because

[795.899 - 801.48] you know you need to eat and you need to

[798.779 - 804.36] pay for your house and whatever

[801.48 - 805.98] and so what we're going to have to do is

[804.36 - 809.339] then recalibrate

[805.98 - 810.54] how we think of meaning and purpose and

[809.339 - 813.839] success

[810.54 - 815.279] and this includes uh maybe uh shifting

[813.839 - 817.8000000000001] and having a greater emphasis on

[815.279 - 819.48] Creative creativity exploration and

[817.8 - 821.76] self-improvement

[819.48 - 825.24] um and then one idea that that came from

[821.76 - 827.8199999999999] discussing this with chat GPT was that

[825.24 - 830.76] as a society we might instead instead of

[827.82 - 833.0400000000001] focusing on conformance to one standard

[830.76 - 835.5] of Education we might instead really

[833.04 - 838.5] focus on what makes everyone unique

[835.5 - 841.26] which was a really interesting new model

[838.5 - 843.48] of education so imagine that you go to

[841.26 - 845.7] school and instead of like everyone has

[843.48 - 848.339] the same classes you have a broad

[845.7 - 850.38] variety of projects and experiments and

[848.339 - 852.36] things to figure out what it is that one

[850.38 - 854.279] you really care about and two what

[852.36 - 857.1] really makes you stand out and so then

[854.279 - 859.279] everyone can have a very different uh

[857.1 - 861.5400000000001] focus on education

[859.279 - 863.76] my first year of school was at

[861.54 - 867.779] Montessori school and so I can imagine

[863.76 - 868.92] taking that to the next level anyways

[867.779 - 870.959] um I know that there's a lot of people

[868.92 - 873.8389999999999] that say oh well without a job we have

[870.959 - 876.42] no meaning that is your neoliberal

[873.839 - 878.4590000000001] programming speaking I and other people

[876.42 - 880.68] that have made a transition to a

[878.459 - 882.899] different uh different kind of

[880.68 - 884.76] occupation you know my my occupation is

[882.899 - 888.18] now YouTube and patreon which I find

[884.76 - 892.139] much more interesting and and and uh and

[888.18 - 894.3599999999999] rewarding is much closer to Lifestyles

[892.139 - 896.699] that have existed in the past so for

[894.36 - 899.279] instance in ancient Greece particularly

[896.699 - 900.779] in Sparta Spartan citizens were not

[899.279 - 903.779] allowed to have a job

[900.779 - 905.82] their job was to be soldiers to be

[903.779 - 908.76] Hunters to be politicians to participate

[905.82 - 911.88] in culture and Society not to be leather

[908.76 - 914.279] workers or anything else and so

[911.88 - 916.38] obviously ancient Sparta didn't

[914.279 - 918.54] ultimately didn't do so well ancient

[916.38 - 921.24] Athens they did much better very similar

[918.54 - 923.2199999999999] model with the with the the citizen

[921.24 - 925.62] class the Leisure Class

[923.22 - 928.1990000000001] um ditto for ancient Rome so humans have

[925.62 - 931.26] adapted to kind of these effectively

[928.199 - 932.88] opposed scarcity world before but

[931.26 - 936.06] instead of working on the backs of

[932.88 - 939.36] subjugated classes of people we will be

[936.06 - 942.18] we will all enter into a post-scarcity

[939.36 - 943.32] Leisure Class on the backs of AI that's

[942.18 - 944.04] kind of what I predict is going to

[943.32 - 945.6] happen

[944.04 - 947.459] because honestly most people want that

[945.6 - 950.16] anyways and if we have a collective

[947.459 - 951.779] willpower to want that who cares and I

[950.16 - 953.399] can hear some of you already complaining

[951.779 - 954.959] oh corporations are never going to allow

[953.399 - 958.44] that to happen I'm gonna get to that in

[954.959 - 960.0] just a second glad you asked okay so if

[958.44 - 962.5790000000001] we

[960.0 - 964.92] sudden if nobody's job really matters

[962.579 - 966.7199999999999] what do we do then right one of the

[964.92 - 968.04] conversations that I had at a Meetup was

[966.72 - 969.72] like well what if everyone just plays

[968.04 - 971.699] video games there's actually a reason

[969.72 - 974.76] that video games are so popular

[971.699 - 976.68] uh because video games uh can social uh

[974.76 - 979.139] can foster social connection right a lot

[976.68 - 981.12] of games are very very social today and

[979.139 - 983.04] they're also challenging which means

[981.12 - 985.32] that they uh give you a sense of

[983.04 - 987.779] competence a sense of Mastery and

[985.32 - 990.5400000000001] finally video games give you a lot more

[987.779 - 992.82] autonomy like you you can be anyone that

[990.54 - 994.62] you want in a video game world and those

[992.82 - 995.4590000000001] three things satisfy

[994.62 - 997.44] um the three pillars of

[995.459 - 999.54] self-determination Theory autonomy human

[997.44 - 1001.22] connection incompetence which is why so

[999.54 - 1004.16] many people play video games

[1001.22 - 1007.1600000000001] so if you look at sdt self-determination

[1004.16 - 1009.68] Theory and then you say okay well take

[1007.16 - 1012.86] away the need for a job and suddenly AI

[1009.68 - 1014.3] gives us all a lot more autonomy gives

[1012.86 - 1016.279] us an opportunity for more human

[1014.3 - 1017.3599999999999] connection the only remaining thing is

[1016.279 - 1019.399] challenge

[1017.36 - 1021.74] and what happens for a lot of people who

[1019.399 - 1025.28] retire or step away from conventional

[1021.74 - 1027.6200000000001] work is that we realize like oh wait I

[1025.28 - 1028.8799999999999] can challenge myself in new ways

[1027.62 - 1030.9799999999998] um all of you that watch my YouTube

[1028.88 - 1032.72] channel I don't actually need to do all

[1030.98 - 1035.24] the coding experiments that I do but I

[1032.72 - 1037.4] find it deeply satisfying to challenge

[1035.24 - 1038.9] myself to try and solve the problems out

[1037.4 - 1041.1200000000001] there and I'm not saying everyone is

[1038.9 - 1043.22] going to engage in this kind of problem

[1041.12 - 1046.1] solving some people are going to go to

[1043.22 - 1048.799] do martial arts or go climb mountains or

[1046.1 - 1050.12] whatever but we humans love love

[1048.799 - 1053.0] challenges

[1050.12 - 1055.82] we need to feel competent and we need to

[1053.0 - 1058.28] to have a sense of Mastery and um the

[1055.82 - 1061.1] Sam Altman interview he pointed out that

[1058.28 - 1063.1399999999999] yes AI has solved go and chess and other

[1061.1 - 1064.2199999999998] things but we still play chess we just

[1063.14 - 1065.8400000000001] don't play against computers because

[1064.22 - 1067.16] there's no point there's no sense of

[1065.84 - 1069.1999999999998] Mastery against something that you're

[1067.16 - 1071.72] never going to win against

[1069.2 - 1073.3400000000001] um so anyways the long-term effect of

[1071.72 - 1076.1000000000001] this is that we're probably going to see

[1073.34 - 1078.98] new social structures emerge

[1076.1 - 1082.039] um or maybe even older social structures

[1078.98 - 1083.1200000000001] re-emerge I particularly predict that

[1082.039 - 1085.76] we're going to see more

[1083.12 - 1088.6999999999998] multi-generational homes more kind of

[1085.76 - 1091.46] tribal or Village lifestyle things uh

[1088.7 - 1093.98] and re-emerge because suddenly it's like

[1091.46 - 1095.96] okay well there's you know here's a

[1093.98 - 1098.059] dozen people that I really like and none

[1095.96 - 1099.6200000000001] of us have a job so let's go form an Eco

[1098.059 - 1103.28] Village you know out in the countryside

[1099.62 - 1104.4799999999998] or maybe an urban co-living situation in

[1103.28 - 1106.82] the city

[1104.48 - 1107.78] um who knows uh just some speculation

[1106.82 - 1110.6] there

[1107.78 - 1112.16] okay so I promised that we would address

[1110.6 - 1114.1999999999998] the uh some of the elephants in the room

[1112.16 - 1117.02] so let's unpack all the risks and

[1114.2 - 1119.78] factors that will go into this Rosie

[1117.02 - 1120.9189999999999] post Singularity result that I have

[1119.78 - 1123.559] outlined

[1120.919 - 1126.6200000000001] so the first one is the development and

[1123.559 - 1129.02] control of AI obviously many of you are

[1126.62 - 1131.4799999999998] probably aware that there's been a uh

[1129.02 - 1133.22] the the letter circulating that's signed

[1131.48 - 1135.8600000000001] by a whole bunch of people including

[1133.22 - 1139.52] Elon Musk and Max tegmark all calling

[1135.86 - 1141.86] for a a moratorium on the advancement of

[1139.52 - 1143.6] AI for at least six months while we take

[1141.86 - 1147.1999999999998] a breath and reassess

[1143.6 - 1149.6599999999999] so it's an it is possible that if we

[1147.2 - 1152.72] continue at a Breakneck pace and things

[1149.66 - 1154.22] do it and people do it wrong then we're

[1152.72 - 1157.4] going to end up in some kind of

[1154.22 - 1160.1000000000001] dystopian or cataclysmic outcome so

[1157.4 - 1163.16] there's basically two primary uh failure

[1160.1 - 1166.039] modes for this one is we lose control of

[1163.16 - 1167.78] the AI and it decides to kill us all the

[1166.039 - 1170.059] other failure the other major failure

[1167.78 - 1172.039] mode is that we don't lose control of AI

[1170.059 - 1174.26] but the wrong people get the powerful Ai

[1172.039 - 1176.299] and they use it to kill everyone else or

[1174.26 - 1178.52] subjugate everyone else so those are the

[1176.299 - 1181.16] two primary failure modes that have to

[1178.52 - 1183.02] do with AI development and control

[1181.16 - 1184.22] um and this has been explored in a lot

[1183.02 - 1185.9] of fiction

[1184.22 - 1187.4] um and so like I'm kind of tired of it

[1185.9 - 1190.16] so I'm not going to really talk about it

[1187.4 - 1193.52] that much more but point being is that

[1190.16 - 1195.26] 99 of people don't really want an AI

[1193.52 - 1197.72] apocalypse some people seem to really

[1195.26 - 1199.94] wish for it but I think that's a sense

[1197.72 - 1201.44] of nihilism like leaking through some

[1199.94 - 1203.72] people think it's inevitable and there's

[1201.44 - 1205.28] a sort of fatalism about it and that

[1203.72 - 1206.539] again you know I empathize with people

[1205.28 - 1208.7] like that

[1206.539 - 1210.2] um I Echo Sam Altman's sentiment that

[1208.7 - 1211.52] like yeah there are a lot of people

[1210.2 - 1212.9] afraid and I'm not going to tell them

[1211.52 - 1215.12] that they're wrong or that they're

[1212.9 - 1217.8200000000002] stupid or that it's magical thinking

[1215.12 - 1219.7399999999998] like we are playing with fire

[1217.82 - 1222.5] um I just happen to be very sanguine

[1219.74 - 1224.96] about it because I feel like one all the

[1222.5 - 1227.24] problems that exist are solvable and two

[1224.96 - 1228.799] I think that they are solvable in the

[1227.24 - 1232.1] very near term

[1228.799 - 1233.9] okay another big risk is distribution of

[1232.1 - 1235.9399999999998] benefits this is one of the biggest

[1233.9 - 1237.8600000000001] things that people are worried about

[1235.94 - 1240.0800000000002] which is okay

[1237.86 - 1242.24] do you the like the one of the most

[1240.08 - 1243.74] common pushbacks is like do you honestly

[1242.24 - 1246.679] think that corporations are going to

[1243.74 - 1248.539] allow everyone to live a luxurious

[1246.679 - 1251.1200000000001] lifestyle or that the rich and Powerful

[1248.539 - 1252.919] are going to allow everyone else to live

[1251.12 - 1254.539] like they do

[1252.919 - 1256.64] well first I don't know that they'll

[1254.539 - 1259.22] have that much of a choice in it but two

[1256.64 - 1262.16] I think the fact that the the masses

[1259.22 - 1264.5] like you and I the proletariat we don't

[1262.16 - 1267.2] want to live in a cyberpunk Hell right

[1264.5 - 1269.66] and we have seen what happens repeatedly

[1267.2 - 1271.46] through history as people get hungrier

[1269.66 - 1273.26] and more desperate the most recent

[1271.46 - 1276.02] incident was the Arab Spring in which

[1273.26 - 1278.96] case much of the Middle East the Arab

[1276.02 - 1282.26] world rose up and the primary driving

[1278.96 - 1283.82] Factor was economic conditions

[1282.26 - 1286.34] um and then of course you go back even

[1283.82 - 1287.84] further the French Revolution this kind

[1286.34 - 1289.9399999999998] of thing has happened time and time

[1287.84 - 1291.799] again so I'm not too particularly

[1289.94 - 1293.72] worried about that because push comes to

[1291.799 - 1297.3799999999999] shove people are going to stand up and

[1293.72 - 1299.78] and and redistribute forcefully now

[1297.38 - 1301.2800000000002] I'm not advocating for you know Civil

[1299.78 - 1304.1] War or anything I don't even think it's

[1301.28 - 1306.2] going to come to that because you know I

[1304.1 - 1309.32] follow Davos and World economic forum

[1306.2 - 1312.38] and U.N and and all the you know Halls

[1309.32 - 1315.26] of power IMF the World Bank

[1312.38 - 1316.46] um the halls of power really are paying

[1315.26 - 1319.1589999999999] attention to this and I think that

[1316.46 - 1322.64] they're preparing for it honestly so for

[1319.159 - 1324.8600000000001] instance I suspect that the uh the the

[1322.64 - 1327.26] stimulus checks that America did during

[1324.86 - 1329.36] the pandemic I think that that was a

[1327.26 - 1331.46] pilot program to demonstrate that

[1329.36 - 1333.5] redistribution works that it is fast

[1331.46 - 1334.94] efficient and fair because they what

[1333.5 - 1338.12] they did was they did the stimulus

[1334.94 - 1341.1200000000001] checks alongside the the um the paycheck

[1338.12 - 1342.5] uh Protection Program the PPP loans and

[1341.12 - 1345.559] they basically did a side-by-side test

[1342.5 - 1347.78] showing look the PPP loans are expensive

[1345.559 - 1349.34] and Rife with corruption and the

[1347.78 - 1351.62] stimulus checks went directly to people

[1349.34 - 1353.84] who needed it and it all got spent by

[1351.62 - 1356.36] individuals who needed it so I kind of

[1353.84 - 1360.5] think that the the stimulus checks were

[1356.36 - 1362.84] a a pilot program or a prototype for Ubi

[1360.5 - 1364.94] and when you look at the landscape right

[1362.84 - 1368.12] now where there's been over 300 000 Tech

[1364.94 - 1369.8600000000001] layoffs and more other other kinds of

[1368.12 - 1372.5] people are already starting to get laid

[1369.86 - 1374.9599999999998] off and notified of layoffs due to

[1372.5 - 1377.12] Technologies like chat GPT

[1374.96 - 1378.74] um my fiance who's a writer and is in a

[1377.12 - 1380.299] lot of writing discords there are

[1378.74 - 1384.02] copywriters out there who are already

[1380.299 - 1386.84] getting laid off and losing work to AI

[1384.02 - 1388.76] um so like the AI layoffs are coming so

[1386.84 - 1391.34] I think that we're also going to see a

[1388.76 - 1393.26] lot of stimulus checks coming and it's

[1391.34 - 1395.0] just a matter of okay are these stimulus

[1393.26 - 1396.3799999999999] checks permanent and I think that they

[1395.0 - 1398.84] will be

[1396.38 - 1401.48] the regulatory environment so this is

[1398.84 - 1403.82] where that letter that just came out

[1401.48 - 1405.6200000000001] um is is asking for regulation Sam

[1403.82 - 1408.1399999999999] Allman has asked for regulation Elon

[1405.62 - 1409.76] Musk has asked for regulation all kinds

[1408.14 - 1413.2990000000002] of people are asking for more regulation

[1409.76 - 1415.22] now the big problem here though is one

[1413.299 - 1416.84] there's no agreement on how to regulate

[1415.22 - 1420.32] these things and in the conversations

[1416.84 - 1421.82] I've had at meetups the question rapidly

[1420.32 - 1424.1789999999999] comes up how do you even enforce it

[1421.82 - 1426.32] right if all these models are getting

[1424.179 - 1428.8400000000001] faster and more efficient and you can

[1426.32 - 1430.28] run them on laptops now you can't put

[1428.84 - 1432.02] that Genie back in the model so does

[1430.28 - 1434.72] regulation even matter

[1432.02 - 1436.1589999999999] or if it does how

[1434.72 - 1438.559] so

[1436.159 - 1440.1200000000001] the big concern here with the regulatory

[1438.559 - 1442.28] environment at the federal and

[1440.12 - 1444.5] international level is existing power

[1442.28 - 1446.1789999999999] structures and the status quo so the

[1444.5 - 1447.679] wealthy and Powerful are going to want

[1446.179 - 1449.9] to remain the wealthiest and most

[1447.679 - 1452.0590000000002] powerful on the planet that's just how

[1449.9 - 1454.039] it is and how it has always been there

[1452.059 - 1456.559] have been reset events like you know the

[1454.039 - 1457.94] French Revolution American Revolution so

[1456.559 - 1459.86] on and so forth there have been reset

[1457.94 - 1462.3200000000002] events in history but they're generally

[1459.86 - 1464.1789999999999] violent and we want to avoid that so do

[1462.32 - 1466.8799999999999] the so do the powers that be also want

[1464.179 - 1468.919] to avoid that but the biggest problem in

[1466.88 - 1471.38] these conversations that I've had is

[1468.919 - 1473.48] that things are advancing so fast and

[1471.38 - 1474.5] the gerontocracy which is ruled by the

[1473.48 - 1477.38] elderly

[1474.5 - 1479.419] old folks generally don't get AI they

[1477.38 - 1481.5800000000002] don't understand how much is changing

[1479.419 - 1483.7990000000002] and why and what its impact is going to

[1481.58 - 1487.1] be and that honestly could be one of the

[1483.799 - 1489.32] biggest risks is you know us younger

[1487.1 - 1490.8799999999999] people we get it we see it coming

[1489.32 - 1492.6789999999999] even some of the people at the meetups

[1490.88 - 1496.94] that I talk to their children are

[1492.679 - 1498.3200000000002] already acclimating to a an AI world and

[1496.94 - 1499.159] they're going to trust the AI more than

[1498.32 - 1501.26] people because it's like well

[1499.159 - 1502.8200000000002] politicians lie and yeah chat GPT might

[1501.26 - 1504.14] get it wrong sometimes but it's not

[1502.82 - 1506.059] going to lie to you but not like a

[1504.14 - 1508.5200000000002] politician will so we're in we're in for

[1506.059 - 1510.98] some very interesting uh advancements in

[1508.52 - 1514.039] the regulatory front

[1510.98 - 1516.2] public perception and adaptation so

[1514.039 - 1518.9] there's a lot of fun fear uncertainty

[1516.2 - 1520.159] and doubt uh denialism doomerism and

[1518.9 - 1522.679] then also lots of people saying oh

[1520.159 - 1525.2] that's still decades away it's not it's

[1522.679 - 1528.0800000000002] months and years away not decades

[1525.2 - 1529.64] um so the another big problem is a lot

[1528.08 - 1531.1999999999998] of this uncertainty a lot of this

[1529.64 - 1532.76] denialism

[1531.2 - 1535.039] um some of the there's various aspects

[1532.76 - 1536.72] of the of the denialism for instance

[1535.039 - 1538.34] some people think oh well ai's never

[1536.72 - 1539.48] going to be as smart as us or it's never

[1538.34 - 1541.1589999999999] going to be smarter than us and it's

[1539.48 - 1543.98] like I kind of think that it's already

[1541.159 - 1545.1200000000001] smarter than those people it just lacks

[1543.98 - 1548.0] autonomy

[1545.12 - 1549.279] um but you know that's my opinion and I

[1548.0 - 1551.779] know some of you disagree with it

[1549.279 - 1553.279] anyways this is another big risk is

[1551.779 - 1555.26] because a lot of people are sticking

[1553.279 - 1556.82] their head in the sand and then there's

[1555.26 - 1558.2] also comments around the world like

[1556.82 - 1560.48] someone was saying that I think in

[1558.2 - 1562.279] France like they don't even like people

[1560.48 - 1564.559] aren't even talking about it right it's

[1562.279 - 1566.72] so like all of this is happening so

[1564.559 - 1569.12] quickly and most people aren't even

[1566.72 - 1570.74] aware of it of course chat GPT made the

[1569.12 - 1572.8999999999999] news but then people just kind of you

[1570.74 - 1575.059] know World by and large collectively

[1572.9 - 1577.279] Shrugged without understanding how fast

[1575.059 - 1579.98] this is ramping up so public perception

[1577.279 - 1581.96] and and acclimating to this could also

[1579.98 - 1585.02] be a big barrier

[1581.96 - 1588.44] uh Global uh Global cooperation and

[1585.02 - 1591.44] collaboration the big thing here is

[1588.44 - 1593.3600000000001] um what what I call trauma politics so

[1591.44 - 1596.6000000000001] basically you look at people like Putin

[1593.36 - 1599.9599999999998] and Xi Jinping both of whom suffered a

[1596.6 - 1602.8999999999999] tremendous amount of trauma at the hands

[1599.96 - 1605.059] of their dystopian governments

[1602.9 - 1608.3600000000001] um and they basically are seeking power

[1605.059 - 1610.8799999999999] for the purpose of self-soothing

[1608.36 - 1613.76] um that's pretty much all there is to it

[1610.88 - 1615.2] but when when people who have a

[1613.76 - 1617.96] tremendous amount of trauma come into

[1615.2 - 1620.24] power they tend to have a more

[1617.96 - 1623.1200000000001] nihilistic worldview which with which

[1620.24 - 1625.279] then results in things like genocide

[1623.12 - 1627.4399999999998] mass incarceration surveillance States

[1625.279 - 1629.179] because they want control they want as

[1627.44 - 1631.76] much control and power as they can get

[1629.179 - 1632.3600000000001] and it's never enough

[1631.76 - 1636.08] um

[1632.36 - 1638.9599999999998] and so this nihilism also creates a

[1636.08 - 1641.6] self-fulfilling prophecy because they

[1638.96 - 1643.3400000000001] project their pain onto the world which

[1641.6 - 1644.36] causes more trauma look at the war in

[1643.34 - 1645.6789999999999] Ukraine

[1644.36 - 1647.779] um look at China's treatment of the

[1645.679 - 1650.24] uyghurs and then that creates a

[1647.779 - 1652.52] self-perpetuating loop of more trauma

[1650.24 - 1656.0] intergenerational trauma and so forth so

[1652.52 - 1658.039] so on and so forth and so in my opinion

[1656.0 - 1661.1] um this unaddressed uh basically

[1658.039 - 1662.6] intergenerational PTSD or nihilism is

[1661.1 - 1664.3999999999999] the greatest threat to humanity because

[1662.6 - 1666.74] these are the kinds of people who will

[1664.4 - 1668.1200000000001] look at these things Ai and say oh

[1666.74 - 1669.679] that's the perfect weapon for control

[1668.12 - 1672.4399999999998] that's the perfect weapon for

[1669.679 - 1674.72] subjugation whereas healthy individuals

[1672.44 - 1676.88] look at Ai and say maybe we don't do

[1674.72 - 1680.1200000000001] that

[1676.88 - 1681.74] um Singularity facts so uh there is a

[1680.12 - 1683.4799999999998] lot of kind of gotcha questions that

[1681.74 - 1686.84] come up I tried to capture some of the

[1683.48 - 1688.64] best ones what will happen to money post

[1686.84 - 1689.72] Singularity

[1688.64 - 1691.88] um some people think like oh

[1689.72 - 1694.22] cryptocurrency is the future or maybe we

[1691.88 - 1696.14] get do away with money altogether

[1694.22 - 1700.279] well I've got some good news and some

[1696.14 - 1702.2] bad news the uh the good news is that uh

[1700.279 - 1704.24] it is entirely possible that money will

[1702.2 - 1706.4] change monetary systems will change and

[1704.24 - 1708.919] financial policies will change however

[1706.4 - 1711.679] the concept of currency the concept of

[1708.919 - 1713.0] money is too useful and too helpful

[1711.679 - 1715.8200000000002] because

[1713.0 - 1717.98] um it is an abstract uh reserve of value

[1715.82 - 1718.9399999999998] and it is also a really good medium of

[1717.98 - 1721.7] Exchange

[1718.94 - 1723.74] and so you know whether that means that

[1721.7 - 1726.8600000000001] Bitcoin or other cryptocurrencies are

[1723.74 - 1728.659] gonna are gonna you know replace uh fiat

[1726.86 - 1730.76] currency I'm not really going to say one

[1728.659 - 1733.1000000000001] way or another but basically currency is

[1730.76 - 1734.6] here to stay in some form

[1733.1 - 1737.0] um personally I think that there's too

[1734.6 - 1739.4599999999998] many problems with cryptocurrency

[1737.0 - 1741.38] um namely that it is uh subject to

[1739.46 - 1742.279] manipulation because its value can

[1741.38 - 1744.88] change

[1742.279 - 1748.279] a lot right like the the the the the

[1744.88 - 1750.5590000000002] wild swings of value of of Bitcoin and

[1748.279 - 1753.08] stuff basically proves that it is not a

[1750.559 - 1755.059] stable reserve of value and you know

[1753.08 - 1757.1] people have lost fortunes on it people

[1755.059 - 1759.32] have made fortunes on it too usually

[1757.1 - 1761.539] people that with uh

[1759.32 - 1764.059] um not the best intentions oh I don't

[1761.539 - 1767.96] say usually but sometimes basically uh

[1764.059 - 1769.399] organized crime um loves cryptocurrency

[1767.96 - 1772.52] uh what will happen to the human

[1769.399 - 1775.34] population now this one really kind of

[1772.52 - 1777.1399999999999] uh uh is is interesting because there's

[1775.34 - 1778.6399999999999] a lot of debate over what is the actual

[1777.14 - 1781.76] carrying capacity of the planet some

[1778.64 - 1784.8200000000002] people say oh it's easily 50 billion

[1781.76 - 1786.62] um and it's no it's not simply enough no

[1784.82 - 1789.32] the carrying capacity of the planet is

[1786.62 - 1791.7199999999998] nowhere near 50 billion there is

[1789.32 - 1794.539] technically enough room physical room

[1791.72 - 1796.52] 450 billion humans but when you look at

[1794.539 - 1799.1589999999999] the the the constraints of

[1796.52 - 1801.26] thermodynamics hydrological Cycles the

[1799.159 - 1804.679] amount of arable land no

[1801.26 - 1807.3799999999999] now it is possible that the singularity

[1804.679 - 1809.48] with you know its results in nuclear

[1807.38 - 1810.679] fusion and stuff you could probably tip

[1809.48 - 1812.72] that a little bit further right

[1810.679 - 1814.76] especially if you can

[1812.72 - 1818.1200000000001] um synthesize more arable land or grow

[1814.76 - 1819.74] food underground or desalinate water you

[1818.12 - 1822.08] could probably boost the carrying

[1819.74 - 1824.0] capacity of the planet quite a bit 50

[1822.08 - 1826.22] billion still seems way out there for me

[1824.0 - 1827.779] but the biggest thing is going to be is

[1826.22 - 1830.779] not going to be those things like you

[1827.779 - 1832.82] know okay we we overcome those those

[1830.779 - 1834.799] energetic constraints it's still going

[1832.82 - 1837.62] to come down to like

[1834.799 - 1840.2] uh mostly mostly management right

[1837.62 - 1842.799] sustainable management of of the

[1840.2 - 1846.2] population because the thing is you know

[1842.799 - 1848.48] you you if if Logistics breaks down

[1846.2 - 1850.7] today we all starve pretty quickly right

[1848.48 - 1854.0] because we don't have locally sourced

[1850.7 - 1856.7] food our food and water you know

[1854.0 - 1858.799] requires a very stable

[1856.7 - 1860.96] um infrastructure in order to provide

[1858.799 - 1862.399] that and that only gets worse when you

[1860.96 - 1865.1000000000001] have like 50 billion people on the

[1862.399 - 1867.559] planet so you know sustainable and

[1865.1 - 1870.559] responsible management of necessary

[1867.559 - 1873.5] resources primarily food and water are

[1870.559 - 1875.84] going to be the key to what happens with

[1873.5 - 1877.76] the human population now in some of the

[1875.84 - 1879.86] discussions that I've had there's a few

[1877.76 - 1882.08] confounding factors here one thing that

[1879.86 - 1883.9399999999998] isn't mentioned on this slide is what

[1882.08 - 1886.8799999999999] happens if we solve aging

[1883.94 - 1890.0] because what happens with populations is

[1886.88 - 1893.0] as they become more gender equal women

[1890.0 - 1894.679] choose to have fewer children and so

[1893.0 - 1896.96] what if people are living longer but

[1894.679 - 1898.159] having fewer children I kind of predict

[1896.96 - 1899.8400000000001] that the population is going to

[1898.159 - 1902.96] stabilize there's always going to be

[1899.84 - 1905.6589999999999] some people who want children but at the

[1902.96 - 1907.52] same time right like if you if you don't

[1905.659 - 1909.14] if you don't actually really deeply want

[1907.52 - 1911.179] children you're probably not going to

[1909.14 - 1912.8600000000001] have them and then in a post-scarcity

[1911.179 - 1915.44] life like

[1912.86 - 1917.24] maybe you choose never to have children

[1915.44 - 1919.1000000000001] and again some people will choose to

[1917.24 - 1920.659] have children and even if you solve

[1919.1 - 1922.1589999999999] aging people will still die they're

[1920.659 - 1923.96] still going to be accidents right

[1922.159 - 1926.24] they're still going to be

[1923.96 - 1928.3400000000001] um maybe a few a handful of Unsolved

[1926.24 - 1930.86] medical issues but primarily you're

[1928.34 - 1933.62] going to see accidents and also one of

[1930.86 - 1935.24] the conversations that came up was okay

[1933.62 - 1937.039] well if you can if you can

[1935.24 - 1940.34] hypothetically live forever do you want

[1937.039 - 1941.6] to and the I the many people suspect

[1940.34 - 1943.039] that you won't actually want to live

[1941.6 - 1944.7199999999998] forever you might choose to live for a

[1943.039 - 1946.46] few hundred years but then you might get

[1944.72 - 1948.8600000000001] tired of life and then you know quit

[1946.46 - 1950.8400000000001] taking the life extending medicine and

[1948.86 - 1951.86] allow yourself to die naturally who

[1950.84 - 1954.1399999999999] knows

[1951.86 - 1957.4399999999998] um but personally I kind of predict a a

[1954.14 - 1960.5] a a population stabilization

[1957.44 - 1963.44] food so food has been a big thing

[1960.5 - 1965.419] um so on top of you know vertical

[1963.44 - 1967.279] farming or underground farming powered

[1965.419 - 1970.159] by nuclear fusion okay great we can eat

[1967.279 - 1973.0] whatever we want wherever we want I also

[1970.159 - 1976.0390000000002] suspect that biotechnology is going to

[1973.0 - 1978.98] really change our diet and what I mean

[1976.039 - 1982.94] by that is synthetic Foods engineered

[1978.98 - 1986.179] foods and even hyper personalized diets

[1982.94 - 1987.98] so for instance by and large you might

[1986.179 - 1990.2] believe that Dairy is bad for you

[1987.98 - 1993.74] because it's you know got you know anal

[1990.2 - 1996.2] you know saturated fat in it but when I

[1993.74 - 1998.48] started uh when I added more Dairy to my

[1996.2 - 2000.76] diet all my numbers got better because

[1998.48 - 2002.8600000000001] it's just in my jeans it's in whatever

[2000.76 - 2005.5] and so but I had to figure that out

[2002.86 - 2007.299] through trial and error Dairy raises

[2005.5 - 2010.36] some people's cholesterol in my case it

[2007.299 - 2012.6399999999999] lowered it so the combination of

[2010.36 - 2014.26] engineered Foods better bioinformatics

[2012.64 - 2017.3200000000002] and biotech

[2014.26 - 2018.82] um and and things like mobile Farms oh

[2017.32 - 2021.1589999999999] there's actually I actually saw an ad

[2018.82 - 2023.6789999999999] for it the the first like portable Farms

[2021.159 - 2026.44] are actually the the container shipping

[2023.679 - 2028.299] container Farms are are coming so that

[2026.44 - 2030.519] only that only ramps up and gets better

[2028.299 - 2031.84] over time so that means you go to the

[2030.519 - 2033.94] grocery store

[2031.84 - 2035.62] and everything that you could possibly

[2033.94 - 2038.3200000000002] want is there and it's fresh and it's

[2035.62 - 2039.76] local so that so you know some people

[2038.32 - 2040.899] are worried like oh well they're going

[2039.76 - 2042.7] to take our steaks they're going to take

[2040.899 - 2044.3799999999999] our Burgers I don't think so I think

[2042.7 - 2045.82] you're actually going to have much more

[2044.38 - 2047.6200000000001] options and they're going to be

[2045.82 - 2049.599] healthier options uh in a post

[2047.62 - 2052.179] Singularity world

[2049.599 - 2055.0] uh War so I did mention trauma politics

[2052.179 - 2057.46] and and geopolitics earlier

[2055.0 - 2060.879] obviously the biggest the absolute

[2057.46 - 2064.2400000000002] biggest risk here is an AI arms race

[2060.879 - 2066.7599999999998] um even Nations liberal democracies that

[2064.24 - 2069.52] are not run by deeply traumatized

[2066.76 - 2072.7000000000003] tyrants are still going to be engaged in

[2069.52 - 2074.08] some kind of AI arms race uh which is an

[2072.7 - 2075.7] unfortunate reality I'm not saying that

[2074.08 - 2077.02] that's a good thing I'm not passing

[2075.7 - 2079.06] moral judgment on it it's just an

[2077.02 - 2081.399] observation every time there's new

[2079.06 - 2084.639] technology it is integrated into the

[2081.399 - 2085.5989999999997] military apparatus I don't all I also

[2084.639 - 2087.46] don't think that we're going to end up

[2085.599 - 2089.8] with a one world government

[2087.46 - 2091.78] at least not anytime soon

[2089.8 - 2093.7000000000003] um and there's numerous reasons for this

[2091.78 - 2096.46] not the least of which is language

[2093.7 - 2098.02] barriers cultural differences

[2096.46 - 2101.26] um uh

[2098.02 - 2102.58] past grievances between cultures

[2101.26 - 2105.76] um you know it could take many many

[2102.58 - 2108.4] generations to heal those um those

[2105.76 - 2111.7000000000003] Intercultural wounds before people even

[2108.4 - 2113.6800000000003] want to uh collaborate you look at the

[2111.7 - 2116.14] the animosity between like China and

[2113.68 - 2119.3199999999997] Japan between Israel and Palestine

[2116.14 - 2121.7799999999997] between Iran and and a bunch of other

[2119.32 - 2124.3] nations and so on and so forth it takes

[2121.78 - 2126.099] a lot of work to heal those wounds

[2124.3 - 2128.92] and there's a lot of resistance to

[2126.099 - 2131.44] Healing those wounds and those wounds

[2128.92 - 2134.619] could continue to Fester what I'm hoping

[2131.44 - 2137.2000000000003] is that AI actually helps us break the

[2134.619 - 2139.0] cycle of intergenerational trauma and so

[2137.2 - 2141.339] then within maybe you know two or three

[2139.0 - 2144.28] generations we're ready for a more

[2141.339 - 2145.54] peaceful Global community and again I

[2144.28 - 2147.28] still don't think that a global

[2145.54 - 2148.74] government is going to happen

[2147.28 - 2150.88] um just because like

[2148.74 - 2152.3799999999997] geographically speaking like it kind of

[2150.88 - 2154.2400000000002] makes sense to have the uh the the

[2152.38 - 2156.099] Nations the nation states and then the

[2154.24 - 2157.8999999999996] union model that makes the most sense

[2156.099 - 2159.46] right now like you know France is still

[2157.9 - 2160.96] France Great Britain is still Great

[2159.46 - 2162.16] Britain but they're part of the European

[2160.96 - 2164.8] Union right

[2162.16 - 2166.72] and over time I do suspect that those

[2164.8 - 2169.0600000000004] Continental sized unions will get

[2166.72 - 2171.0989999999997] stronger but not that they'll replace

[2169.06 - 2173.14] the local governments just like you know

[2171.099 - 2175.42] we have municipals we have local city

[2173.14 - 2176.44] governments we have County we have state

[2175.42 - 2177.7000000000003] governments and we have Federal

[2176.44 - 2180.099] governments I think that we're just

[2177.7 - 2182.5] going to add a few tiers on top of that

[2180.099 - 2184.78] and eventually we will end up with a

[2182.5 - 2185.98] global governance but again I think that

[2184.78 - 2187.96] it's probably at least two or three

[2185.98 - 2190.72] generations away minimum

[2187.96 - 2193.06] and then finally corporations so I did

[2190.72 - 2195.4599999999996] promise that I would address this

[2193.06 - 2197.74] um so some people and this includes

[2195.46 - 2200.079] myself I hope that corporations as we

[2197.74 - 2202.7799999999997] know them go away because corporations

[2200.079 - 2204.94] are intrinsically amoral and I don't

[2202.78 - 2208.0600000000004] mean immoral amoral and corporations

[2204.94 - 2210.28] their morality is only beholden to the

[2208.06 - 2211.9] investor right uh to the shareholders

[2210.28 - 2213.3390000000004] and the shareholders just want more

[2211.9 - 2215.6800000000003] value

[2213.339 - 2218.02] um whatever it costs right and

[2215.68 - 2219.7599999999998] corporations will always explore every

[2218.02 - 2222.22] little nook and cranny of what they can

[2219.76 - 2225.4] legally get away with

[2222.22 - 2227.52] um and that often is results in bad

[2225.4 - 2230.2000000000003] things such as mistreatment of people

[2227.52 - 2232.18] environmental abuse and so on

[2230.2 - 2234.2799999999997] so because corporations are

[2232.18 - 2236.56] intrinsically amoral I hope that they go

[2234.28 - 2239.079] away but I don't think that they will

[2236.56 - 2240.7599999999998] um I tried I tried to figure out how the

[2239.079 - 2242.079] singularity could result in this but I I

[2240.76 - 2243.7000000000003] the more I explore it the more I

[2242.079 - 2245.32] realized like no

[2243.7 - 2248.02] basically what's going to happen is that

[2245.32 - 2249.6400000000003] AI is going to allow corporations to

[2248.02 - 2251.68] produce more with less

[2249.64 - 2253.839] so productivity will continue to go up

[2251.68 - 2256.2999999999997] while head count goes down and I talked

[2253.839 - 2258.52] about this in my AI job apocalypse video

[2256.3 - 2261.1600000000003] a couple months ago basically what's

[2258.52 - 2263.44] going to happen is that uh you're going

[2261.16 - 2265.42] to see corporations replace as many of

[2263.44 - 2266.92] their workers as they can and so then

[2265.42 - 2268.96] you have

[2266.92 - 2271.359] um the the owner the ownership class

[2268.96 - 2275.56] whether it's shareholders CEOs whoever

[2271.359 - 2277.96] is going to have basically unmitigated

[2275.56 - 2279.64] um stock price growth because suddenly

[2277.96 - 2281.14] the greatest constraint and the most

[2279.64 - 2284.0789999999997] expensive aspect of running a

[2281.14 - 2286.9] corporation human labor is no longer a

[2284.079 - 2289.3590000000004] factor so we I think that we are at risk

[2286.9 - 2292.599] of seeing like the mega Corp things that

[2289.359 - 2294.339] you see in like uh dystopian sci-fi I

[2292.599 - 2296.619] think that we probably are at risk of

[2294.339 - 2298.96] seeing you know multi-trillion dollar

[2296.619 - 2300.76] quadrillion dollar companies out there

[2298.96 - 2303.16] that have almost no employees that are

[2300.76 - 2304.3] all entirely run by shareholders and

[2303.16 - 2306.22] then AI

[2304.3 - 2308.26] uh so that is

[2306.22 - 2310.24] an interesting thing now as to whether

[2308.26 - 2312.3390000000004] or not they will allow the rest of us to

[2310.24 - 2315.16] live in certain ways I kind of think

[2312.339 - 2317.14] that they don't care right because as as

[2315.16 - 2319.7799999999997] obscenely wealthy as corporations are

[2317.14 - 2322.24] going to be like

[2319.78 - 2324.099] there's just it doesn't make sense for

[2322.24 - 2326.02] them to expend any energy depriving

[2324.099 - 2328.1800000000003] everyone else

[2326.02 - 2331.24] um and so like let's just imagine that

[2328.18 - 2333.8799999999997] like Elon Musk takes SpaceX and uses it

[2331.24 - 2337.1189999999997] to start harvesting asteroids and SpaceX

[2333.88 - 2339.6400000000003] becomes a 20 trillion dollar Company by

[2337.119 - 2343.06] harvesting iridium and Cobalt and

[2339.64 - 2345.2799999999997] platinum from asteroids great is is Elon

[2343.06 - 2346.2999999999997] Musk going to personally say actually I

[2345.28 - 2349.2400000000002] don't think that I think that everyone

[2346.3 - 2350.98] should live in slums and favelas around

[2349.24 - 2352.8999999999996] the world no he he's not going to care

[2350.98 - 2354.82] he doesn't give a crap how everyone else

[2352.9 - 2356.8] lives as long as he's a trillionaire

[2354.82 - 2358.96] right and so when I think it through it

[2356.8 - 2361.3590000000004] that way it's like it would take a lot

[2358.96 - 2364.599] of deliberate effort on corporate

[2361.359 - 2367.0] um to on on behalf of Corporations to

[2364.599 - 2368.26] deliberately deprive the rest of us of a

[2367.0 - 2369.94] better life so I don't think that's

[2368.26 - 2371.619] going to happen certainly it's something

[2369.94 - 2374.02] to be aware of because again

[2371.619 - 2377.56] corporations are intrinsically amoral

[2374.02 - 2379.3] which is one of the biggest risks to our

[2377.56 - 2382.2999999999997] standard of living in the future

[2379.3 - 2383.8] okay that's that thanks for watching um

[2382.3 - 2386.6800000000003] I hope you thought found this video

[2383.8 - 2390.4] enlightening and thought-provoking

[2386.68 - 2391.7799999999997] um yeah I know that uh there will

[2390.4 - 2393.7000000000003] probably be some disagreements in the

[2391.78 - 2397.2000000000003] comments um keep it civil or you get

[2393.7 - 2397.2] banned thanks bye