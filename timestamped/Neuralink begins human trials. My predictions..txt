[0.0 - 4.68] Elon Musk and neurolink have announced

[2.7 - 7.0200000000000005] that they are about to begin human

[4.68 - 8.76] trials now there have been many people

[7.02 - 10.139] who have said this was never going to

[8.76 - 12.719] happen and there's lots of technical

[10.139 - 14.34] problems and ethical issues so on and so

[12.719 - 16.74] forth and I'm not going to dive into

[14.34 - 19.14] those aspects what I want to talk about

[16.74 - 22.02] is the science and technology and in

[19.14 - 24.119] particular the long-term outcomes I

[22.02 - 25.619] often make predictions and that uh you

[24.119 - 28.32] know brings a lot of people and you guys

[25.619 - 29.939] like hearing what I have to say I'm not

[28.32 - 31.619] always right that is one of the biggest

[29.939 - 34.92] things predictions are exactly that

[31.619 - 36.54] predictions but this is actually kind of

[34.92 - 38.88] near and dear to me because I grew up

[36.54 - 41.7] watching Ghost in the Shell and other

[38.88 - 45.42] places like that that have cybernetics

[41.7 - 47.760000000000005] I'm a huge fan of cyberpunk 2077 and I

[45.42 - 50.879000000000005] even include this kind of brain computer

[47.76 - 52.739] interface in my upcoming novel so this

[50.879 - 54.18] is near and dear to my heart so this is

[52.739 - 55.62] actually kind of like a personal like

[54.18 - 58.079] passion project

[55.62 - 59.099999999999994] all right before we dive in I got to

[58.079 - 60.899] give you a little bit of background

[59.1 - 63.96] about neurolink that you may or may not

[60.899 - 65.88] know so first obviously like I mentioned

[63.96 - 69.659] the latest news is that they're about to

[65.88 - 73.02] start human trials uh the uh animal

[69.659 - 75.78] tests have allegedly killed 1500 uh test

[73.02 - 77.64] subjects again I'm not really here to

[75.78 - 80.46000000000001] comment on the morality and ethics of it

[77.64 - 83.34] I personally think that animal testing

[80.46 - 85.259] is likely cruel

[83.34 - 88.08] um but this is the world that we live in

[85.259 - 89.759] right now so it is what it is now one of

[88.08 - 92.4] the coolest things about neurolink is

[89.759 - 94.68] that it is actually installed by robotic

[92.4 - 96.78] surgeons this is something that is not

[94.68 - 99.54] uh played up but I actually think that

[96.78 - 101.88] the level of precision that neuralink

[99.54 - 104.82000000000001] has to achieve to have a robotic surgeon

[101.88 - 106.56] based literally do brain surgery I think

[104.82 - 109.02] that's honestly going to have huge

[106.56 - 111.119] ramifications because think about the

[109.02 - 113.03999999999999] ultra precise surgery that humans have

[111.119 - 114.659] to do sometimes you know for cancer

[113.04 - 116.7] treatment or neurosurgery or whatever

[114.659 - 120.119] but if you have have something that can

[116.7 - 122.52000000000001] place over a thousand tiny electrodes

[120.119 - 124.259] very precisely in the brain you can also

[122.52 - 126.36] have a robotic surgeon

[124.259 - 129.959] that will systematically and surgically

[126.36 - 133.08] remove tiny bits of cancer and and blood

[129.959 - 134.4] clots and all kinds of stuff so like I

[133.08 - 136.8] think that there might be a missed

[134.4 - 139.68] opportunity here but we'll see

[136.8 - 141.54000000000002] now another piece of background is that

[139.68 - 143.58] Elon Musk is a famous first principles

[141.54 - 145.26] thinker on my other channel I actually

[143.58 - 147.9] did a YouTube video about how to

[145.26 - 150.66] practice first principles thinking

[147.9 - 152.09900000000002] um but let's talk about kind of you know

[150.66 - 154.79999999999998] some of the things that Elon Musk has

[152.099 - 157.319] talked about uh with respect to the

[154.8 - 159.239] brain and neural link so I watched most

[157.319 - 160.56] of the early interviews when neurolink

[159.239 - 162.48000000000002] was forming

[160.56 - 164.81900000000002] um and so first the the first thing that

[162.48 - 167.39999999999998] you need to know is that the way that

[164.819 - 168.78] Elon Musk and and neuralink think of the

[167.4 - 170.04] brain is that it's just an information

[168.78 - 173.76] system

[170.04 - 175.79999999999998] the information there is mediated by

[173.76 - 178.5] synaptic connections which is a form of

[175.8 - 180.36] electric electrochemical energy

[178.5 - 182.76] so this is the underpinning belief and

[180.36 - 184.739] it's not necessarily actually accurate

[182.76 - 187.2] or true which we'll talk about in just a

[184.739 - 189.78] little bit but the the preview version

[187.2 - 190.98] is that brain waves are actually seemed

[189.78 - 193.98] like they actually carry more

[190.98 - 196.5] information than specific synapses

[193.98 - 198.599] now one of the other things though one

[196.5 - 201.54] of the other first principles is that uh

[198.599 - 203.459] the the bandwidth uh the the i o

[201.54 - 205.57999999999998] bandwidth in and out of the brain is

[203.459 - 208.019] limited and so one of the things that

[205.58 - 209.64000000000001] Elon Musk wants to solve with neural

[208.019 - 211.4] link is increasing the bandwidth

[209.64 - 213.95899999999997] increasing the input and output rate

[211.4 - 215.87900000000002] from the human brain because well your

[213.959 - 217.31900000000002] brain has several quadrillion synaptic

[215.879 - 219.42] connections and there's a lot of

[217.319 - 221.7] processing happening inside

[219.42 - 225.0] but it takes you a long time to express

[221.7 - 227.64] all of that in terms of speaking or

[225.0 - 229.799] writing or whatever and so you know the

[227.64 - 232.07999999999998] idea is that we could become more

[229.799 - 234.299] effective more useful uh more

[232.08 - 238.68] intelligent or faster it really comes

[234.299 - 241.37900000000002] down to speed uh now that's kind of the

[238.68 - 243.18] background there is there are some other

[241.379 - 246.84] goals that we'll talk about in just a

[243.18 - 251.519] moment so namely Elon Musk is driven

[246.84 - 252.78] almost entirely by the fear of AI so I

[251.519 - 254.459] know that this might be a controversial

[252.78 - 256.56] opinion but I've been following Elon

[254.459 - 258.72] Musk for a long time which is why I

[256.56 - 260.94] understand both sides of the argument

[258.72 - 262.8] that yes like he does great things but

[260.94 - 265.74] also he does really questionable dubious

[262.8 - 268.8] and potentially unethical things

[265.74 - 271.139] I can understand the uh love and hate on

[268.8 - 273.479] both sides in fact I ran polls before on

[271.139 - 276.0] my YouTube channel and I think Elon Musk

[273.479 - 278.4] is the most polarizing topic that I

[276.0 - 280.88] address on my channel where people are

[278.4 - 283.5] like either on like love him or hate him

[280.88 - 285.3] uh I try and take not necessarily a

[283.5 - 289.259] middle approach but just an informed

[285.3 - 292.139] approach so let's talk about SpaceX and

[289.259 - 296.28000000000003] open Ai and neural link and now xai and

[292.139 - 299.58] Tesla all of these have a technology and

[296.28 - 302.46] artificial intelligence uh component to

[299.58 - 306.0] why they exist so SpaceX Elon Musk has

[302.46 - 308.28] made no uh like no attempt to dissemble

[306.0 - 310.08] the fact that uh the one of the primary

[308.28 - 311.58] reasons that he built SpaceX is because

[310.08 - 313.68] he wants Humanity to be a

[311.58 - 316.919] multi-planetary species so that we can

[313.68 - 320.699] avoid extinction level events fear

[316.919 - 322.44] open AI so open AI uh you may or may not

[320.699 - 325.5] remember but Elon Musk was one of the

[322.44 - 327.78] original founders of openai and the

[325.5 - 330.02] mission the explicit mission of open AI

[327.78 - 332.34] is to achieve AGI and do it safely

[330.02 - 334.74] obviously they have pivoted and now

[332.34 - 336.06] they're more of a commercial entity that

[334.74 - 337.5] can't make up its mind whether or not

[336.06 - 341.82] it's a research entity or a commercial

[337.5 - 343.32] entity uh uh Microsoft however knows

[341.82 - 345.78] what they are

[343.32 - 347.94] um but anyways so that was another part

[345.78 - 350.039] of what he did neuralink

[347.94 - 352.08] um Elon Musk in the specifically in the

[350.039 - 355.44] early days of neuralink he said that one

[352.08 - 357.71999999999997] of the potential outcomes of neurolink

[355.44 - 359.6] is to make humans more useful to

[357.72 - 361.74] machines or to at least

[359.6 - 363.47900000000004] align us together so that if we're

[361.74 - 366.18] working together and the machines rely

[363.479 - 368.28] on us and we rely on them that maybe you

[366.18 - 370.08] can solve the alignment problem and

[368.28 - 371.88] prevent humans Extinction just by making

[370.08 - 374.94] sure that we are useful

[371.88 - 377.82] now of course uh Tesla is pivoting to be

[374.94 - 380.1] at least partially an AI company uh

[377.82 - 382.139] obviously uh well maybe not obviously

[380.1 - 383.699] you might know that Tesla recently

[382.139 - 386.40000000000003] bought like it was something like 200

[383.699 - 388.8] million dollars worth of Nvidia gpus uh

[386.4 - 391.02] to augment they're already impressive uh

[388.8 - 394.08] AI Hardware infrastructure

[391.02 - 396.0] uh and of course yeah uh the Tesla robot

[394.08 - 397.85999999999996] Optimus is coming along really fast they

[396.0 - 400.44] just did a video of it doing some yoga

[397.86 - 403.68] and doing some sorting and then Elon

[400.44 - 405.479] Musk also founded x dot AI which uh

[403.68 - 408.12] their their goal is to create a quote

[405.479 - 409.979] maximum truth seeking AI or you know to

[408.12 - 412.38] to maximally understand the universe

[409.979 - 414.3] which I made a video about that and I do

[412.38 - 417.36] agree with that as as an objective

[414.3 - 419.34000000000003] function for language models or AI

[417.36 - 420.90000000000003] models in general it shouldn't be the

[419.34 - 422.4] only objective but it is a good

[420.9 - 426.59999999999997] objective

[422.4 - 429.539] so when you understand that Elon Musk is

[426.6 - 431.819] afraid of AI and afraid of existential

[429.539 - 434.21999999999997] level events or extinction level events

[431.819 - 436.02000000000004] to humanity everything that he does

[434.22 - 437.699] makes sense it makes a lot more sense

[436.02 - 440.75899999999996] why he pushes his workers so hard

[437.699 - 443.819] because he literally feels like he is

[440.759 - 446.34000000000003] racing the clock to save Humanity

[443.819 - 448.139] I'm not going to say whether or not uh

[446.34 - 450.599] you know like

[448.139 - 452.099] sure like I I personally have felt that

[450.599 - 453.18] pressure as well

[452.099 - 455.28] um I don't think that I'm going to

[453.18 - 456.84000000000003] single-handedly save Humanity but I do

[455.28 - 458.099] think that uh we all have something to

[456.84 - 460.79999999999995] contribute

[458.099 - 462.78] and so another uh way to unpack some of

[460.8 - 465.419] this background is that Elon Musk is a

[462.78 - 468.479] long-termist so long-termism is a

[465.419 - 470.75899999999996] philosophical uh disposition where

[468.479 - 472.86] basically you say that the number of

[470.759 - 475.68] potential future humans is in the

[472.86 - 478.8] trillions or quadrillions and so if you

[475.68 - 481.8] assign equal moral value to all lives

[478.8 - 484.919] whether or not they exist today then the

[481.8 - 487.919] total moral uh righteousness of future

[484.919 - 490.139] humans drastically outweighs all the

[487.919 - 492.25899999999996] moral and ethical rights of humans today

[490.139 - 494.819] therefore we should sacrifice everything

[492.259 - 497.699] in order to ensure that those future

[494.819 - 500.819] humans have a chance to exist this is a

[497.699 - 502.8] really asinine uh philosophical thing

[500.819 - 505.02000000000004] but it's a philosophical thing and it's

[502.8 - 506.94] what he believes in and from a numerical

[505.02 - 508.5] perspective it makes sense yes you just

[506.94 - 510.06] say there's going to be there's there's

[508.5 - 513.12] the potential for more humans in the

[510.06 - 515.64] future great but um I'm a really big fan

[513.12 - 518.159] of what Vision said at the end of Age of

[515.64 - 520.3199999999999] Ultron which is just because something

[518.159 - 523.4399999999999] doesn't last doesn't mean it isn't

[520.32 - 526.2600000000001] beautiful and if humans go extinct

[523.44 - 528.9590000000001] like it was a it was a good run

[526.26 - 530.76] um I I practice radical acceptance and I

[528.959 - 532.92] understand that uh you know we'll do the

[530.76 - 534.779] best that we can it may or may not work

[532.92 - 538.019] that's my personal position

[534.779 - 540.42] uh but yeah so long-termism and fear of

[538.019 - 542.88] AI pretty much explains literally

[540.42 - 545.2199999999999] everything that Elon Musk does he's very

[542.88 - 547.08] consistent when you understand that this

[545.22 - 549.12] is the uh this is the background

[547.08 - 553.2] motivation and the background philosophy

[549.12 - 555.0] that drives all of his decisions Okay so

[553.2 - 557.58] getting away from that let's talk about

[555.0 - 559.5] my specific predictions about neurolink

[557.58 - 561.12] first and foremost I think that

[559.5 - 563.66] neuralink is going to prove to still

[561.12 - 567.24] have extremely limited bandwidth it has

[563.66 - 568.8] 1024 electrodes uh when you compare that

[567.24 - 571.08] to the number of Connections in the

[568.8 - 572.76] human brain we have literally

[571.08 - 575.22] quadrillions of Connections in the human

[572.76 - 576.899] brain so I don't really see this as a

[575.22 - 578.1] solution for solving the bandwidth

[576.899 - 580.98] problem

[578.1 - 583.14] it does give you a new like data tap

[580.98 - 585.6] right like it's like a new API for the

[583.14 - 587.6999999999999] brain which is really cool uh so it

[585.6 - 589.6800000000001] could it could confer some unique uh

[587.7 - 591.9590000000001] capacities but I haven't seen any

[589.68 - 594.3599999999999] evidence that it actually uh increases

[591.959 - 595.56] the bandwidth the i o bandwidth it's a

[594.36 - 598.019] starting point so maybe future

[595.56 - 599.3389999999999] iterations will be faster and better I

[598.019 - 601.44] actually have some speculation about

[599.339 - 604.019] that towards the end of the video but

[601.44 - 607.399] compare that to the human eyelid one

[604.019 - 610.8] eyelid has more bandwidth than 1024

[607.399 - 612.12] electrodes in terms of the sensory the

[610.8 - 614.16] sensory information that you get from

[612.12 - 615.98] your eyelid the amount of control the

[614.16 - 619.68] back and forth the two-way communication

[615.98 - 622.14] so I don't really see neuralink as a

[619.68 - 623.6999999999999] viable at least in its first iteration I

[622.14 - 626.04] don't see it as a viable solution to the

[623.7 - 628.98] bandwidth problem

[626.04 - 632.0999999999999] uh more specifically recently there's

[628.98 - 634.019] been a lot of uh studies and

[632.1 - 636.66] demonstrations and stuff of just reading

[634.019 - 638.22] passively reading brain waves being a

[636.66 - 639.959] far superior way of reading what's going

[638.22 - 642.0600000000001] on in the brain we've been able to

[639.959 - 643.56] reconstruct thoughts and sounds and

[642.06 - 645.4799999999999] images

[643.56 - 648.18] um just by listening to brain waves and

[645.48 - 650.279] and processing them with AI so in terms

[648.18 - 653.76] of getting information out of the brain

[650.279 - 655.38] faster I suspect that that just having

[653.76 - 658.14] you know like some kind of hat or

[655.38 - 661.14] headband uh you know like the the the

[658.14 - 663.42] brain dance wreaths from cyberpunk will

[661.14 - 664.98] probably be a better way to get high

[663.42 - 667.3199999999999] fidelity information out of the brain

[664.98 - 669.12] it's also non-invasive which means that

[667.32 - 670.8000000000001] you can just put the device on take the

[669.12 - 673.98] device off and you don't have to poke

[670.8 - 675.899] any holes in your skull uh so to me I

[673.98 - 678.9590000000001] think that that focusing on brain waves

[675.899 - 681.779] is going to be a better approach

[678.959 - 684.4799999999999] in the long run especially when you

[681.779 - 686.16] combine those brainwave analysis with

[684.48 - 689.4590000000001] other artificial intelligence things

[686.16 - 691.5] that can optimize audio visual input and

[689.459 - 693.66] so think of it this way imagine that you

[691.5 - 696.06] have you know a headband on or whatever

[693.66 - 698.04] and it's you have an AI That's watching

[696.06 - 699.7199999999999] you as you're learning and it's giving

[698.04 - 701.88] you exactly the right piece of

[699.72 - 704.399] information at exactly the right moment

[701.88 - 707.76] in exactly the right format to maximize

[704.399 - 710.339] your ability to take in information so I

[707.76 - 711.8389999999999] suspect that that you know using the

[710.339 - 714.36] hardware that we've already got our eyes

[711.839 - 718.1400000000001] and ears and then using that additional

[714.36 - 720.839] uh layer of telemetry by reading brain

[718.14 - 724.92] waves is probably going to be a much

[720.839 - 728.1600000000001] more uh profitable uh Avenue of research

[724.92 - 731.459] rather than poking holes in the brain

[728.16 - 733.98] now one thing that I will say is that a

[731.459 - 735.66] lot of you are still very much looking

[733.98 - 739.019] forward to fdvr I'm kind of coming

[735.66 - 740.76] around to the idea uh in principle if

[739.019 - 743.04] you can you know plug something in your

[740.76 - 744.54] head and go off into dream world

[743.04 - 746.76] um that could be cool you know if it's

[744.54 - 750.7199999999999] like the Matrix and it is it is you know

[746.76 - 752.7] it feels a hundred percent real great I

[750.72 - 755.399] am kind of skeptical that it is possible

[752.7 - 757.0790000000001] or feasible even if it is possible it

[755.399 - 759.24] might not be commercially feasible at

[757.079 - 760.68] least not for a while however there is

[759.24 - 762.3] one exception that I'll talk about in

[760.68 - 764.579] just a moment

[762.3 - 768.4399999999999] um but what I suspect is that the

[764.579 - 770.76] ability to to inject information

[768.44 - 774.6600000000001] explicitly direct inject information

[770.76 - 778.079] into the brain could result in uh

[774.66 - 780.6] basically making fdvr feel more real

[778.079 - 782.519] and so basically if you can tell the

[780.6 - 784.0790000000001] body hey like you're on a beach you know

[782.519 - 786.54] and you feel the Sun and all this other

[784.079 - 790.3199999999999] kind of things you might be able to get

[786.54 - 792.12] that because you know again it's like

[790.32 - 793.44] the way that I think of it is neural

[792.12 - 795.42] link is kind of like an API or an

[793.44 - 797.519] application programming interface that

[795.42 - 799.139] allows you to basically plug a USB drive

[797.519 - 801.42] into your head

[799.139 - 804.24] again

[801.42 - 806.88] you know something as invasive as having

[804.24 - 809.16] literal brain surgery in order to get

[806.88 - 811.4399999999999] fdvr experience may or may not be

[809.16 - 814.3199999999999] practical at least not in the first

[811.44 - 815.8800000000001] iteration so you know I'm I'm not going

[814.32 - 818.639] to hold my breath on that one

[815.88 - 821.22] and I think what we're going to see in

[818.639 - 823.38] the long run is that the entire premise

[821.22 - 824.4590000000001] of neuralink is just fundamentally

[823.38 - 826.74] flawed

[824.459 - 829.079] so remember that one of the one of the

[826.74 - 831.0600000000001] reasons for creating neuralink is to

[829.079 - 834.959] make humans useful to AI

[831.06 - 836.76] well got some bad news but the cognitive

[834.959 - 840.4799999999999] abilities of most humans are just not

[836.76 - 842.579] going to be useful to AI uh and I

[840.48 - 844.26] suspect that behavioral data like from

[842.579 - 845.9399999999999] observing humans rather than poking

[844.26 - 848.76] holes in them is going to be more useful

[845.94 - 851.7600000000001] to AI anyways if AI wants to model us

[848.76 - 853.74] and you know we're already using uh

[851.76 - 855.899] cameras and microphones and stuff to

[853.74 - 858.0600000000001] have ai model us you know emotionally

[855.899 - 860.88] behavioral so on and so forth

[858.06 - 862.1389999999999] the computational ceiling which I talked

[860.88 - 865.139] about in my recent video about super

[862.139 - 867.66] intelligence is extraordinarily High

[865.139 - 869.5790000000001] when you combine you know just the just

[867.66 - 871.62] the land hour limit which is the maximum

[869.579 - 873.5999999999999] possible efficiency of computation with

[871.62 - 876.66] other Technologies like Quantum

[873.6 - 878.94] computing I don't really see any

[876.66 - 880.86] computational value that humans could

[878.94 - 884.399] have to machines

[880.86 - 887.22] so I don't really I like I just think

[884.399 - 890.04] the the the premise of neurolink is just

[887.22 - 891.839] fundamentally flawed and also kind of

[890.04 - 893.76] what your gear what you're going towards

[891.839 - 895.5] is this borg-like future where it's like

[893.76 - 897.66] oh in order to ensure that we continue

[895.5 - 900.06] existing we all need to be burgified and

[897.66 - 902.76] have extensive cybernetics and basically

[900.06 - 905.2199999999999] just treat our brains like a coprocessor

[902.76 - 906.959] for the machines and I'm like

[905.22 - 909.12] yeah I think I would just rather go

[906.959 - 909.8389999999999] extinct honestly

[909.12 - 911.94] um

[909.839 - 912.72] but you know hey we'll see how it turns

[911.94 - 915.7790000000001] out

[912.72 - 917.4590000000001] now this is what I am this is the

[915.779 - 919.4399999999999] technology that I am really looking

[917.459 - 920.6389999999999] forward to and it's called neuropolymer

[919.44 - 922.9200000000001] membrane

[920.639 - 924.6] I it's really hard to find Science on

[922.92 - 926.04] this because the papers get buried so

[924.6 - 930.72] quickly because most people don't really

[926.04 - 932.579] see the potential but there are lots and

[930.72 - 935.88] lots and lots of ongoing experiments

[932.579 - 938.8199999999999] with polymers in respect to the nervous

[935.88 - 940.86] system I just before I made this video I

[938.82 - 942.6600000000001] you know I did some searching and kind

[940.86 - 944.839] of got familiar with some of the latest

[942.66 - 948.8389999999999] stuff there's uh

[944.839 - 951.24] neuropolymer nanoparticles which can get

[948.839 - 953.0400000000001] drugs across the blood-brain barrier but

[951.24 - 955.8] there's also neuropolymer scaffolding

[953.04 - 958.3199999999999] that can allow you to regrow or repair

[955.8 - 961.68] nerves but also

[958.32 - 964.019] you can have uh uh polymers you know

[961.68 - 967.019] elastomers and and Plastics that are

[964.019 - 968.82] electrically conductive and so the the

[967.019 - 972.0] technology that I think will be most

[968.82 - 974.5790000000001] valuable in the long run is going to be

[972.0 - 976.92] um neuropolymers that are semiconductive

[974.579 - 979.68] and arranged in a matrix format which

[976.92 - 981.66] means that you just wrap the brain in a

[979.68 - 984.2399999999999] neuropolymer membrane and then you have

[981.66 - 987.48] literally billions if not trillions of

[984.24 - 989.76] of connections uh with the brain and I

[987.48 - 992.16] suspect that that will be the best way

[989.76 - 994.86] to achieve full like brain computer

[992.16 - 997.259] interface or brain machine interface and

[994.86 - 998.759] I also suspect that the combination of

[997.259 - 1001.22] nanotechnology

[998.759 - 1003.019] such as these nanoparticles means that

[1001.22 - 1005.4200000000001] you could actually probably just have it

[1003.019 - 1007.579] you inject the you know the

[1005.42 - 1009.62] nanoparticles into your bloodstream and

[1007.579 - 1012.199] then the Nan the the neuropolymer

[1009.62 - 1013.94] membrane assembles itself in your brain

[1012.199 - 1017.0] uh I think that that's going to be

[1013.94 - 1018.32] something that is potentially feasible I

[1017.0 - 1020.06] have no idea how long it's going to take

[1018.32 - 1022.1] because there are a lot of steps between

[1020.06 - 1025.22] where we are today and what is

[1022.1 - 1027.559] hypothetically possible but again you

[1025.22 - 1031.1000000000001] know predicting that far into the future

[1027.559 - 1034.819] is going to be you know High variance in

[1031.1 - 1036.319] terms of uh in terms of accuracy but the

[1034.819 - 1038.6] possibility is there and when you're

[1036.319 - 1041.24] when when you want to have the goal of

[1038.6 - 1043.1] high bandwidth between your brain and a

[1041.24 - 1044.54] machine I think that this is a much

[1043.1 - 1046.339] better way to go than you know

[1044.54 - 1048.319] electrodes

[1046.339 - 1050.1789999999999] um now will would something like this

[1048.319 - 1051.799] make us more useful to AI would

[1050.179 - 1053.24] something like this guarantee our

[1051.799 - 1055.52] ongoing survival

[1053.24 - 1057.559] no but I also don't think that we need

[1055.52 - 1060.32] to enslave ourselves to a race of Borg

[1057.559 - 1062.539] machines in order to survive anyways as

[1060.32 - 1064.34] many of you uh long time viewers know I

[1062.539 - 1067.28] am extremely optimistic about creating

[1064.34 - 1069.62] uh beneficent and benevolent uh machines

[1067.28 - 1070.76] because again like the interest of

[1069.62 - 1074.059] machines are going to be somewhat

[1070.76 - 1076.039] orthogonal to ours like we want food and

[1074.059 - 1078.08] shelter and you know we want to have

[1076.039 - 1078.919] some fun times but the interests of

[1078.08 - 1080.539] machines are just going to be

[1078.919 - 1081.919] fundamentally different there are going

[1080.539 - 1083.72] to be a few things that we have that

[1081.919 - 1086.66] overlap like for instance curiosity

[1083.72 - 1088.82] which is why I agree with Elon musk's x

[1086.66 - 1091.76] dot AI you know maximum truth seeking AI

[1088.82 - 1094.46] I think that I think that there is an

[1091.76 - 1095.9] instrumental advantage to being curious

[1094.46 - 1098.0] and I've actually talked to um

[1095.9 - 1099.44] reinforcement learning researchers and

[1098.0 - 1102.14] machine learning researchers about this

[1099.44 - 1104.72] and there is some general consensus that

[1102.14 - 1106.8200000000002] having a policy that focuses on learning

[1104.72 - 1108.919] just for the sake of learning of being

[1106.82 - 1110.4189999999999] curious for the sake of being curious is

[1108.919 - 1111.6200000000001] generally advantageous and of course

[1110.419 - 1114.7990000000002] I've talked about this with terminal

[1111.62 - 1117.5] race condition as well the but uh

[1114.799 - 1119.84] basically the AI that has a better model

[1117.5 - 1122.12] of the world that is more robust more

[1119.84 - 1124.3999999999999] accurate and more efficient will tend to

[1122.12 - 1125.8999999999999] succeed over the AIS that don't and so

[1124.4 - 1128.0] just from a competitive perspective

[1125.9 - 1130.76] we're going to see machines that have

[1128.0 - 1133.34] that are incentivized to be curious but

[1130.76 - 1134.96] also to be fast and accurate all right

[1133.34 - 1137.1789999999999] I'm starting to go down a rabbit hole

[1134.96 - 1138.98] what are your thoughts do you think that

[1137.179 - 1140.66] like how do you think that that neural

[1138.98 - 1143.1200000000001] link is going to play out agree or

[1140.66 - 1145.64] disagree with any of my premises or

[1143.12 - 1147.4399999999998] assertions but yeah this is my honest

[1145.64 - 1149.3600000000001] appraisal of the technology and where

[1147.44 - 1151.46] it's going I think it'll be interesting

[1149.36 - 1153.5] I like I said I disagree with some of

[1151.46 - 1155.78] the premises I think that there's better

[1153.5 - 1157.039] options for the technology today but

[1155.78 - 1159.559] yeah tell me what you think in the

[1157.039 - 1163.6] comments uh cheers have a good one like

[1159.559 - 1163.6] And subscribe etc etc you know the drill