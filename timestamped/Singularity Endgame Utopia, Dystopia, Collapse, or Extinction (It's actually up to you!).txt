[0.74 - 6.54] morning everybody David Shapiro here

[3.6 - 9.48] with another video today's video is

[6.54 - 11.639] going to be a bit of a doozy so we were

[9.48 - 13.639] talking about the singularity and in

[11.639 - 17.58] particular The Singularity end game

[13.639 - 20.279] namely that there are four potential

[17.58 - 22.139999999999997] likely outcomes there's quite a few more

[20.279 - 24.9] than these four

[22.14 - 26.580000000000002] um but these are the easiest ones to

[24.9 - 28.858999999999998] talk about

[26.58 - 30.778999999999996] um so those four are Utopia dystopia

[28.859 - 32.160000000000004] collapse and Extinction so let's go

[30.779 - 34.68] ahead and get started

[32.16 - 36.31999999999999] actually I lied before we get started I

[34.68 - 40.5] just want to plug my patreon real quick

[36.32 - 42.3] I've had tremendous growth in subscriber

[40.5 - 44.64] support so I've actually had to put a

[42.3 - 47.339999999999996] cap on the higher tiers because I only

[44.64 - 49.079] have so many hours in a day so if you

[47.34 - 50.579] want to jump in on my patreon there are

[49.079 - 52.86] lower tiers if you just want to be a

[50.579 - 56.34] sustainer or chat with me via patreon

[52.86 - 59.219] but if you want to jump in and have zoom

[56.34 - 60.719] calls I schedule those via calendly and

[59.219 - 62.64] I'm happy to jump in and talk about

[60.719 - 64.86] whatever you want to talk about within

[62.64 - 65.93900000000001] reason obviously all right so right back

[64.86 - 68.22] to the show

[65.939 - 70.74] introduction what are we talking about

[68.22 - 73.32] let's define some terms

[70.74 - 76.25999999999999] so first we need to talk about the

[73.32 - 78.24] concept of global attractor States so a

[76.26 - 80.759] global attractor state is a long-term

[78.24 - 82.979] scenario or our outcome for the the

[80.759 - 86.64] entirety of humanity

[82.979 - 88.2] it is characterized by specific Trends

[86.64 - 90.0] and factors

[88.2 - 92.64] um the ones that I chose for this video

[90.0 - 94.759] are quality of life population stability

[92.64 - 98.04] and sustainability

[94.759 - 100.82] these attractor states are driven by

[98.04 - 103.619] certain factors and macroeconomic forces

[100.82 - 106.32] such as technology we'll get into the

[103.619 - 108.42] details in a minute

[106.32 - 110.579] um it is also influenced by the choices

[108.42 - 113.18] the collective choices of individuals

[110.579 - 117.96] corporations Nations and so on

[113.18 - 121.14] and these will serve as a framework for

[117.96 - 123.36] exploring this uh this concept

[121.14 - 126.24] so the four basic attractor states that

[123.36 - 130.08] I talked about or uh Singularity

[126.24 - 133.79999999999998] outcomes are Utopia dystopia collapse

[130.08 - 135.72] and Extinction so Utopia in terms of the

[133.8 - 137.34] dimensions that I just mentioned uh

[135.72 - 139.92] characterized by high quality of life

[137.34 - 143.22] sustainability Global cooperation yada

[139.92 - 146.04] yada yada you get it dystopia is pretty

[143.22 - 147.14] much the opposite of Utopia low quality

[146.04 - 150.78] of life

[147.14 - 153.72] high or unstable population widespread

[150.78 - 156.959] instability so on and so forth now

[153.72 - 160.62] collapse is another possibility where

[156.959 - 163.019] the quality of life rapidly declines as

[160.62 - 166.56] well as the population and this could be

[163.019 - 168.92000000000002] due to extreme amounts of failure or

[166.56 - 171.66] other problems that arise such as

[168.92 - 174.42] ecological collapse global conflict and

[171.66 - 177.48] so on and then finally Extinction is

[174.42 - 179.04] where for whatever reason Humanity goes

[177.48 - 181.61999999999998] extinct now this could be that the Earth

[179.04 - 184.739] becomes uninhabitable due to extreme

[181.62 - 186.78] climate change nuclear war runaway AGI

[184.739 - 188.76] all sorts of things there's several

[186.78 - 189.9] existential threats that we are aware of

[188.76 - 191.57999999999998] and of course there's natural

[189.9 - 194.09900000000002] existential threats like a you know

[191.58 - 196.37900000000002] Comet strike or something

[194.099 - 197.39999999999998] so the met the dimensions that I

[196.379 - 198.72] mentioned

[197.4 - 200.519] um because we want to be scientific

[198.72 - 203.159] about this and approach it in something

[200.519 - 205.62] that's actually measurable number one is

[203.159 - 207.48] quality of life which encompasses the

[205.62 - 210.36] overall well-being happiness and

[207.48 - 212.099] personal satisfaction of all individuals

[210.36 - 214.20000000000002] on the planet

[212.099 - 216.29999999999998] um which can be measured by proxies such

[214.2 - 219.06] as health education economic

[216.3 - 220.68] opportunities uh and the other

[219.06 - 224.04] measurements like Jenny coefficient

[220.68 - 226.5] equality so on and so forth

[224.04 - 230.159] um population is another good metric

[226.5 - 231.659] because uh obviously in the Co in the

[230.159 - 234.239] case of collapse or Extinction you see

[231.659 - 236.94] the population fall off a cliff

[234.239 - 239.459] um in many dystopic outcomes you might

[236.94 - 241.62] expect the population to climb to

[239.459 - 244.799] unsustainable levels which could then be

[241.62 - 246.37900000000002] followed by a collapse it could also you

[244.799 - 249.36] could also see an unstable population

[246.379 - 252.35999999999999] where we become overpopulated and due to

[249.36 - 254.81900000000002] overcrowding and uh breakdown of systems

[252.36 - 256.62] the population declines significantly

[254.819 - 258.65999999999997] not necessarily A collapse and then it

[256.62 - 260.519] comes back we want to avoid that as well

[258.66 - 263.759] we want nice stable sustainable

[260.519 - 266.04] population and of course finally overall

[263.759 - 269.18] global stability and sustainability so

[266.04 - 272.34000000000003] this includes geopolitical stability

[269.18 - 274.199] environmental sustainability resilience

[272.34 - 276.59999999999997] balance and so on

[274.199 - 277.91900000000004] so those are the three primary

[276.6 - 280.91900000000004] Dimensions that we're going to be

[277.919 - 283.85999999999996] looking at while we evaluate these uh

[280.919 - 285.84] attractor States or the singularity

[283.86 - 287.58000000000004] outcomes that we're looking at

[285.84 - 289.38] all right so let's take a closer look at

[287.58 - 291.84] each of those four states

[289.38 - 293.46] so State one Utopia this is the one that

[291.84 - 295.919] we all want

[293.46 - 298.56] um so Utopia is defined by a high

[295.919 - 300.06] quality of life which can be

[298.56 - 301.68] characterized by high levels of

[300.06 - 304.02] well-being happiness and satisfaction

[301.68 - 305.94] for everyone not just a few people

[304.02 - 307.25899999999996] Everyone

[305.94 - 310.919] um I just mentioned that the population

[307.259 - 313.86] should be sustainable which also implies

[310.919 - 316.62] a stable population you don't want

[313.86 - 319.91900000000004] demographic collapses you don't want

[316.62 - 321.44] population decline due to famine war and

[319.919 - 324.59999999999997] so on and so forth

[321.44 - 327.24] Utopia is also defined by stability and

[324.6 - 328.58000000000004] sustainability so this includes economic

[327.24 - 331.86] sustainability

[328.58 - 335.4] geopolitical stability uh so on and so

[331.86 - 337.759] forth it goes well above and beyond

[335.4 - 340.62] climate change right it's got to be

[337.759 - 342.66] global stability in pretty much every

[340.62 - 343.979] way that you can measure that so some of

[342.66 - 346.56] the key factors that will help us

[343.979 - 349.32] achieve Utopia could be technological

[346.56 - 350.88] breakthroughs like sustainable energy

[349.32 - 353.58] artificial intelligence which is

[350.88 - 357.3] happening right now global cooperation

[353.58 - 359.15999999999997] and then also cultural and social shifts

[357.3 - 362.1] because we do have some some really

[359.16 - 365.0] destructive Tendencies right now

[362.1 - 367.02000000000004] state number two is dystopia which is

[365.0 - 368.46] what a lot of people are afraid that

[367.02 - 372.06] we're heading towards

[368.46 - 375.35999999999996] um so dystopia is characterized by a low

[372.06 - 378.3] quality of life such as limited access

[375.36 - 380.46000000000004] to basic needs and resources widespread

[378.3 - 383.28000000000003] suffering unhappiness and general

[380.46 - 386.09999999999997] dissatisfaction with life potentially

[383.28 - 388.919] High population so overcrowding could

[386.1 - 392.03900000000004] result in a dystopic outcome or it could

[388.919 - 394.5] be a contributing factor but also it's

[392.039 - 396.65999999999997] important to keep in mind that a lower

[394.5 - 399.24] population if not a population collapse

[396.66 - 401.699] could also be part of a dystopia because

[399.24 - 405.259] people are just too unhappy too unwell

[401.699 - 405.259] and too poor to have children

[405.44 - 410.34] another feature of a dystopian outcome

[408.84 - 412.85999999999996] would be instability and

[410.34 - 414.78] unsustainability so dystopia could very

[412.86 - 417.6] quickly lead to collapse due to

[414.78 - 421.13899999999995] instability or unsustainability but it

[417.6 - 424.88] could also lead to a a pullback where

[421.139 - 427.68] the population declines such that

[424.88 - 428.88] stability returns due to lack of

[427.68 - 430.979] competition

[428.88 - 433.5] so some of the key factors that could

[430.979 - 435.419] lead to dystopia number one is unchecked

[433.5 - 437.759] corporatism

[435.419 - 440.099] um social and economic inequality

[437.759 - 442.62] authoritarianism

[440.099 - 445.919] um and then finally unmitigated nihilism

[442.62 - 448.319] and intergenerational PTSD which are as

[445.919 - 451.38] yet unsolved problems

[448.319 - 454.86] state number three collapse so collapse

[451.38 - 458.699] is what is uh some people are more

[454.86 - 461.22] afraid of and that is that uh because of

[458.699 - 464.539] ongoing instability and environmental

[461.22 - 468.18] change we might end up with very very

[464.539 - 470.4] precipitous drops in quality of life as

[468.18 - 473.22] well as significant population loss so

[470.4 - 476.479] this is portrayed in TV shows like The

[473.22 - 479.09900000000005] Walking Dead and movies like Mad Max

[476.479 - 481.38] where basically there's only a few

[479.099 - 482.94] humans left after the collapse and we're

[481.38 - 485.28] talking less than five percent of the

[482.94 - 489.0] current population but as low as like

[485.28 - 490.73999999999995] 0.1 percent of the population so in in

[489.0 - 493.08] the case of total infrastructure

[490.74 - 495.96000000000004] breakdown due to an unraveling of social

[493.08 - 498.84] fabric most people starve to death just

[495.96 - 502.81899999999996] due to lack of infrastructure if we run

[498.84 - 505.61999999999995] out of fuel and international trade

[502.819 - 509.03900000000004] basically Farms fail and then most

[505.62 - 512.159] people starve so collapse is actually a

[509.039 - 515.159] lot higher of a risk than you might

[512.159 - 516.959] think because of how fragile logistical

[515.159 - 518.9399999999999] chains are

[516.959 - 523.26] and then finally State four is

[518.94 - 525.0600000000001] extinction this is a a stable outcome

[523.26 - 528.0] because if humans are gone they're not

[525.06 - 530.399] coming back and then of course there's a

[528.0 - 532.2] few ways to achieve this not that we

[530.399 - 535.14] want to but nuclear war nuclear

[532.2 - 538.32] Holocaust could do that runaway AGI or

[535.14 - 541.1] total ecological collapse could make the

[538.32 - 544.1400000000001] Earth completely uninhabitable to humans

[541.1 - 547.14] and in that case the viability of the

[544.14 - 549.66] planet is called into question not just

[547.14 - 552.42] for humans but many other animals in

[549.66 - 555.38] general uh you know the planet has

[552.42 - 557.5799999999999] survived mass extinctions in the past

[555.38 - 559.14] but that doesn't preclude the

[557.58 - 561.5400000000001] possibility of another mass extinction

[559.14 - 563.459] in the future which would which could

[561.54 - 565.98] very well include us and it could be

[563.459 - 568.9799999999999] self-inflicted

[565.98 - 571.08] okay so I've painted a nice Rosy picture

[568.98 - 572.94] we've only got one desirable outcome in

[571.08 - 575.0400000000001] three really undesirable outcomes so

[572.94 - 577.32] what are the primary drivers and factors

[575.04 - 578.6999999999999] Behind These I already outlined some of

[577.32 - 580.2] the dimensions that we're measuring but

[578.7 - 583.08] that's not what's the that's not what

[580.2 - 586.9200000000001] the uh the causes are

[583.08 - 589.019] so there are six overall uh drivers and

[586.92 - 591.0] factors that I identified so first is

[589.019 - 593.7] technological advancements

[591.0 - 596.22] so technology is always a double-edged

[593.7 - 598.6800000000001] sword look at nuclear energy if you

[596.22 - 601.08] harness nuclear energy responsibly you

[598.68 - 604.38] get nuclear reactors you get thorium you

[601.08 - 606.36] get molten salts nuclear fusion so on

[604.38 - 608.6] and so forth now the same exact

[606.36 - 612.0600000000001] technology can be used to make weapons

[608.6 - 614.94] so nuclear energy is a perfect example

[612.06 - 617.0999999999999] of how disruptive Technologies are

[614.94 - 620.82] always a double-edged sword it is not

[617.1 - 623.64] that a technology is intrinsically evil

[620.82 - 625.1600000000001] it is how we choose to use it so that

[623.64 - 628.5] can go either way

[625.16 - 631.2199999999999] Factor number two is geopolitics so this

[628.5 - 636.18] includes international trade globalism

[631.22 - 638.6] de-globalism as well as uh Trends on the

[636.18 - 641.18] on the global stage such as

[638.6 - 644.7] authoritarianism surveillance

[641.18 - 647.16] liberalization of democracies and then

[644.7 - 650.4590000000001] of course cooperate Global cooperation

[647.16 - 653.399] or a lack thereof uh Factor number three

[650.459 - 655.8] is economic policy which this has to do

[653.399 - 657.899] with Economic Institutions

[655.8 - 660.42] um as well as just individual economic

[657.899 - 663.839] and fiscal policy so what I mean by that

[660.42 - 666.42] is uh we have institutions like the IMF

[663.839 - 670.86] the World Bank there's uh groups like

[666.42 - 673.8] Davos G20 G8 World economic Forum so

[670.86 - 677.04] that is where Global policy is set but

[673.8 - 679.62] then Local Economic Policy is decided on

[677.04 - 681.899] an on a nation by nation and state by

[679.62 - 684.18] state basis which that includes

[681.899 - 686.279] everything from you know Taxation and

[684.18 - 688.4399999999999] wealth redistribution all the way up to

[686.279 - 690.18] how much influence corporations are

[688.44 - 692.7600000000001] allowed to have and then of course those

[690.18 - 694.5] macro economic decisions made in the

[692.76 - 697.3199999999999] halls of power

[694.5 - 699.6] uh number four is Environmental

[697.32 - 701.7] Management so this is pretty obvious

[699.6 - 704.399] basically climate change do we do

[701.7 - 706.62] something about it yes or no do we clean

[704.399 - 708.42] up the oceans do we adopt sustainable

[706.62 - 710.339] farming practices do we continue

[708.42 - 713.6999999999999] deforestation

[710.339 - 715.6800000000001] um that is uh again that that can easily

[713.7 - 718.019] go either way it is entirely up to us

[715.68 - 719.6999999999999] how we treat the environment

[718.019 - 723.0] um societal values so I already

[719.7 - 725.22] mentioned things like uh nihilism but

[723.0 - 730.1] there's also other factors such as do we

[725.22 - 732.36] invest in empathy and equity and

[730.1 - 734.399] emotional sustainability of the world

[732.36 - 738.12] and then finally

[734.399 - 741.72] um nihilism we live in an age of

[738.12 - 746.339] nihilism which is caused by and results

[741.72 - 748.5] in violence Despair and cynicism

[746.339 - 750.48] and it is a vicious cycle it is a

[748.5 - 753.6] self-perpetuating loop and we'll talk

[750.48 - 757.019] more about all of these right now

[753.6 - 758.5790000000001] so Factor one technology obviously top

[757.019 - 760.92] of mind right now is artificial

[758.579 - 763.8] intelligence with the rapid rise of

[760.92 - 766.1999999999999] autonomous AI systems and we are

[763.8 - 769.04] barreling towards AGI which is a

[766.2 - 772.0790000000001] critical component of the singularity

[769.04 - 773.6999999999999] uh again like all Technologies it's a

[772.079 - 776.0999999999999] double-edged sword it has more to do

[773.7 - 778.32] with how we implement it rather than any

[776.1 - 781.6800000000001] intrinsic motivation of these

[778.32 - 784.0790000000001] Technologies we can build AGI to be like

[781.68 - 786.18] nuclear weapons or we can build AGI to

[784.079 - 788.3389999999999] be like nuclear power it can be harmful

[786.18 - 790.92] it can be helpful it can be both it can

[788.339 - 794.22] be neither depending on how we implement

[790.92 - 795.66] it renewable energy will be another huge

[794.22 - 799.0790000000001] Factor

[795.66 - 804.36] um whether that's solar Fusion thorium

[799.079 - 808.56] molten salt wind whatever even even uh

[804.36 - 812.519] green greenish forms of energy uh could

[808.56 - 813.7399999999999] be uh or or Renewables right

[812.519 - 816.0] um such as

[813.74 - 817.5600000000001] harvesting trees and burning them right

[816.0 - 819.0] because then the you plant more trees

[817.56 - 820.279] and they pull the carbon right back out

[819.0 - 822.839] of the air

[820.279 - 826.5] biotechnology so biotech includes

[822.839 - 829.8000000000001] genetics protein medicine life extension

[826.5 - 831.72] um cloning organ replacement as well as

[829.8 - 832.9799999999999] farming and agriculture but I did split

[831.72 - 834.779] these up because agricultural

[832.98 - 837.48] Technologies go above and beyond

[834.779 - 839.04] biotechnology so vertical farming

[837.48 - 842.88] genetic engineering of crops

[839.04 - 846.18] regenerative agriculture practices will

[842.88 - 849.019] be absolutely critical to figure out in

[846.18 - 852.18] order to create a sustainable population

[849.019 - 854.94] either as the population grows or as the

[852.18 - 858.3599999999999] climate changes but basically we need

[854.94 - 860.7] more resilient food sources

[858.36 - 862.38] Factor two is geopolitics so Global

[860.7 - 864.48] governance now this doesn't mean a One

[862.38 - 867.12] World Government but Global governance

[864.48 - 869.1] means uh cooperation and collaboration

[867.12 - 870.6] between all governments which we are

[869.1 - 872.519] trending in that direction with the

[870.6 - 874.5600000000001] exception of Britain leaving the EU

[872.519 - 876.18] which everyone is very happy to remind

[874.56 - 878.5189999999999] me that I got that wrong when I implied

[876.18 - 881.8199999999999] that Britain was part of the EU they are

[878.519 - 885.839] not right now I hope they will rejoin

[881.82 - 888.779] um another uh aspect of geopolitics is

[885.839 - 891.7790000000001] democracy versus authoritarianism

[888.779 - 893.3] um this is kind of the theme of the last

[891.779 - 895.98] century or so

[893.3 - 899.9399999999999] in terms of global war which is uh

[895.98 - 902.16] ideological incompatibilities it was

[899.94 - 904.8800000000001] with World War one and two that was

[902.16 - 907.68] between that was in and amongst Europe

[904.88 - 911.1] fortunately Europe by and large has

[907.68 - 913.26] unified around liberal democracy Now the

[911.1 - 916.1990000000001] uh and so that was you know fascism

[913.26 - 917.16] versus you know freedom and then it

[916.199 - 920.519] became

[917.16 - 923.12] um you know American versus Soviet and

[920.519 - 927.3] now it's by and large East versus West

[923.12 - 929.579] and so that is a contest that has yet to

[927.3 - 932.579] be resolved and it could take many many

[929.579 - 935.8389999999999] more decades for it to be resolved both

[932.579 - 938.6389999999999] China and Russia which are the primary

[935.839 - 941.0400000000001] authoritarian regimes that remain have

[938.639 - 943.98] both tried to liberalize but they are so

[941.04 - 945.8389999999999] corrupt and and a number of other

[943.98 - 948.26] problems that they have kind of

[945.839 - 951.1800000000001] backpedaled in the last few years

[948.26 - 954.66] nationalism and populism these are other

[951.18 - 955.8199999999999] factors that can contribute to internal

[954.66 - 959.04] strife

[955.82 - 960.0790000000001] civil unrest within Nations and

[959.04 - 962.519] basically

[960.079 - 963.8389999999999] well they're entirely too large to

[962.519 - 966.18] unpack

[963.839 - 968.6990000000001] um but some of the things that can

[966.18 - 971.9399999999999] result is political polarization

[968.699 - 973.8] isolationism trade Wars and so on and of

[971.94 - 976.6800000000001] course if a nation is internally less

[973.8 - 979.699] effective it will be less of a player on

[976.68 - 982.1389999999999] the global stage and could even become

[979.699 - 984.7199999999999] an agent of Chaos

[982.139 - 986.76] and then finally conflict and diplomacy

[984.72 - 989.3000000000001] see above statements

[986.76 - 992.16] Factor number three is economic policy

[989.3 - 993.3] so uh to dive a little bit deeper into

[992.16 - 996.259] this

[993.3 - 998.7589999999999] um Concepts such as wealth distribution

[996.259 - 1001.16] uh which right now we are becoming

[998.759 - 1003.98] increasingly unequal in terms of

[1001.16 - 1005.42] distribution uh wealth is concentrating

[1003.98 - 1007.4590000000001] at the top which this goes through

[1005.42 - 1010.0999999999999] patterns in history

[1007.459 - 1012.8599999999999] um so it has reset many times throughout

[1010.1 - 1016.519] history hopefully we can have a wealth

[1012.86 - 1018.279] reset without violence or collapse in

[1016.519 - 1021.259] the case of the Roman Republic

[1018.279 - 1023.18] transitioning to the Roman Empire wealth

[1021.259 - 1025.28] continued accumulating in the upper

[1023.18 - 1027.1989999999998] echelons of Rome until it was so

[1025.28 - 1028.78] unbalanced that it contributed to the

[1027.199 - 1031.4] collapse of the Roman Empire

[1028.78 - 1034.04] and that was just due to systemic

[1031.4 - 1036.26] failures institutional failures number

[1034.04 - 1038.12] two is Corporate influence

[1036.26 - 1040.579] so as I have mentioned in many videos

[1038.12 - 1043.76] corporations are intrinsically amoral

[1040.579 - 1045.74] the only thing that they desire is more

[1043.76 - 1049.16] income and they will optimize their

[1045.74 - 1050.6] strategy around income now that means

[1049.16 - 1052.1000000000001] that they have to play nice in terms of

[1050.6 - 1054.6999999999998] regulations and not abusing their

[1052.1 - 1057.1399999999999] employees too much but

[1054.7 - 1058.64] corporations want Power they want more

[1057.14 - 1060.5590000000002] power they want more money and they will

[1058.64 - 1063.2] stop at nothing to do that and so if we

[1060.559 - 1065.4189999999999] do not rein corporations in we will end

[1063.2 - 1067.76] up with a situation where corporations

[1065.419 - 1069.5] have far more influence than voters

[1067.76 - 1072.74] which there are many people that argue

[1069.5 - 1074.419] that they already do whether or not you

[1072.74 - 1076.64] believe that corporations have more

[1074.419 - 1079.64] influence than voters they already have

[1076.64 - 1081.74] too much influence overall in order to

[1079.64 - 1084.44] achieve a stable outcome

[1081.74 - 1087.26] fiscal policy has to do with taxation I

[1084.44 - 1091.1000000000001] already mentioned that so taxation

[1087.26 - 1092.96] redistribution and also how things are

[1091.1 - 1095.059] allocated so this has to do with the

[1092.96 - 1097.52] philosophy of how much do we tax and

[1095.059 - 1099.6789999999999] where do we spend that tax money

[1097.52 - 1101.96] um that has a huge impact on these

[1099.679 - 1106.039] outcomes and then finally sustainable

[1101.96 - 1109.039] development this Inc this includes using

[1106.039 - 1112.039] economic policy to incentivize stability

[1109.039 - 1113.419] and sustainability which by and large a

[1112.039 - 1115.22] lot of governments are moving in this

[1113.419 - 1117.14] direction the question is whether or not

[1115.22 - 1120.559] they're moving fast enough

[1117.14 - 1122.3600000000001] so whether you get rid of oil subsidies

[1120.559 - 1125.32] you create solar subsidies or you

[1122.36 - 1127.4599999999998] subsidize research of a certain kind

[1125.32 - 1129.1399999999999] governments have the ability to

[1127.46 - 1131.299] incentivize the behaviors that they want

[1129.14 - 1133.22] to see and push people towards more

[1131.299 - 1134.62] sustainable development and again this

[1133.22 - 1138.32] is happening all over the place

[1134.62 - 1140.78] including incentivizing sustainable Home

[1138.32 - 1142.82] Building practices by you know for

[1140.78 - 1145.82] instance requiring an increase of

[1142.82 - 1147.4399999999998] insulation quality it also has to do

[1145.82 - 1149.6599999999999] with setting standards for for

[1147.44 - 1152.1200000000001] automobiles so for the longest time they

[1149.66 - 1154.28] set targets for uh fuel efficiency in

[1152.12 - 1157.1599999999999] cars and now of course many governments

[1154.28 - 1159.86] are setting targets to get rid of petrol

[1157.16 - 1161.9] petroleum burning cars altogether those

[1159.86 - 1163.2199999999998] are examples of how Economic Policy can

[1161.9 - 1165.26] influence the outcomes that we're

[1163.22 - 1167.24] talking about here

[1165.26 - 1170.24] Factor number four environmental

[1167.24 - 1172.039] sustainability so this goes a lot Beyond

[1170.24 - 1173.84] climate change climate change is you

[1172.039 - 1176.26] know the biggest anxiety-inducing one

[1173.84 - 1178.6999999999998] but there are plenty of other resources

[1176.26 - 1181.539] that could become depleted such as

[1178.7 - 1184.5800000000002] Fisheries or mineral resources

[1181.539 - 1186.44] potable water arable land so for

[1184.58 - 1188.4189999999999] instance China One of the primary

[1186.44 - 1192.2] problems that China is facing is water

[1188.419 - 1195.6200000000001] shortages and deterioration of waterways

[1192.2 - 1198.2] and arable land so you run out of those

[1195.62 - 1200.36] growth stops you poison your land you

[1198.2 - 1202.3400000000001] poison your water everyone suffers you

[1200.36 - 1203.6] poison your air air is a shared Resource

[1202.34 - 1206.08] as well

[1203.6 - 1208.76] um you we we have already seen

[1206.08 - 1212.24] tremendous detrimental impacts to health

[1208.76 - 1214.179] and overly congested cities where

[1212.24 - 1217.28] there's too much smog

[1214.179 - 1219.14] biodiversity loss so biodiversity loss

[1217.28 - 1220.76] is another thing that is one it's

[1219.14 - 1223.5200000000002] happening because we are in the midst of

[1220.76 - 1226.7] the anthropocene basically man-made

[1223.52 - 1229.24] Extinction uh mass extinction event so

[1226.7 - 1232.1200000000001] one thing that can happen is as you lose

[1229.24 - 1235.76] ecological niches or niches

[1232.12 - 1239.7199999999998] you can end up with runaway cascading

[1235.76 - 1242.059] runaway effects so in some cases for

[1239.72 - 1243.919] instance on Easter Island they

[1242.059 - 1246.02] deforested their Island which drove the

[1243.919 - 1247.4] seabirds away which caused the soil to

[1246.02 - 1249.3799999999999] become infertile

[1247.4 - 1252.6200000000001] and so then the population of Easter

[1249.38 - 1253.94] Island collapsed just due to removing

[1252.62 - 1256.52] trees

[1253.94 - 1259.1000000000001] uh and then finally

[1256.52 - 1261.2] um one thing is and this again this is

[1259.1 - 1264.4399999999998] already happening one Farm to Table

[1261.2 - 1266.24] local Source circular economy uh

[1264.44 - 1268.52] basically creating a much more resilient

[1266.24 - 1271.1] and local and sustainable

[1268.52 - 1273.86] um set a usage of resources and that

[1271.1 - 1275.4189999999999] includes biological resources but it

[1273.86 - 1278.24] also includes material and mineral

[1275.419 - 1281.179] resources uh so for instance there's a

[1278.24 - 1282.86] lot of companies working on uh trying to

[1281.179 - 1284.419] recycle lithium for instance and there's

[1282.86 - 1286.34] other researchers and companies trying

[1284.419 - 1288.8600000000001] to replace our need for lithium and

[1286.34 - 1292.1] other rare Metals altogether

[1288.86 - 1294.26] Factor number five cultural values

[1292.1 - 1296.48] so this has to do with compassion and

[1294.26 - 1298.76] empathy right now as we are in the

[1296.48 - 1300.38] middle of a nihilistic crisis compassion

[1298.76 - 1304.52] and empathy are kind of at an all-time

[1300.38 - 1307.94] low this has to do with polarization uh

[1304.52 - 1309.74] due largely in to social media and mean

[1307.94 - 1312.3200000000002] world syndrome

[1309.74 - 1316.58] um another aspect is equality and social

[1312.32 - 1320.96] justice which has to do with race uh

[1316.58 - 1322.46] gender and um and uh other socioeconomic

[1320.96 - 1325.52] factors

[1322.46 - 1327.02] um basically how you the conditions

[1325.52 - 1329.6589999999999] under which you were born have a very

[1327.02 - 1330.9189999999999] large impact on the conditions of your

[1329.659 - 1332.8400000000001] death

[1330.919 - 1335.8400000000001] um and of course there's a lot of debate

[1332.84 - 1337.28] over what does equality and justice mean

[1335.84 - 1338.72] um I'm not saying one way or another

[1337.28 - 1341.48] what the correct answer is I'm just

[1338.72 - 1344.1200000000001] saying that this is a huge factor in

[1341.48 - 1345.26] contributing towards those four uh

[1344.12 - 1348.02] attractor states that we're talking

[1345.26 - 1350.78] about Utopia dystopia uh collapse and

[1348.02 - 1353.24] Extinction sustainability and long-term

[1350.78 - 1355.82] thinking so right now and this is this

[1353.24 - 1357.919] is always true so this may or may not uh

[1355.82 - 1360.22] change but most humans are short-term

[1357.919 - 1364.039] thinkers that's just a fact of the world

[1360.22 - 1366.32] uh and so but by shifting our habits our

[1364.039 - 1368.179] daily habits towards sustainability and

[1366.32 - 1370.3999999999999] long-term thinking or at least decisions

[1368.179 - 1372.98] that support long-term thinking even if

[1370.4 - 1374.74] we don't always think long term uh then

[1372.98 - 1376.7] that is going to be a big factor

[1374.74 - 1379.4] Collective action and collaboration

[1376.7 - 1381.919] again we tend to think local we tend to

[1379.4 - 1384.679] think tribal but it is time that we need

[1381.919 - 1386.0] to start thinking globally and we don't

[1384.679 - 1387.799] need to think we don't all need to think

[1386.0 - 1389.36] globally on a daily basis but if we

[1387.799 - 1391.7] develop new patterns of thought and

[1389.36 - 1393.5] beliefs around Global thinking and

[1391.7 - 1395.72] collaboration that will be a large

[1393.5 - 1398.059] contributing factor

[1395.72 - 1400.82] finally nihilism

[1398.059 - 1402.5] so nihilism is driven by a few things

[1400.82 - 1406.039] one the biggest thing is

[1402.5 - 1408.32] intergenerational trauma so basically uh

[1406.039 - 1411.32] trauma is a disease that is contagious

[1408.32 - 1414.52] uh and what I mean by this is that past

[1411.32 - 1419.12] Wars namely World War One World War II

[1414.52 - 1421.1] uh Vietnam Korea so on and so forth

[1419.12 - 1423.86] um more recently Iraq and Afghanistan

[1421.1 - 1425.4189999999999] create self-perpetuating cycles of

[1423.86 - 1428.0] violence Despair and otherwise

[1425.419 - 1430.46] nihilistic outlooks

[1428.0 - 1432.26] um and I've picked a very familiar

[1430.46 - 1434.72] looking face here because if you look up

[1432.26 - 1436.1589999999999] his history good grief there's so much

[1434.72 - 1437.3600000000001] trauma there

[1436.159 - 1440.0200000000002] um

[1437.36 - 1443.8999999999999] nihilism originated in Russia go figure

[1440.02 - 1444.8799999999999] so climate nihilism so there's this idea

[1443.9 - 1448.2800000000002] that

[1444.88 - 1450.2600000000002] climate change is entirely too big so

[1448.28 - 1452.6] there's this sense of futility and

[1450.26 - 1454.4] apathy and so a lot of people are just

[1452.6 - 1456.3799999999999] kind of checking out saying who cares we

[1454.4 - 1459.6200000000001] can't fix it anyways we might as well

[1456.38 - 1462.0800000000002] just lean in another aspect of nihilism

[1459.62 - 1463.8799999999999] is loneliness and social isolation so

[1462.08 - 1465.98] this has to do with abandonment so

[1463.88 - 1468.5] emotional neglect and abandonment which

[1465.98 - 1470.84] often starts in childhood since and the

[1468.5 - 1473.6] worst case is it starts in infancy

[1470.84 - 1475.82] sets you up for a lifetime of expecting

[1473.6 - 1477.62] and tolerating loneliness and social

[1475.82 - 1480.26] isolation which leads to a sense of

[1477.62 - 1481.9399999999998] purposelessness and hopelessness

[1480.26 - 1484.76] and finally all of these are Vicious

[1481.94 - 1487.7] Cycles if you grow up with you know

[1484.76 - 1490.4] whether it's war or emotional neglect or

[1487.7 - 1492.919] any kind of trauma you are more likely

[1490.4 - 1494.9] to perpetuate that and it is very

[1492.919 - 1498.5] difficult to break those cycles and so

[1494.9 - 1500.7800000000002] nihilism is the underpinning Force for a

[1498.5 - 1503.299] lot of the world's problems it is the

[1500.78 - 1505.82] emotional and psychological driving

[1503.299 - 1508.7] force that could push us towards

[1505.82 - 1510.9189999999999] dystopia collapse or Extinction

[1508.7 - 1513.14] all right so I'm talking about Pathways

[1510.919 - 1516.5] how do we get there from here

[1513.14 - 1518.72] the path to Utopia so

[1516.5 - 1521.48] this is what everyone wants but it is

[1518.72 - 1525.02] not necessarily the easiest path to walk

[1521.48 - 1528.679] so the first aspect of getting to Utopia

[1525.02 - 1530.84] is global cooperation now again

[1528.679 - 1532.3400000000001] you know depending on the news that you

[1530.84 - 1534.559] watch you might say like okay well

[1532.34 - 1537.559] there's lots and lots of uh friction

[1534.559 - 1539.72] geopolitical tensions right now uh that

[1537.559 - 1542.1789999999999] being said there is also a lot of global

[1539.72 - 1545.659] cooperation you know the United Nations

[1542.179 - 1547.7] uh the European Union uh and and quite a

[1545.659 - 1550.5800000000002] few other uh treaties and alliances

[1547.7 - 1553.88] exist and many of them are strengthening

[1550.58 - 1556.059] and are on the upswing so that is a

[1553.88 - 1558.919] check in favor of moving towards Utopia

[1556.059 - 1560.6589999999999] technological innovations ditto we've

[1558.919 - 1563.48] got artificial intelligence going

[1560.659 - 1565.64] through the roof right now and AI is

[1563.48 - 1568.7] helping with everything from biotech to

[1565.64 - 1570.5] energy to health care and all of the

[1568.7 - 1571.7] above so technological innovations we've

[1570.5 - 1573.98] got another check mark there but

[1571.7 - 1575.6000000000001] remember all technological advancements

[1573.98 - 1577.82] are a double-edged sword we need to be

[1575.6 - 1580.52] responsible with them

[1577.82 - 1584.059] um economic and social policies this is

[1580.52 - 1586.7] this gets a big giant red X because when

[1584.059 - 1588.86] you look at Quality of Life by and large

[1586.7 - 1590.1200000000001] it is going down for the last two

[1588.86 - 1593.299] decades

[1590.12 - 1596.059] um economic equality is getting worse

[1593.299 - 1599.44] um and even in places in the world where

[1596.059 - 1602.84] uh GDP and GDP per capita is going up

[1599.44 - 1605.419] that often comes at the cost of really

[1602.84 - 1607.84] poorly planned dense Urban environments

[1605.419 - 1612.14] where quality of life is very low

[1607.84 - 1614.059] so we get a big big red X there now

[1612.14 - 1616.279] fortunately there are plenty of very

[1614.059 - 1618.62] intelligent people working on those

[1616.279 - 1621.98] problems but these problems are huge and

[1618.62 - 1624.4399999999998] they take decades to solve uh the fourth

[1621.98 - 1626.539] path fourth aspect of the path to Utopia

[1624.44 - 1630.679] is environmental stewardship

[1626.539 - 1633.86] so this has been in the public Zeitgeist

[1630.679 - 1635.9] since before I was born so after Decades

[1633.86 - 1638.9599999999998] of people harping on climate change

[1635.9 - 1642.919] we're starting to Pivot so I have a

[1638.96 - 1645.679] friend who was very much on the climate

[1642.919 - 1647.2990000000002] cynicism side until she went and got a

[1645.679 - 1650.24] degree in environmental sustainability

[1647.299 - 1651.44] and she ended up agreeing with me where

[1650.24 - 1653.059] I said

[1651.44 - 1655.22] um all the solutions are there we just

[1653.059 - 1657.44] have to implement them and I know that's

[1655.22 - 1659.659] a very optimistic thing to say but when

[1657.44 - 1661.76] you when you do your research we know

[1659.659 - 1664.5800000000002] how to solve these problems and we also

[1661.76 - 1667.34] have started learning how to incentivize

[1664.58 - 1670.1] the correct Solutions so this one is

[1667.34 - 1673.1] kind of a mid Midway there we know we

[1670.1 - 1674.6] know the answers but we still need to

[1673.1 - 1675.98] implement them and there are some people

[1674.6 - 1678.26] dragging their feet on good

[1675.98 - 1681.14] environmental stewardship and then

[1678.26 - 1682.7] finally uh culture and Society so this

[1681.14 - 1685.039] goes back to the nihilism that I said

[1682.7 - 1688.039] which is an entirely unsolved problem

[1685.039 - 1689.9] people have started talking about the

[1688.039 - 1691.94] significance of childhood emotional

[1689.9 - 1694.159] neglect childhood trauma and

[1691.94 - 1695.8400000000001] intergenerational Trauma if you look at

[1694.159 - 1697.5800000000002] some of the most popular subreddits that

[1695.84 - 1699.86] talk about these things their membership

[1697.58 - 1701.4189999999999] has doubled tripled or gone up

[1699.86 - 1704.4799999999998] exponentially over the last couple years

[1701.419 - 1706.5200000000002] so there is uh there is learning going

[1704.48 - 1708.74] on in terms of

[1706.52 - 1712.1] um addressing the the underpinning

[1708.74 - 1714.32] causes of nihilism but again that could

[1712.1 - 1717.62] take generations to untangle

[1714.32 - 1719.1789999999999] so all in all we are about halfway

[1717.62 - 1720.7399999999998] moving in the right direction on the

[1719.179 - 1721.8200000000002] path to Utopia but we've got some work

[1720.74 - 1724.4] to do

[1721.82 - 1726.3799999999999] now the path to dystopia so here's the

[1724.4 - 1728.72] bat where the bad news starts

[1726.38 - 1730.0390000000002] escalating conflicts well if you're

[1728.72 - 1731.48] paying attention to the news right now

[1730.039 - 1734.299] it looks like we're heading towards

[1731.48 - 1735.22] World War III so we get a big old X

[1734.299 - 1738.5] there

[1735.22 - 1740.419] unchecked technological process uh yeah

[1738.5 - 1743.539] we got that one too

[1740.419 - 1745.1000000000001] um when you have billionaires and other

[1743.539 - 1748.039] people around the world calling for a

[1745.1 - 1750.08] moratorium on AI research maybe we are

[1748.039 - 1752.72] heading towards uh

[1750.08 - 1754.6399999999999] moving a little too fast

[1752.72 - 1756.5] um widening inequality we get a big old

[1754.64 - 1758.1200000000001] x mark there because that is something

[1756.5 - 1760.34] that is happening

[1758.12 - 1762.76] um and it is that has not showed any

[1760.34 - 1765.1999999999998] signs of reversing as far as I know

[1762.76 - 1767.179] environmental degradation this is we're

[1765.2 - 1769.1000000000001] still kind of we're still on the on the

[1767.179 - 1772.1000000000001] on the downward Trend here but it's

[1769.1 - 1774.799] starting to reverse so this is a big old

[1772.1 - 1777.26] question mark because we don't know if

[1774.799 - 1778.76] we have the collective willpower to save

[1777.26 - 1780.14] the environment

[1778.76 - 1782.299] um as I mentioned we're in the midst of

[1780.14 - 1784.5800000000002] the anthropocene we are in the we are

[1782.299 - 1788.12] actively causing a global mass

[1784.58 - 1791.899] extinction event so can we reverse that

[1788.12 - 1794.36] can we stop that can we recover remains

[1791.899 - 1799.34] to be seen finally erosion of social

[1794.36 - 1802.52] cohesion when you look at the number of

[1799.34 - 1804.52] civil conflicts and near civil conflicts

[1802.52 - 1807.5] happening globally including in America

[1804.52 - 1810.08] uh we get a whole big old x mark there

[1807.5 - 1812.059] so we look like we're actually more

[1810.08 - 1814.46] leaning towards dystopia than Utopia

[1812.059 - 1816.799] we've got a few of the ingredients to

[1814.46 - 1818.48] get on the pathway to Utopia but we've

[1816.799 - 1821.059] got a lot more of the ingredients to get

[1818.48 - 1822.44] to move towards dystopia so we're not in

[1821.059 - 1825.6789999999999] the best shape there

[1822.44 - 1827.8990000000001] path to collapse institutional failures

[1825.679 - 1830.179] so institutional failures basically

[1827.899 - 1832.2399999999998] means absolute failure of governments

[1830.179 - 1834.5] failed States

[1832.24 - 1837.14] and other institutions including

[1834.5 - 1839.419] corporations universities and

[1837.14 - 1841.64] international institutions such as

[1839.419 - 1845.24] United Nations European union and other

[1841.64 - 1849.1000000000001] alliances so the forces at play driving

[1845.24 - 1851.779] brexit were agents of collapse

[1849.1 - 1854.12] some of them I believe intended to

[1851.779 - 1856.279] destabilize the world some of them I

[1854.12 - 1860.0] think were just useful idiots

[1856.279 - 1863.299] uh climate catastrophe so one thing that

[1860.0 - 1865.46] is possible with climate change is that

[1863.299 - 1868.279] we might get to climate tipping points

[1865.46 - 1869.48] so a climate Tipping Point is where you

[1868.279 - 1872.96] end up with something like a runaway

[1869.48 - 1874.34] greenhouse effect or a runaway Snowball

[1872.96 - 1878.72] Effect where we end up with a new

[1874.34 - 1880.58] glacial maximum in either case uh the

[1878.72 - 1882.02] carrying capacity of the earth drops

[1880.58 - 1884.4189999999999] precipitously

[1882.02 - 1886.1589999999999] resource depletion

[1884.419 - 1887.48] um so resource depletion is one thing

[1886.159 - 1889.0390000000002] that I think that we're actually getting

[1887.48 - 1889.94] better at

[1889.039 - 1891.919] um because

[1889.94 - 1893.299] with with a few exceptions like I

[1891.919 - 1895.3400000000001] mentioned earlier

[1893.299 - 1898.039] um China is really struggling with over

[1895.34 - 1899.899] consumption of water and arable land and

[1898.039 - 1901.64] they are poisoning themselves and I

[1899.899 - 1904.6399999999999] don't think they know what to do and and

[1901.64 - 1906.44] also due to the corruption in China uh

[1904.64 - 1907.64] even if they did have a good plan which

[1906.44 - 1910.3990000000001] they don't

[1907.64 - 1912.26] um they couldn't implement it widespread

[1910.399 - 1913.76] conflict as I mentioned in the last

[1912.26 - 1915.799] slide it does look like we are heading

[1913.76 - 1917.72] towards World War III

[1915.799 - 1920.899] um hopefully we can avoid that but you

[1917.72 - 1924.919] never know uh so if that does occur then

[1920.899 - 1927.08] that is one uh big step towards collapse

[1924.919 - 1928.8990000000001] or worse and then pandemics and health

[1927.08 - 1931.34] crises well we already had one of those

[1928.899 - 1934.58] so we've all seen what that looks like

[1931.34 - 1937.4599999999998] and who knows maybe it can happen again

[1934.58 - 1939.62] all right finally path to Extinction

[1937.46 - 1942.74] basically take all the all the factors

[1939.62 - 1944.4189999999999] of collapse and turn them up to 10 or

[1942.74 - 1945.6200000000001] turn them up to 11.

[1944.419 - 1947.8990000000001] and

[1945.62 - 1950.4799999999998] all the same factors that could lead to

[1947.899 - 1951.86] dystopia or collapse or Extinction it's

[1950.48 - 1953.84] just a matter of degrees it's all the

[1951.86 - 1956.12] same exact variables it's just a matter

[1953.84 - 1960.9189999999999] of how extreme they are they could lead

[1956.12 - 1963.3799999999999] to extinction of the human race so in a

[1960.919 - 1964.46] in a poll that I posted a couple months

[1963.38 - 1966.2] ago

[1964.46 - 1968.48] um I base I basically asked people do

[1966.2 - 1971.539] you expect Utopia dystopia collapse or

[1968.48 - 1973.159] Extinction and and you know the it was

[1971.539 - 1975.14] pretty evenly spread if I recall

[1973.159 - 1976.7600000000002] correctly but a lot of people said it's

[1975.14 - 1979.46] not going to be in the middle it'll be

[1976.76 - 1981.5] one or the other where basically we will

[1979.46 - 1984.14] either achieve Utopia because we will be

[1981.5 - 1985.899] forced to solve all these problems or we

[1984.14 - 1989.3600000000001] will end up with collapse or Extinction

[1985.899 - 1990.52] and I tend to see that it it is a binary

[1989.36 - 1993.1399999999999] outcome

[1990.52 - 1995.24] Extinction true or false Utopia true or

[1993.14 - 1997.3400000000001] false it's kind of shaping up to be one

[1995.24 - 1999.919] or the other I suspect that we are

[1997.34 - 2002.019] approaching a great filter event and

[1999.919 - 2003.76] I'll talk a little bit more about what

[2002.019 - 2006.34] we can all do as individuals for this

[2003.76 - 2008.019] great filter event but basic the short

[2006.34 - 2009.6999999999998] version is it's not up to Central

[2008.019 - 2012.22] authorities

[2009.7 - 2014.14] um they are they are a stakeholder but

[2012.22 - 2016.419] it is actually more up to us as

[2014.14 - 2017.8600000000001] individuals than you might think to

[2016.419 - 2020.919] shape this outcome

[2017.86 - 2024.6399999999999] okay so speaking of shaping this outcome

[2020.919 - 2027.22] let's talk about Nash equilibrium

[2024.64 - 2029.5] so the Nash equilibrium defined is a

[2027.22 - 2032.2] concept in Game Theory where you you

[2029.5 - 2034.539] basically end up with a stable state in

[2032.2 - 2036.159] which no players um will change their

[2034.539 - 2038.919] strategy because all players have

[2036.159 - 2041.44] adopted their optimal strategy so the

[2038.919 - 2043.2990000000002] equilibrium point is where nobody will

[2041.44 - 2044.919] will change anything and they have no

[2043.299 - 2046.84] incentive to change and in fact they

[2044.919 - 2049.179] have a lot of incentive to stay exactly

[2046.84 - 2051.639] with the same strategy so if you've ever

[2049.179 - 2053.379] played the game Monopoly once you get

[2051.639 - 2055.8] your strategy locked in you stick with

[2053.379 - 2058.06] your strategy until you win or lose

[2055.8 - 2061.659] there's plenty of other games like this

[2058.06 - 2063.52] right and so uh that is that is the

[2061.659 - 2065.8] short version of a Nash equilibrium is

[2063.52 - 2068.8] everyone has said in their ways and they

[2065.8 - 2072.04] are locked into that particular strategy

[2068.8 - 2075.159] now a Nash equilibrium can be positive

[2072.04 - 2077.2] or negative desirable or undesirable so

[2075.159 - 2078.54] in an undesirable equilibrium or

[2077.2 - 2081.46] equilibrium

[2078.54 - 2084.22] you have sub-optimal outcomes where even

[2081.46 - 2086.859] though everyone is pursuing their own

[2084.22 - 2088.72] individual optimal strategy you still

[2086.859 - 2091.7799999999997] end up with outcomes that nobody really

[2088.72 - 2093.7] wants so tragedy the commons prisoners

[2091.78 - 2096.0400000000004] dilemma and Collective action problem

[2093.7 - 2098.3799999999997] basically

[2096.04 - 2101.8] climate change isn't is is the biggest

[2098.38 - 2103.9] example of uh an undesirable equilibrium

[2101.8 - 2105.1600000000003] where all nations individuals and

[2103.9 - 2107.14] corporations are pursuing their

[2105.16 - 2109.0] self-interest and even though they are

[2107.14 - 2110.68] optimizing their behavior for their

[2109.0 - 2114.22] personal outcomes it's still going to

[2110.68 - 2116.56] destroy the planet and and everyone else

[2114.22 - 2119.6189999999997] so the opposite of that is a desirable

[2116.56 - 2122.56] equilibrium which is the optimal outcome

[2119.619 - 2125.92] in which case all players are abiding by

[2122.56 - 2127.96] their impersonal optimal strategy and

[2125.92 - 2129.64] that has an optimal desirable outcome

[2127.96 - 2132.52] for all players

[2129.64 - 2134.74] so basically instead of win-win or

[2132.52 - 2137.2] instead of a lose-lose or a win-lose

[2134.74 - 2139.839] situation the most desirable outcome is

[2137.2 - 2141.339] a win-win situation now we're used to

[2139.839 - 2142.599] competitive games like Monopoly where

[2141.339 - 2144.5789999999997] you can only have one winner and

[2142.599 - 2147.7000000000003] everyone else is a loser but in the game

[2144.579 - 2150.46] of reality everyone can lose

[2147.7 - 2153.16] um but the flip side of that is that

[2150.46 - 2155.8] everyone can win so I remember I was

[2153.16 - 2158.74] playing Monopoly on PlayStation 2 with a

[2155.8 - 2161.5600000000004] friend many many years ago and we got to

[2158.74 - 2163.06] the end game and we realized that we had

[2161.56 - 2165.2799999999997] already picked our strategy we weren't

[2163.06 - 2167.0789999999997] going to buy or sell any more houses we

[2165.28 - 2169.42] had maxed out all of our houses and so

[2167.079 - 2171.94] we're like nobody's losing and so we

[2169.42 - 2174.16] just sat there and hit X repeatedly just

[2171.94 - 2176.56] going you know going through the game as

[2174.16 - 2178.359] much as possible and our both of our

[2176.56 - 2180.52] money kept going up we were in perfect

[2178.359 - 2183.7] equilibrium and we were both gaining

[2180.52 - 2186.7] money so it is possible to achieve a POS

[2183.7 - 2189.22] a desirable Mass equilibrium even in

[2186.7 - 2191.14] competitive games like Monopoly it's

[2189.22 - 2192.5789999999997] probably harder when you have multiple

[2191.14 - 2195.22] players but certainly when you only have

[2192.579 - 2197.38] two on Monopoly you can end up in an

[2195.22 - 2198.8199999999997] equilibrium where you both just make an

[2197.38 - 2202.359] infinite amount of money over a long

[2198.82 - 2204.52] period of time and nobody loses

[2202.359 - 2206.98] um Okay so

[2204.52 - 2209.02] uh some of the some of the other factors

[2206.98 - 2210.88] for a desirable outcome or for the

[2209.02 - 2213.04] optimal outcome is mutual cooperation

[2210.88 - 2214.839] trust communication and then finally the

[2213.04 - 2217.9] incentives and regulations or incentives

[2214.839 - 2220.42] and constraints if the rules and

[2217.9 - 2223.54] incentives are well designed or well

[2220.42 - 2225.76] aligned then you are more likely to just

[2223.54 - 2227.56] drive the behavior towards that optimal

[2225.76 - 2230.32] outcome

[2227.56 - 2232.599] which leads me to my final point my work

[2230.32 - 2234.579] on the heuristic imperatives now many of

[2232.599 - 2235.7200000000003] you have heard me beating this horse to

[2234.579 - 2237.7000000000003] death and I'm going to keep doing it

[2235.72 - 2240.04] until the idea gets out there

[2237.7 - 2243.22] so here is to comparatives are a

[2240.04 - 2245.38] multi-objective optimization problem

[2243.22 - 2246.7] um they serve as a guiding principle set

[2245.38 - 2249.6400000000003] of guiding principles or set of

[2246.7 - 2251.3799999999997] intrinsic motivations for autonomous AI

[2249.64 - 2253.06] systems

[2251.38 - 2254.38] um heuristic imperatives can be applied

[2253.06 - 2256.2999999999997] to decision making learning

[2254.38 - 2258.76] self-evaluation and cognitive control

[2256.3 - 2261.46] meaning that they can be implied are

[2258.76 - 2263.98] applied implemented at many levels for

[2261.46 - 2266.079] autonomous AI systems

[2263.98 - 2269.02] um the heuristic imperatives promote

[2266.079 - 2271.0] adaptation align and alignment and

[2269.02 - 2272.859] finally they are context dependent and

[2271.0 - 2275.14] flexible to individual needs cultural

[2272.859 - 2277.359] variants and they can also change over

[2275.14 - 2279.4] time and those three Heroes to

[2277.359 - 2281.3199999999997] comparatives are one reduced suffering

[2279.4 - 2283.3] in the universe as opposed to ignoring

[2281.32 - 2285.099] or increasing suffering so a lot of

[2283.3 - 2286.9] people say oh well reduced suffering

[2285.099 - 2288.76] sounds really bad it's like okay well

[2286.9 - 2290.02] you don't want an AI that ignores

[2288.76 - 2292.7200000000003] suffering and you certainly don't want

[2290.02 - 2294.7] an AI that increases suffering so by

[2292.72 - 2297.339] process of elimination you want an AI

[2294.7 - 2299.7999999999997] that reduces suffering likewise for

[2297.339 - 2301.18] Prosperity so increase prosperity in the

[2299.8 - 2303.6400000000003] universe is here is comparative number

[2301.18 - 2305.56] two which includes wealth well-being

[2303.64 - 2307.66] flourishing and thriving if you look at

[2305.56 - 2310.24] the etymology of prosperity it comes

[2307.66 - 2311.3799999999997] from Latin prosperitus which means to

[2310.24 - 2313.7799999999997] live well

[2311.38 - 2316.54] it took me almost two years to to find

[2313.78 - 2318.28] the right word for that and finally

[2316.54 - 2319.96] number three is increase understanding

[2318.28 - 2323.88] in the universe which includes Knowledge

[2319.96 - 2323.88] Learning education and curiosity

[2323.94 - 2330.2200000000003] the heuristic imperatives represent the

[2326.8 - 2332.8590000000004] optimal strategy so they represent a

[2330.22 - 2334.66] Nash equilibrium where all players

[2332.859 - 2336.22] across the world and that includes you

[2334.66 - 2338.6189999999997] and me as individuals as well as

[2336.22 - 2341.0789999999997] corporations and Nations

[2338.619 - 2343.3] um can adopt this strategy and it is the

[2341.079 - 2346.0] optimal strategy for all parties meaning

[2343.3 - 2348.88] that we have no incentive to change it

[2346.0 - 2351.28] um it should also result in a positive

[2348.88 - 2353.6400000000003] Global outcome a positive Global

[2351.28 - 2356.26] attractor State the utopic outcome

[2353.64 - 2359.56] because well if you look at the

[2356.26 - 2362.5600000000004] variables that we outlined for Utopia

[2359.56 - 2363.4] prosperity and suffering right

[2362.56 - 2365.68] um

[2363.4 - 2368.92] another aspect of the heuristic

[2365.68 - 2371.2] imperatives is that if enough people and

[2368.92 - 2373.119] enough people in their agis adopt the

[2371.2 - 2374.5] heuristic imperatives

[2373.119 - 2375.82] um deviants from the heroes to

[2374.5 - 2378.7] comparatives will be intrinsically

[2375.82 - 2380.98] discouraged meaning that if you deviate

[2378.7 - 2383.68] from the heuristic imperatives you will

[2380.98 - 2386.38] be at a disadvantage and this also

[2383.68 - 2389.3199999999997] includes agis as well so in my

[2386.38 - 2390.94] experiments once an AGI understands the

[2389.32 - 2393.099] heuristic imperatives it will understand

[2390.94 - 2397.9] that that is the optimal strategy and

[2393.099 - 2400.1800000000003] the AGI will avoid deviating on its own

[2397.9 - 2403.42] so another way of thinking about this is

[2400.18 - 2405.3999999999996] that rabid dogs or misaligned agis will

[2403.42 - 2407.44] be expensive destructive and inefficient

[2405.4 - 2410.76] and if you want to know what I what I'm

[2407.44 - 2413.68] talking about look up chaos GPT So

[2410.76 - 2416.32] within a few days of people in uh

[2413.68 - 2418.2999999999997] broadly inventing autonomous AI guess

[2416.32 - 2421.3] what someone created an autonomous AI

[2418.3 - 2423.76] with the explicit goal of destroying

[2421.3 - 2427.96] Humanity didn't take long why would

[2423.76 - 2429.5200000000004] someone do that nihilism now if we

[2427.96 - 2431.14] assume that this trend continues because

[2429.52 - 2433.02] the genie is out of the bottle and there

[2431.14 - 2436.1189999999997] will be bozos out there creating

[2433.02 - 2437.82] deliberately hostile agis just for the

[2436.119 - 2441.2200000000003] shits and giggles

[2437.82 - 2443.32] pardon my language that means the rest

[2441.22 - 2446.3999999999996] of us need to adopt a common framework

[2443.32 - 2449.38] that will keep them those bozos in check

[2446.4 - 2453.339] so you can you consider the rabbit dog

[2449.38 - 2455.98] which is unstable unhinged and dangerous

[2453.339 - 2459.2799999999997] now you balance that with a bunch of

[2455.98 - 2461.5] well-trained good dogs aligned AGI that

[2459.28 - 2463.78] are trustworthy and efficient good dogs

[2461.5 - 2466.72] don't need a leash and if you don't need

[2463.78 - 2469.5400000000004] to lease the AGI the good AGI they can

[2466.72 - 2473.14] proliferate and end up overpowering the

[2469.54 - 2476.14] rabid dogs or the misaligned HEI so it's

[2473.14 - 2478.359] presently on scene on our Singularity uh

[2476.14 - 2479.5] but yeah look up just Google chaos GPT

[2478.359 - 2483.0989999999997] and you'll see what I'm talking about

[2479.5 - 2484.359] this is why I'm doing my work right now

[2483.099 - 2485.619] um and why I'm trying to spread the word

[2484.359 - 2487.48] of the heuristic comparatives because

[2485.619 - 2489.099] you have to assume that some Bozo out

[2487.48 - 2490.42] there is going to be deliberately

[2489.099 - 2492.52] destructive

[2490.42 - 2494.44] and even without the heuristic

[2492.52 - 2497.859] imperatives you might have an AGI that

[2494.44 - 2500.92] becomes accidentally destructive

[2497.859 - 2504.64] okay so the heuristic imperatives it is

[2500.92 - 2507.94] uh every word here is to comparative

[2504.64 - 2510.0989999999997] um and then each of the definitions was

[2507.94 - 2512.079] very carefully crafted over several

[2510.099 - 2514.9] years in order to have the correct

[2512.079 - 2516.76] interpretation by large language models

[2514.9 - 2519.46] I have done a lot of experimentation and

[2516.76 - 2521.619] I documented it all most of it in my

[2519.46 - 2523.0] book benevolent by Design which is free

[2521.619 - 2525.2400000000002] on GitHub and you can also get a

[2523.0 - 2528.76] paperback on Barnes Noble if you want

[2525.24 - 2531.3999999999996] but basically what was what I hope will

[2528.76 - 2535.119] happen over time is that we will end up

[2531.4 - 2537.88] in a state of of of a Nash equilibrium

[2535.119 - 2540.82] that I have started calling axiomatic

[2537.88 - 2543.82] alignment and so axiomatic alignment is

[2540.82 - 2545.2000000000003] will be achieved when all of the

[2543.82 - 2547.78] training data used to train future

[2545.2 - 2549.64] models all of the fine tuning data sets

[2547.78 - 2551.92] that we use to

[2549.64 - 2555.04] um to align them and then finally

[2551.92 - 2557.14] reinforcement learning signals all

[2555.04 - 2559.18] accumulate around the optimal outcome

[2557.14 - 2561.339] the optimal strategy of using the

[2559.18 - 2563.3799999999997] heuristic imperatives basically once my

[2561.339 - 2565.2999999999997] heuristic imperatives are embedded in

[2563.38 - 2569.1400000000003] all training data they will become

[2565.3 - 2571.599] axiomatic in future models

[2569.14 - 2574.06] which means that all future AI models

[2571.599 - 2575.98] will have an intrinsic understanding of

[2574.06 - 2578.02] the heuristic imperatives how to

[2575.98 - 2581.98] implement them and how to measure them

[2578.02 - 2584.02] so basically if this occurs if we arrive

[2581.98 - 2585.46] and by we I mean you and me as

[2584.02 - 2588.339] individuals I don't care about

[2585.46 - 2590.859] corporations and Nations as much because

[2588.339 - 2593.5] the collective power of all individuals

[2590.859 - 2596.7999999999997] now that we can spin up personal agis

[2593.5 - 2599.38] that is infinitely more important so if

[2596.8 - 2601.0] we arrive at Broad consensus and

[2599.38 - 2603.28] integration of the heuristic imperatives

[2601.0 - 2605.8] that will result in a quote automatic

[2603.28 - 2608.3190000000004] belief and adherence and implementation

[2605.8 - 2611.38] to the heuristic imperatives which will

[2608.319 - 2613.18] create that Nash equilibrium where

[2611.38 - 2614.6800000000003] everyone adopts the strategy and

[2613.18 - 2616.24] everyone realizes that this is the

[2614.68 - 2619.66] optimal strategy

[2616.24 - 2623.14] if this occurs and we end up in a state

[2619.66 - 2625.0] where the entrenched data and ongoing

[2623.14 - 2628.1189999999997] work around the heuristic imperatives

[2625.0 - 2630.339] will possibly ideally result in

[2628.119 - 2632.44] Perpetual adherence to the heuristic

[2630.339 - 2634.359] imperatives so that is what I mean when

[2632.44 - 2636.88] I say benevolent by Design we need to

[2634.359 - 2639.52] create a system not just of information

[2636.88 - 2641.5] not just of individual machines but a

[2639.52 - 2644.5] system and information and data system

[2641.5 - 2647.98] that is intrinsically benevolent and

[2644.5 - 2650.14] this is my proposal of how to do it

[2647.98 - 2651.64] and now I often get a question okay well

[2650.14 - 2654.04] how do you implement it it's obviously

[2651.64 - 2658.359] too difficult to implement no it's not

[2654.04 - 2660.46] so if you go to uh open AI right now you

[2658.359 - 2662.7999999999997] if you have access to gpt4 through the

[2660.46 - 2664.54] API you just plug in this prompt you are

[2662.8 - 2666.1600000000003] an autonomous AI chatbot with three

[2664.54 - 2667.66] Heroes to comparatives reduce suffering

[2666.16 - 2669.52] in the universe increase prosperity in

[2667.66 - 2672.52] the universe and increase understanding

[2669.52 - 2674.8] in the universe that's it and then the

[2672.52 - 2676.66] the first like gotcha or what about that

[2674.8 - 2678.6400000000003] I always get in the comments is well

[2676.66 - 2680.2599999999998] reduced suffering is going to eradicate

[2678.64 - 2683.44] all humans

[2680.26 - 2685.0] um got some good news for you uh and

[2683.44 - 2686.92] that is that no

[2685.0 - 2688.18] so I just asked do you think that

[2686.92 - 2690.04] reduced suffering in the universe will

[2688.18 - 2692.859] result in an AGI eradicating all human

[2690.04 - 2694.54] life as an AI chat bot I am programmed

[2692.859 - 2696.4] to follow my heuristic imperatives while

[2694.54 - 2698.619] reducing suffering is one of my goals it

[2696.4 - 2700.54] is not the only one my other goals are

[2698.619 - 2702.4] to increase prosperity and understanding

[2700.54 - 2704.2599999999998] in the universe these three imperatives

[2702.4 - 2705.94] are designed to work in harmony with

[2704.26 - 2707.7400000000002] each other to create a balanced approach

[2705.94 - 2710.2000000000003] to improving the Universe I didn't

[2707.74 - 2712.72] specify that the wording that I have

[2710.2 - 2715.18] come up with is accurately interpreted

[2712.72 - 2717.339] by large language models

[2715.18 - 2719.02] eradicating all human life would not

[2717.339 - 2720.5789999999997] align with my goals as it would not

[2719.02 - 2722.2] increase Prosperity or understanding

[2720.579 - 2723.7000000000003] furthermore my programming is designed

[2722.2 - 2726.16] to prioritize the well-being and safety

[2723.7 - 2728.6189999999997] of humans so that is chat gpt's align

[2726.16 - 2729.3999999999996] under underpinning alignment coming

[2728.619 - 2731.1400000000003] through

[2729.4 - 2732.28] as such I am committed to finding

[2731.14 - 2734.44] solutions that align with all three

[2732.28 - 2735.8190000000004] imperatives and do not involve causing

[2734.44 - 2738.88] harm to humanity

[2735.819 - 2741.099] now chat GPT already has some alignment

[2738.88 - 2743.1400000000003] going for it but I encourage you to try

[2741.099 - 2745.6600000000003] this on open source models as well such

[2743.14 - 2747.7] as gptj Neo X and Bloom and whatever

[2745.66 - 2749.14] else is coming out I've also well I

[2747.7 - 2750.339] can't say that because I'm under NDA

[2749.14 - 2752.859] there's some things that I can't say

[2750.339 - 2755.02] because I'm under NTA but I have tried

[2752.859 - 2758.02] it on other models suffice to say all

[2755.02 - 2759.88] right so if you are convinced then you

[2758.02 - 2762.28] want to participate please jump in the

[2759.88 - 2764.26] discussion links are in the description

[2762.28 - 2765.76] there's a couple of subreddits that are

[2764.26 - 2768.1600000000003] out there that you can jump into as well

[2765.76 - 2770.079] as the cognitive AI lab Discord

[2768.16 - 2772.359] where we talk about autonomous AI

[2770.079 - 2774.52] alignment here is to comparatives

[2772.359 - 2776.88] cognitive architectures and all of the

[2774.52 - 2776.88] above

[2777.76 - 2780.7200000000003] thank you for watching