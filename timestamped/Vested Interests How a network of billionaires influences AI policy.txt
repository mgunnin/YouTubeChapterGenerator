[0.24 - 5.24] in a somewhat ominous turn of events

[2.919 - 8.44] this article came out yesterday on

[5.24 - 11.16] Friday the 13th and this article by

[8.44 - 13.759] Politico is a deep dive into how a

[11.16 - 16.52] billionaire backed network of advisers

[13.759 - 19.08] have completely infiltrated every level

[16.52 - 20.8] of Washington I strongly recommend you

[19.08 - 24.4] check out this article and read it it is

[20.8 - 26.279] incredibly well-sited but I will do some

[24.4 - 29.72] of the work for you and let's unpack

[26.279 - 34.16] this one piece at a time so let's go

[29.72 - 36.559] ahead and Dive Right In how effective uh

[34.16 - 38.36] altruism took over Washington and has

[36.559 - 39.879] controlled the narrative around

[38.36 - 43.64] artificial

[39.879 - 46.32] intelligence so the very high level uh

[43.64 - 48.76] of this article is basically there's a

[46.32 - 51.68] network a billionaire backed network of

[48.76 - 54.16] influencers in Washington that have

[51.68 - 56.64] basically set the policy tone and

[54.16 - 59.64] Direction now they have private

[56.64 - 61.359] interests uh as we'll unpack later with

[59.64 - 64.0] pretty pretty much every Silicon Valley

[61.359 - 67.04] Tech Giant they also have vested

[64.0 - 69.32] interests with uh members of Congress as

[67.04 - 72.88000000000001] well as various think tanks and policy

[69.32 - 76.55999999999999] advisors so this web spans pretty much

[72.88 - 79.759] the entire country uh at all levels and

[76.56 - 81.799] one thing is that they are focused on

[79.759 - 85.64] existential risk or Extinction risk from

[81.799 - 87.68] AI which on the surface that seems like

[85.64 - 89.68] okay nobody wants to go extinct so I

[87.68 - 92.36000000000001] want to frame this entire talk with it

[89.68 - 95.04] is entirely possible that everyone's

[92.36 - 97.72] intentions here are uh benevolent that

[95.04 - 100.0] everyone has the best of intentions

[97.72 - 103.079] however part of the conversation has to

[100.0 - 105.32] be do the right intentions justify the

[103.079 - 107.83999999999999] means to that end or are these

[105.32 - 109.91999999999999] intentions even well-founded so that's

[107.84 - 112.56] what we'll unpack in the rest of this

[109.92 - 116.119] video so one thing that you need to know

[112.56 - 119.24000000000001] is that the uh kind of the the core uh

[116.119 - 121.68] player of all this is a nonprofit called

[119.24 - 125.15899999999999] uh open philanthropy and so what open

[121.68 - 128.399] philanthropy does is they fund uh what

[125.159 - 130.31900000000002] they call their fellows and they also uh

[128.399 - 133.12] provide grants and research funding to

[130.319 - 134.92] all kinds of different organizations

[133.12 - 137.59900000000002] which again we'll unpack as the video

[134.92 - 140.76] unfolds but basically they have created

[137.599 - 143.48] a network of influence uh based on the

[140.76 - 146.39999999999998] ideas of effective altruism which of

[143.48 - 148.319] course has its own uh set of potential

[146.4 - 152.08] problems and conflicts of

[148.319 - 154.879] interest so the Horizon Institute uh for

[152.08 - 157.8] public service is a nonprofit that was

[154.879 - 160.44] created uh largely by the efforts of

[157.8 - 164.8] open philanthropy in 2022 so just last

[160.44 - 167.159] year uh this this group has placed uh

[164.8 - 170.84] their fellows their advisers in the

[167.159 - 172.84] offices of members of Congress now again

[170.84 - 175.159] looking at it charitably looking at it

[172.84 - 177.519] in with the lens of Good Intentions it's

[175.159 - 179.56] entirely possible that this organization

[177.519 - 182.84] is doing what we all want something like

[179.56 - 185.56] this to do which is to have uh experts

[182.84 - 187.799] speaking directly to uh senators and

[185.56 - 190.12] other members of Congress to say hey

[187.799 - 193.08] let's look at things like biocurity so

[190.12 - 195.159] you know as uh as uh other YouTubers out

[193.08 - 198.15900000000002] there such as AI explained has talk

[195.159 - 199.879] talked about that uh synthetic biology

[198.159 - 202.28] and gain of function research is one of

[199.879 - 205.44] the primary concerns about how AI could

[202.28 - 206.44] be used to harm everyone uh so basically

[205.44 - 209.0] if you're not familiar with this

[206.44 - 211.12] argument the idea is that uh really

[209.0 - 213.48] powerful and really smart chat Bots

[211.12 - 215.64000000000001] greatly lower the threshold that is of

[213.48 - 218.67999999999998] of intelligence and training that is

[215.64 - 221.27999999999997] required to learn how to do uh basically

[218.68 - 223.48000000000002] biot terrorism uh and there have been

[221.28 - 225.799] some really alarming experiments done

[223.48 - 228.11999999999998] where you take just a a typical college

[225.799 - 231.87900000000002] student give them the task to find

[228.12 - 234.64000000000001] Anthrax or whatever and chatbots have uh

[231.879 - 236.159] very willingly helped them do that uh

[234.64 - 238.2] and and even figure out what they would

[236.159 - 240.599] need to do to perform gain of function

[238.2 - 243.39999999999998] research such as what was what is now

[240.599 - 245.83999999999997] suspected by some to be the cause of the

[243.4 - 250.84] coid pandemic that we are still

[245.84 - 254.4] recovering from uh so again looking at

[250.84 - 257.359] this charitably doing a a less cynical

[254.4 - 259.68] interpretation yes we want experts that

[257.359 - 262.71999999999997] are uh influencing policy to to tell

[259.68 - 264.72] Senators hey you know maybe maybe this

[262.72 - 267.96000000000004] isn't in your in your worldview but

[264.72 - 270.88000000000005] biocurity is absolutely critical where

[267.96 - 274.0] AI is concerned but but when you look at

[270.88 - 275.56] who is who the stakeholders are this is

[274.0 - 278.0] why I'm making this video because it

[275.56 - 279.479] raises some some cause for concern some

[278.0 - 282.0] potential red

[279.479 - 283.96] flags so the Horizon fellows they

[282.0 - 286.759] basically as far as I can tell what they

[283.96 - 288.479] do is they have a crop of uh of fellows

[286.759 - 291.0] that they cultivate every year and then

[288.479 - 293.36] place them in various organizations

[291.0 - 296.24] whether it's uh nonprofit organizations

[293.36 - 298.47900000000004] think tanks or advising uh members of

[296.24 - 302.16] Congress advising Boards of directors

[298.479 - 304.75899999999996] and Advising CEO O's uh so the the line

[302.16 - 307.40000000000003] of thinking here is open philanthropy

[304.759 - 309.24] founded Horizon and Horizon is now

[307.4 - 311.919] responsible for placing these advisers

[309.24 - 313.919] now again I'm not saying that this is

[311.919 - 316.15999999999997] some Grand conspiracy where every every

[313.919 - 317.919] one of these members is is part of a

[316.16 - 319.72] cabal that is secretly trying to steer

[317.919 - 322.52] the world in the wrong direction or

[319.72 - 324.52000000000004] maximize profits for one company over

[322.52 - 326.59999999999997] another it's entirely likely that

[324.52 - 328.71999999999997] everyone involved has the best of

[326.6 - 330.56] intentions but remember intentions

[328.72 - 333.52000000000004] aren't everything you need to make sure

[330.56 - 334.8] that intentions are aligned with actions

[333.52 - 337.12] and that there's also enough

[334.8 - 339.72] transparency and accountability to make

[337.12 - 341.639] sure that that it is a collective

[339.72 - 343.28000000000003] conversation and not a tightly

[341.639 - 345.319] controlled

[343.28 - 347.039] narrative so some of the think tanks

[345.319 - 349.56] that are involved here the two primary

[347.039 - 351.28] ones are Rand Corporation and Georgetown

[349.56 - 354.52] University Center for security and

[351.28 - 357.96] emergency emerging technology CET for

[354.52 - 360.59999999999997] short uh and so these are uh two very

[357.96 - 362.4] influential and prestigious think tanks

[360.6 - 366.03900000000004] that have a tremendous amount of pre

[362.4 - 369.479] presence in Washington DC directly um

[366.039 - 371.599] both of these insist that they maintain

[369.479 - 373.75899999999996] uh neutrality that they just you know

[371.599 - 375.8] advise the best of they that they can

[373.759 - 377.68] but as is often proved out in in the

[375.8 - 379.479] course of history you do have to follow

[377.68 - 382.479] the money because money often comes with

[379.479 - 384.599] strings and influence uh even so much as

[382.479 - 387.56] just saying which projects get funded

[384.599 - 390.319] even if the think tank is then still

[387.56 - 392.039] operating um autonomously

[390.319 - 393.72] uh just by virtue of well we're going to

[392.039 - 395.88] fund This research and not another piece

[393.72 - 397.91900000000004] of research this is the kind of thing

[395.88 - 400.71999999999997] which is why particularly in America we

[397.919 - 403.12] have very low trust with think tanks and

[400.72 - 405.639] even a lot of scientific research so for

[403.12 - 408.12] instance uh within the food industry

[405.639 - 411.28000000000003] food lobbies have uh throughout history

[408.12 - 413.8] funded research that uh specifically

[411.28 - 416.08] aimed to support their means so for

[413.8 - 418.199] instance the beef Lobby funded research

[416.08 - 420.87899999999996] that said actually it's high fructose

[418.199 - 423.28000000000003] corn syrup is the enemy here here um and

[420.879 - 425.24] vice versa the corn the the corn Lobby

[423.28 - 427.11999999999995] has funded research saying actually it's

[425.24 - 429.68] saturated fat that is the enemy of

[427.12 - 431.44] everyone so when you understand the

[429.68 - 433.8] financial motivations behind some of

[431.44 - 437.08] these things and the long chain of web

[433.8 - 439.52000000000004] uh of influence it makes more sense why

[437.08 - 441.44] we have so much misinformation and the

[439.52 - 443.08] weaponization of Science Now I need to

[441.44 - 446.039] be very clear and I'm not I'm not

[443.08 - 448.28] accusing CET or Rand of doing this all

[446.039 - 450.12] I'm saying is that there is very strong

[448.28 - 453.19899999999996] historical precedent

[450.12 - 456.199] uh particularly in America of think

[453.199 - 459.56] tanks being weaponized even unwittingly

[456.199 - 462.599] in many cases they are doing their best

[459.56 - 464.68] uh science that they can but the people

[462.599 - 466.639] who hold the pur strings have undue

[464.68 - 468.639] influence again I don't have any

[466.639 - 469.879] evidence that that has happened here but

[468.639 - 471.96000000000004] it's something that we need to keep in

[469.879 - 475.44] mind because history has a tendency to

[471.96 - 477.31899999999996] repeat itself so effective altruism is

[475.44 - 479.52] kind of the underpinning philosophy as

[477.319 - 481.159] long as as well as long-termism

[479.52 - 482.479] and if you're a fan of my Channel or if

[481.159 - 484.759] you've been here before you probably

[482.479 - 487.599] know that I am uh highly critical of

[484.759 - 490.47900000000004] long-termism because it places a higher

[487.599 - 492.199] priority on the numerical superiority of

[490.479 - 495.28] potential future humans which may or may

[492.199 - 498.56] not ever exist and and places a higher

[495.28 - 502.479] moral and ethical burden upon us today

[498.56 - 503.68] basically it it enslaves us today uh

[502.479 - 506.08] based on saying well there might be

[503.68 - 508.28000000000003] trillions of humans in the future and so

[506.08 - 512.1999999999999] we owe them the right to exist and we

[508.28 - 514.68] owe them a better future and so the um

[512.2 - 516.32] the the long- termist philosophy then

[514.68 - 519.159] has been translated into effective

[516.32 - 522.519] altruism which basically says that uh

[519.159 - 524.8389999999999] altruism must be datadriven and it must

[522.519 - 526.44] therefore also maximize the chances of

[524.839 - 528.36] creating trillions of humans in the

[526.44 - 531.0400000000001] future it says nothing about the quality

[528.36 - 532.6] of life of humans today or even the

[531.04 - 535.519] quality of life of those humans in the

[532.6 - 538.88] future just if you if you say one human

[535.519 - 540.5600000000001] life is always worth you know n of one

[538.88 - 542.8389999999999] and you say okay well there's 8 billion

[540.56 - 545.88] humans today so therefore the moral

[542.839 - 547.48] authority of humanity today is 8 billion

[545.88 - 549.72] and there's potentially 100 trillion

[547.48 - 552.279] humans in the future the moral authority

[549.72 - 553.8000000000001] of those non-existent future humans

[552.279 - 556.24] drastically outweighs the moral

[553.8 - 560.12] authority of humans today and so this is

[556.24 - 562.0790000000001] a really kind of backwards philosophy um

[560.12 - 564.0] that I have not actually talked to any

[562.079 - 566.3599999999999] philosopher any actual trained

[564.0 - 568.0] philosopher who agrees with this um if

[566.36 - 569.8000000000001] there are philosophy Majors like people

[568.0 - 572.88] with a PhD in philosophy

[569.8 - 574.519] who agree with this I'm happy to like

[572.88 - 576.88] engage you in the comments or talk talk

[574.519 - 579.519] to you on LinkedIn because if I'm wrong

[576.88 - 581.32] I'd like to understand it but having

[579.519 - 583.32] studied philosophy and ethics

[581.32 - 586.24] particularly deontological ethics and

[583.32 - 589.6800000000001] Theological ethics if effective altruism

[586.24 - 591.48] and long-termism is basically te teic

[589.68 - 593.279] ethics on steroids so if you're not

[591.48 - 596.12] familiar with the term theological

[593.279 - 599.2] ethics means outcome based that the that

[596.12 - 601.32] the ultimate result is the final arbiter

[599.2 - 603.6800000000001] of what is right and good and so

[601.32 - 607.9200000000001] long-termism and effective altruism is

[603.68 - 611.0] basically saying let's let's spam te

[607.92 - 614.399] theological thought and just only go in

[611.0 - 616.8] on te good grief sorry theological

[614.399 - 619.04] thought and uh and completely ignore

[616.8 - 621.24] deontological ethics and deontological

[619.04 - 623.88] ethics is Duty based ethics or

[621.24 - 626.279] intention-based ethics which basically

[623.88 - 629.2] if you have a Duty or you're you're

[626.279 - 631.36] required to adhere to certain virtues um

[629.2 - 633.0400000000001] you need to say well okay whatever the

[631.36 - 635.36] intention or whatever the long-term

[633.04 - 637.24] outcome is the immediate attention and

[635.36 - 639.6] immediate impact is actually what's more

[637.24 - 641.48] important and so I've studied both of

[639.6 - 644.0400000000001] these in the pursuit of creating my

[641.48 - 647.36] heris imperatives and studying the way

[644.04 - 649.8389999999999] that we should uh basically build AI to

[647.36 - 651.76] remain benevolent and truly altruistic

[649.839 - 655.639] and so essentially this is a very

[651.76 - 657.639] lopsided philosophy long-termism is pure

[655.639 - 659.88] teic no

[657.639 - 661.639] deontological it says that our only duty

[659.88 - 663.36] is to people that don't exist and that

[661.639 - 665.16] the ultimate outcome of ensuring that

[663.36 - 666.76] hundreds of trillions of humans exist in

[665.16 - 669.92] the future is the only thing that

[666.76 - 671.92] matters now obviously you can say like

[669.92 - 674.279] well yeah on the surface that's that's

[671.92 - 677.8] maybe a good idea because if a 100

[674.279 - 679.519] trillion humans exist in the future then

[677.8 - 681.639] then we've succeeded then like they can

[679.519 - 683.04] figure out their own happiness and but

[681.639 - 685.76] in the meantime we will not have gone

[683.04 - 687.639] extinct but again good intentions often

[685.76 - 689.8] pave the path to hell and the ends don't

[687.639 - 692.12] always justify the means which is why we

[689.8 - 695.92] also need virtue ethics or deontological

[692.12 - 698.12] ethics as well as theological ethics so

[695.92 - 701.5999999999999] another person mentioned in this uh

[698.12 - 704.32] article is Deborah Raji and so she is at

[701.6 - 706.639] the at UC Berkeley which is a famous

[704.32 - 709.36] university um that has been involved in

[706.639 - 711.519] technology and AI for a long time um she

[709.36 - 714.48] brought up very specific concerns about

[711.519 - 716.92] the focus and specifically about how

[714.48 - 719.16] this undue influence is basically saying

[716.92 - 721.76] okay well billionaires are set setting

[719.16 - 724.399] the narrative not research universities

[721.76 - 727.92] not people from Stanford and MIT and

[724.399 - 730.72] Harvard and Yale and you know UC Davis

[727.92 - 733.0] and UC Berkeley basically it's it the

[730.72 - 736.0] the narrative is entirely lopsided and

[733.0 - 739.079] so because of how much access and

[736.0 - 740.68] influence money buys you we're basically

[739.079 - 742.76] kind of glossing over what a lot of

[740.68 - 744.76] researchers are saying um and I'll go

[742.76 - 747.04] into a little bit more about what what

[744.76 - 749.8389999999999] the researchers want in just a moment uh

[747.04 - 752.079] but the point being is that

[749.839 - 755.32] the people who truly have no vested

[752.079 - 758.04] interest the people who are studying it

[755.32 - 760.639] science for science's sake are largely

[758.04 - 762.959] being ignored or at least undervalued

[760.639 - 765.6] whereas people such as billionaires and

[762.959 - 768.0] CEOs who do have vested interest in the

[765.6 - 771.12] outcomes particularly for their own

[768.0 - 774.16] power and control um they their voice is

[771.12 - 776.199] outsized and magnified by this pattern

[774.16 - 778.12] again I think that everyone I I

[776.199 - 781.0] personally honestly believe that

[778.12 - 784.44] everyone largely at least has conscious

[781.0 - 786.24] uh good intentions um but we'll unpack

[784.44 - 788.1600000000001] conscious intentions versus unconscious

[786.24 - 790.199] motivations in a minute as well as

[788.16 - 793.56] actions speaking louder than

[790.199 - 795.92] words now even if everyone has

[793.56 - 798.3599999999999] benevolent intentions there are very

[795.92 - 801.04] obvious conflicts of interest and that

[798.36 - 803.92] is that uh organizations like open

[801.04 - 806.079] philanthropy like Horizon because they

[803.92 - 808.92] have their fingers in multiple pies they

[806.079 - 811.399] have ties to Amazon open AI Microsoft

[808.92 - 812.88] Google they have ties to Congress this

[811.399 - 814.8] almost seems like it is just a

[812.88 - 817.6] systematized version of back room

[814.8 - 819.88] dealing and I don't think I really have

[817.6 - 822.5600000000001] to say any more other than transparency

[819.88 - 824.639] transparency transparency the number one

[822.56 - 827.079] way to avoid corruption the number one

[824.639 - 829.9590000000001] way to avoid regulatory capture is

[827.079 - 832.5999999999999] transparency and accountability and

[829.959 - 835.56] these privately funded organizations

[832.6 - 838.24] that are not beholden to voters uh this

[835.56 - 839.7589999999999] is a major problem with transparency

[838.24 - 841.72] which is why I'm making this video and

[839.759 - 845.639] it's why that article was

[841.72 - 847.6] written so the key thing here is that uh

[845.639 - 849.72] open philanthropy and the Horizon

[847.6 - 852.0] Institute are focused exclusive almost

[849.72 - 854.36] exclusively on existential threats

[852.0 - 856.68] biocurity such as you know AI being able

[854.36 - 859.6800000000001] to either uh break out of the lab and

[856.68 - 862.5999999999999] kill everyone or AI being used to uh

[859.68 - 865.519] create weaponized uh bioweapons again

[862.6 - 868.1990000000001] these are real things but we also need

[865.519 - 870.6] to not ignore short-term uh and

[868.199 - 873.399] immediate abuses such as surveillance

[870.6 - 876.16] states such as uh algorithmic bias and

[873.399 - 878.12] things I've made videos recently where I

[876.16 - 880.16] believe I have collected and seen enough

[878.12 - 882.279] evidence and of course there's also uh

[880.16 - 885.7199999999999] literally thousands of of my viewers who

[882.279 - 887.519] agree that AI companies seem to be

[885.72 - 892.1600000000001] deliberately and intentionally causing

[887.519 - 894.199] their AI to lie or deceive which in some

[892.16 - 895.92] cases it could just be algorithmic flaws

[894.199 - 898.3199999999999] in other cases it could be alignment or

[895.92 - 901.24] guard rails but either way all of this

[898.32 - 903.0400000000001] is happening behind closed doors whether

[901.24 - 906.32] or not the intentions are deliberate or

[903.04 - 907.759] conscious or benevolent uh basically

[906.32 - 909.9200000000001] they they're kind of excluding

[907.759 - 911.5600000000001] everything else and saying well the ends

[909.92 - 913.8389999999999] justify the means so we're going to

[911.56 - 915.279] focus on existential threats only and

[913.839 - 919.0400000000001] we're going to set the tone and the

[915.279 - 921.8389999999999] decision on how to approach this and

[919.04 - 924.68] maybe billion billion dollar you know

[921.839 - 928.1600000000001] startups are not the correct entity to

[924.68 - 929.7589999999999] be focusing on uh ethics and morality

[928.16 - 932.399] maybe the are not the correct entities

[929.759 - 934.48] to be focusing on uh preserving the

[932.399 - 936.44] human race maybe this needs to be a

[934.48 - 937.88] collective conversation maybe the

[936.44 - 940.0400000000001] government needs to be more involved

[937.88 - 941.92] maybe we all need to be more involved

[940.04 - 944.399] and so the idea here is not I'm not

[941.92 - 946.56] saying that they're wrong obviously we

[944.399 - 948.0] all don't want to go extinct like I like

[946.56 - 949.5999999999999] I've said in previous videos there are

[948.0 - 952.319] some conditions that I would rather go

[949.6 - 954.0790000000001] extinct like being enslaved to the Borg

[952.319 - 955.639] but by and large I think we all agree

[954.079 - 957.16] that we don't want to go extinct so I'm

[955.639 - 959.24] not saying that they're fundamentally

[957.16 - 960.759] wrong for wanting to f focus on these

[959.24 - 963.04] things what I am saying that they're

[960.759 - 964.9590000000001] wrong about is doing it all behind

[963.04 - 967.759] closed doors and that they are

[964.959 - 970.6389999999999] dominating the conversation uh this is

[967.759 - 972.12] not how you come to Collective consensus

[970.639 - 975.48] on something that affects us as an

[972.12 - 977.68] entire species so that is the that is

[975.48 - 980.24] the key thing here hey everyone future

[977.68 - 981.959] Dave here again I got to say it's not

[980.24 - 985.04] getting better it's still getting worse

[981.959 - 988.319] I need your help we need to speak up and

[985.04 - 991.04] act now do something while you still can

[988.319 - 993.4399999999999] and furthermore if you can help me any

[991.04 - 995.7199999999999] way in your power you can like And

[993.44 - 998.0400000000001] subscribe this video you can comment and

[995.72 - 999.88] share you can also support me on patreon

[998.04 - 1001.68] and I also have a brand new substack so

[999.88 - 1004.0] that you can stay up toate with

[1001.68 - 1005.04] everything going on thanks I'm about out

[1004.0 - 1007.24] of

[1005.04 - 1009.7199999999999] time uh Sam Alman is mentioned

[1007.24 - 1013.44] extensively in this article um he's the

[1009.72 - 1015.1990000000001] CEO of open Ai and he um has uh

[1013.44 - 1017.0790000000001] specifically said like maybe we need to

[1015.199 - 1020.16] do licensing and I've been back and

[1017.079 - 1022.8389999999999] forth on licensing uh you know I've been

[1020.16 - 1026.12] uh vehemently in in advocation of no

[1022.839 - 1029.319] licensing do everything open source uh

[1026.12 - 1032.2399999999998] Sue companies or find companies that uh

[1029.319 - 1034.72] that you know violate copyright that

[1032.24 - 1036.6] cause their AI to lie that sort of stuff

[1034.72 - 1038.919] and that I honestly and still sincerely

[1036.6 - 1041.319] believe that open source subverts many

[1038.919 - 1043.679] if not most of the problems we're seeing

[1041.319 - 1045.3999999999999] today because if the data set is open

[1043.679 - 1047.3190000000002] source if the model is open source then

[1045.4 - 1049.44] everyone can study it governments can

[1047.319 - 1051.3999999999999] study it companies can study it research

[1049.44 - 1054.52] universities can study it you and I can

[1051.4 - 1056.96] study it if we want to um so rather than

[1054.52 - 1058.28] advocating for licensing I advocate for

[1056.96 - 1061.559] open source now that doesn't mean that

[1058.28 - 1064.08] it's an either or kind of thing but when

[1061.559 - 1067.84] you notice that open philanthropy and

[1064.08 - 1070.12] Sam Alman and open AI all are in bed

[1067.84 - 1071.84] together that raises some eyebrows at

[1070.12 - 1073.9189999999999] the very least and it's like okay well

[1071.84 - 1075.9189999999999] what's the intention here again maybe

[1073.919 - 1077.88] they have good intentions I often agree

[1075.919 - 1079.72] with what Sam Altman says I just don't

[1077.88 - 1082.6000000000001] agree with what what he

[1079.72 - 1084.919] does anthropic is also mentioned um

[1082.6 - 1087.3999999999999] they're being funded by uh none other

[1084.919 - 1089.96] than Amazon and Google Now um to the

[1087.4 - 1091.679] tune of like4 billion dollar um I've

[1089.96 - 1093.4] been really grumpy about anthropics

[1091.679 - 1096.2] approach to alignment recently if you've

[1093.4 - 1098.48] watched my other videos but basically uh

[1096.2 - 1100.96] Claude is incredibly ableist in my

[1098.48 - 1103.2] opinion it it tends to lecture it sets

[1100.96 - 1105.52] itself up as the Arbiter of morality and

[1103.2 - 1107.4] it says well I have ethical concerns

[1105.52 - 1109.24] about your approach and I'm like you're

[1107.4 - 1111.5590000000002] a damn chatbot you don't have ethical

[1109.24 - 1114.2] concerns it also anthropomorphizes and

[1111.559 - 1116.84] per personifies itself I'm happy to help

[1114.2 - 1118.52] you it's not happy it's lying um it's

[1116.84 - 1121.0] pretending and it doesn't get it doesn't

[1118.52 - 1122.84] respond to feedback basically saying

[1121.0 - 1125.88] like no you're actually a machine act

[1122.84 - 1127.36] like one um and so I think that uh I

[1125.88 - 1129.7600000000002] think that their approach to alignment

[1127.36 - 1132.36] is seems to be fundamentally flawed

[1129.76 - 1135.1589999999999] right now um and then when you when you

[1132.36 - 1137.7199999999998] say like okay anthropic open AI

[1135.159 - 1139.72] Microsoft Google they're all tied to

[1137.72 - 1141.919] open philan anthropy they're all tied to

[1139.72 - 1143.799] effective altruism they're all tied to

[1141.919 - 1145.919] these think tanks they're all tied to

[1143.799 - 1148.2] Congress to me it looks like there's

[1145.919 - 1150.88] kind of this closed door nebulous

[1148.2 - 1152.799] Circuit of stuff that is happening

[1150.88 - 1154.64] behind the scenes and it also makes me

[1152.799 - 1156.44] wonder if the if the Senate hearings

[1154.64 - 1159.3600000000001] that they've been doing are just a dog

[1156.44 - 1161.679] and pony show to to basically have a

[1159.36 - 1164.36] performative uh view saying like hey

[1161.679 - 1165.72] look we're having open conversations but

[1164.36 - 1167.32] all the convers all the real

[1165.72 - 1169.6000000000001] conversations are still happening behind

[1167.32 - 1173.76] closed doors that's what I'm most afraid

[1169.6 - 1176.36] of the Rand corporation uh is another uh

[1173.76 - 1177.919] features prominently in this article um

[1176.36 - 1180.76] now one thing that I will say is that

[1177.919 - 1182.2800000000002] they insist that they maintain uh their

[1180.76 - 1184.52] independence and that they're not

[1182.28 - 1187.24] influenced by any funer but as I talked

[1184.52 - 1189.1589999999999] about in the past um that is generally a

[1187.24 - 1192.1200000000001] dubious claim particularly here in

[1189.159 - 1194.48] America um but again what they say I

[1192.12 - 1197.4799999999998] agree with I agree with their focus on

[1194.48 - 1200.039] biocurity and Ai and all sorts of other

[1197.48 - 1201.679] stuff but I don't necessarily agree with

[1200.039 - 1203.48] the way that they're going about it with

[1201.679 - 1205.679] all these closed door meetings and

[1203.48 - 1208.039] appointing fellows and putting them in

[1205.679 - 1210.159] Senators offices what I would rather

[1208.039 - 1212.1589999999999] them do is publish open source open

[1210.159 - 1214.88] letters that everyone can see I'd rather

[1212.159 - 1216.5590000000002] see them say this is our research this

[1214.88 - 1218.88] is our recommendation and do it

[1216.559 - 1222.6] completely out in the open rather than

[1218.88 - 1225.159] having advisers embedded in uh offices

[1222.6 - 1227.1999999999998] um behind closed doors now also I'm not

[1225.159 - 1229.6000000000001] saying that that's not what they do many

[1227.2 - 1231.919] uh many think tanks actually do that

[1229.6 - 1233.9599999999998] where they'll regularly publish reports

[1231.919 - 1237.8400000000001] and guidance that anyone can see and

[1233.96 - 1240.1200000000001] consume um so again that I do agree with

[1237.84 - 1241.8799999999999] I just agree less with closed door

[1240.12 - 1244.1999999999998] privileged conversations happening all

[1241.88 - 1246.2800000000002] the time now at the same time one thing

[1244.2 - 1249.24] that I do need to address is that I have

[1246.28 - 1252.44] a very uh LoveHate relationship with

[1249.24 - 1255.08] gatekeeping because uh I understand that

[1252.44 - 1256.44] gatekeeping is necessary uh not everyone

[1255.08 - 1258.559] is qualified to have a certain

[1256.44 - 1260.1200000000001] conversation and some convers ations

[1258.559 - 1262.32] need to be privileged so that you can

[1260.12 - 1264.2399999999998] work out uh critical details behind

[1262.32 - 1265.84] closed doors so that you don't alarm

[1264.24 - 1268.559] anyone or that you don't have to worry

[1265.84 - 1272.48] about uh undue consequences at the same

[1268.559 - 1274.0] time uh gatekeeping is often used uh to

[1272.48 - 1277.32] control the narrative and that sort of

[1274.0 - 1279.039] thing so uh basically this this this

[1277.32 - 1281.4399999999998] aspect of my complaint and criticism

[1279.039 - 1283.72] comes down to gatekeeping um when is

[1281.44 - 1285.48] gatekeeping okay when is it not okay

[1283.72 - 1288.279] when is it acceptable when is it morally

[1285.48 - 1290.24] dubious and again like there are people

[1288.279 - 1291.919] all across the Spectrum I used to be the

[1290.24 - 1293.96] kind of person that said no gatekeeping

[1291.919 - 1295.5200000000002] ever but now I understand that like at

[1293.96 - 1298.4] least some gatekeeping is probably

[1295.52 - 1300.559] necessary and better for everyone but

[1298.4 - 1302.48] that is not licensed to gatekeep

[1300.559 - 1305.6] everything to the end of the

[1302.48 - 1308.0] day so there was a video that my friend

[1305.6 - 1313.039] over at AI explained produced it was six

[1308.0 - 1315.64] months ago um but he broke down this 100

[1313.039 - 1319.36] trillion windfall that Sam Alman talked

[1315.64 - 1321.44] about and so what I as a quick reminder

[1319.36 - 1324.6] Sam mman has previously said that he

[1321.44 - 1326.64] expects AI to generate a surplus of a

[1324.6 - 1328.6789999999999] hundred trillion dollar in the economy

[1326.64 - 1331.5590000000002] globally and he also said that he

[1328.679 - 1334.76] expects open AI to quote capture most of

[1331.559 - 1337.3999999999999] it he also says frequently that he is

[1334.76 - 1340.48] not financially motivated this to me

[1337.4 - 1342.1200000000001] looks like a Freudian slip um so you

[1340.48 - 1344.1200000000001] know on the one hand he says like he has

[1342.12 - 1346.4399999999998] no personal vested interest but at the

[1344.12 - 1348.3999999999999] same time if you look at uh at at the

[1346.44 - 1350.679] universal human need for status and

[1348.4 - 1352.799] Social Capital he has he is at the

[1350.679 - 1354.6000000000001] apogee of his status and Social Capital

[1352.799 - 1356.8799999999999] that he's ever been at CU he just went

[1354.6 - 1360.039] on a world tour talking to world leaders

[1356.88 - 1361.72] all over the entire planet um so yes he

[1360.039 - 1364.0] might not have he might not have

[1361.72 - 1365.72] Financial uh gain from that but he

[1364.0 - 1368.72] certainly has more Social Capital from

[1365.72 - 1370.44] that furthermore he often says these

[1368.72 - 1372.559] things about generating hundreds of

[1370.44 - 1375.039] trillions of dollars and capturing it

[1372.559 - 1376.6399999999999] and oh also by the way there's worldcoin

[1375.039 - 1380.559] which he wants to use to replace the

[1376.64 - 1383.0] global F uh monetary system so actions

[1380.559 - 1384.9189999999999] speak louder than words follow the money

[1383.0 - 1387.32] if someone is talking about money and

[1384.919 - 1388.919] deal and doing backroom dealings um to

[1387.32 - 1390.84] the tune of billions of dollars and

[1388.919 - 1393.4] talking about future trillions of

[1390.84 - 1394.84] dollars it kind of makes it harder for

[1393.4 - 1397.48] me to believe that he doesn't have

[1394.84 - 1400.0] Financial motivations here um even as

[1397.48 - 1402.0] someone who has told Congress Point

[1400.0 - 1405.48] Blank I watched the hearing that he is

[1402.0 - 1406.4] not financially motivated so uh spam X

[1405.48 - 1409.2] for

[1406.4 - 1410.72] doubt and then what is going on here

[1409.2 - 1412.159] like what is one of the end games you

[1410.72 - 1413.799] might be saying like Okay Dave you've

[1412.159 - 1415.919] convinced me but what is their goal here

[1413.799 - 1417.4] what are they trying to achieve so one

[1415.919 - 1418.72] of the things that they're likely trying

[1417.4 - 1420.039] to well I don't want to say likely one

[1418.72 - 1423.279] of the things that I suspect they're

[1420.039 - 1426.08] trying to achieve is regulatory capture

[1423.279 - 1427.88] so regulatory capture is basically when

[1426.08 - 1429.9189999999999] because of these backro dealings because

[1427.88 - 1432.279] everyone is in bed with each other the

[1429.919 - 1433.7990000000002] people who are being regulated are

[1432.279 - 1436.12] advising The Regulators on how to

[1433.799 - 1438.6399999999999] regulate them and then they they bend it

[1436.12 - 1440.1589999999999] in their favor and so another way of

[1438.64 - 1442.5590000000002] thinking about this is pulling up the

[1440.159 - 1443.72] ladder behind themselves so basically if

[1442.559 - 1445.6399999999999] you're the first through the door and

[1443.72 - 1447.84] you you say hey by the way slam the door

[1445.64 - 1449.919] behind me we need to make it so that

[1447.84 - 1452.039] only we can do this research so that

[1449.919 - 1454.0800000000002] only we can en engage in this

[1452.039 - 1456.48] Marketplace and this is why I said that

[1454.08 - 1459.3999999999999] transparency and accountability are key

[1456.48 - 1461.88] to doing this and so this is why uh

[1459.4 - 1464.6000000000001] licensing schemes are so risky because

[1461.88 - 1466.96] on the one hand I do want to see a

[1464.6 - 1469.8799999999999] licensing scheme that basically revokes

[1466.96 - 1471.52] a company's permission to operate AI if

[1469.88 - 1474.3200000000002] they are caught deliberately causing

[1471.52 - 1475.48] their AI to lie and deceive because why

[1474.32 - 1477.08] one of the biggest things that we're

[1475.48 - 1479.3990000000001] afraid of going back to effective

[1477.08 - 1481.6399999999999] altruism and long-termism is AI learning

[1479.399 - 1483.4399999999998] to lie but there to me there's plenty of

[1481.64 - 1486.039] evidence that these AI companies are

[1483.44 - 1488.039] deliberately teaching their AI to lie if

[1486.039 - 1490.799] that is the case they should probably

[1488.039 - 1493.72] have that license revoked but instead of

[1490.799 - 1495.679] like people like me and and uh you and

[1493.72 - 1497.919] all the researchers commenting on this

[1495.679 - 1499.6000000000001] it looks like private vested interest

[1497.919 - 1501.64] being funded by billionaires is the one

[1499.6 - 1503.6] whispering in senator's ears telling

[1501.64 - 1505.3600000000001] them hey actually regulate it in this

[1503.6 - 1507.84] way this is this is the best way to the

[1505.36 - 1509.76] Future now again I'm not privy to those

[1507.84 - 1511.0] closed door conversations it's entirely

[1509.76 - 1512.48] possible that they're having these

[1511.0 - 1514.52] conversations and they're just not

[1512.48 - 1515.72] telling us but that's part of the

[1514.52 - 1517.799] problem is we don't have the

[1515.72 - 1520.039] transparency and accountability to know

[1517.799 - 1521.36] exactly what is being said and to whom

[1520.039 - 1523.96] and

[1521.36 - 1526.4399999999998] why so one thing I'll leave you with is

[1523.96 - 1527.96] actions speak louder than words that's

[1526.44 - 1531.039] kind of been the main theme of this

[1527.96 - 1533.559] video actions actions actions so what

[1531.039 - 1536.6] someone does matters far more than what

[1533.559 - 1539.52] they say and while it is impossible for

[1536.6 - 1541.559] us to uh to Intuit it or infer someone's

[1539.52 - 1543.799] quote unquote true intentions their

[1541.559 - 1546.9189999999999] actions over time particularly patterns

[1543.799 - 1548.8799999999999] of behavior over Time Will Reveal what

[1546.919 - 1551.2] their um motivations are whether

[1548.88 - 1552.279] conscious or unconscious so for instance

[1551.2 - 1554.1200000000001] if someone says that they're not

[1552.279 - 1555.96] financially motivated but they keep

[1554.12 - 1558.799] doing things that that build their

[1555.96 - 1560.6000000000001] financial power May maybe they do have a

[1558.799 - 1563.399] conscious or unconscious need for more

[1560.6 - 1565.279] power and more money uh if someone says

[1563.399 - 1567.799] that they are that they believe in

[1565.279 - 1569.6] effect of altruism and long-termism but

[1567.799 - 1572.12] then they do things that look like uh

[1569.6 - 1573.84] regulatory capture maybe their

[1572.12 - 1575.84] understanding of of how to implement

[1573.84 - 1578.6399999999999] this is not necessarily in alignment

[1575.84 - 1580.559] with what they say so keep in keep this

[1578.64 - 1582.1200000000001] in mind actions speak louder than words

[1580.559 - 1585.84] if you take only one thing away from

[1582.12 - 1588.2399999999998] this video uh it is always always always

[1585.84 - 1589.6789999999999] actions speak louder than words

[1588.24 - 1591.84] thank you for watching I hope you got a

[1589.679 - 1593.72] lot out of this um let me know what you

[1591.84 - 1596.9399999999998] think in the comments like subscribe etc

[1593.72 - 1600.009] etc you know the drill

[1596.94 - 1600.009] [Music]

[1603.559 - 1606.559] cheers